{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertConfig, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sent_encoder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['ai4bharat/indic-bert','../temp/tapt-model']\n",
    "model_num = 1\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[model_num])\n",
    "labels = ['fake','hate', 'defamation','offensive','non-hostile']\n",
    "lab_num = 3\n",
    "src = '../temp/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../datasets/covid/Constraint_English_Train - Sheet1.csv')\n",
    "# test = pd.read_csv('../datasets/covid/Constraint_English_Val - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(src+'train.pickle','rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    train = pd.DataFrame.from_dict(train)\n",
    "    train.drop(train.head(1).index, inplace=True)\n",
    "with open(src+'valid.pickle','rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "    valid = pd.DataFrame.from_dict(valid)\n",
    "    valid.drop(valid.head(1).index, inplace=True)\n",
    "with open(src+'test.pickle','rb') as f:\n",
    "    test = pickle.load(f)\n",
    "#     del test['task_1']\n",
    "    test = pd.DataFrame.from_dict(test)\n",
    "#     test.drop(test.head(1).index, inplace=True)\n",
    "#     test = pd.DataFrame.from_dict(test)\n",
    "# test = pd.read_csv('data/valid.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>full_tweet</th>\n",
       "      <th>tweet_raw_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>smiley</th>\n",
       "      <th>emoji</th>\n",
       "      <th>url</th>\n",
       "      <th>mentions</th>\n",
       "      <th>numerals</th>\n",
       "      <th>reserved_word</th>\n",
       "      <th>emotext</th>\n",
       "      <th>segmented_hash</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...</td>\n",
       "      <td>‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[20, 6, 10, 20, 6]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...</td>\n",
       "      <td>‡§™‡§ü‡§®‡§æ :  BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/Dq05hREifM]</td>\n",
       "      <td>[@kumarprakash4u]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...</td>\n",
       "      <td>‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä ,  ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞ ,  ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üôè, üòÇ, üëç]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[folded hands, face with tears of joy, thumbs up]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...</td>\n",
       "      <td>‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>RT @_Pb_swain_: ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ...</td>\n",
       "      <td>:  ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§Æ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üëá, üòÇ, üòÇ, üòÇ, üòÇ]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@_Pb_swain_]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[RT]</td>\n",
       "      <td>[backhand index pointing down, face with tears...</td>\n",
       "      <td>[]</td>\n",
       "      <td>RT  : ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...</td>\n",
       "      <td>‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä ,...</td>\n",
       "      <td>[#Maoist, #WestBengal]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/pP1AOvOv0b]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[maoist, west bengal]</td>\n",
       "      <td>‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>#Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...</td>\n",
       "      <td>#Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...</td>\n",
       "      <td>[#Breaking, #Sushantsinghcase, #Kangana]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/szOTZWq1hI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[breaking, sushantsinghcase, kangana]</td>\n",
       "      <td>-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ‡§ß‡§Æ‡§ï‡•Ä ‡§Æ‡§ø‡§≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>@BasudebaTripat4: @Rajanspsingh1 ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ...</td>\n",
       "      <td>:  ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ‡§≤‡•á ‡§ï‡§æ ‡§∏‡§∞ ‡§´‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ ,  ,  ‡§ó‡§∞‡•ç‡§¶‡§®...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@BasudebaTripat4, @Rajanspsingh1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>:   ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ‡§≤‡•á ‡§ï‡§æ ‡§∏‡§∞ ‡§´‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ,, ‡§ó‡§∞‡•ç‡§¶‡§® ‡§§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...</td>\n",
       "      <td>‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...</td>\n",
       "      <td>‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç , ‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤ ‡§∞...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[100, 100, 500, 1000]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tweet_id                                         full_tweet  \\\n",
       "0        1  ‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...   \n",
       "1        2  ‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...   \n",
       "2        3  ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...   \n",
       "3        4  ‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...   \n",
       "4        5  RT @_Pb_swain_: ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ...   \n",
       "5        6  ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...   \n",
       "6        7  #Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...   \n",
       "7        8  @BasudebaTripat4: @Rajanspsingh1 ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ...   \n",
       "8        9  ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...   \n",
       "9       10  ‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...   \n",
       "\n",
       "                                      tweet_raw_text  \\\n",
       "0  ‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...   \n",
       "1  ‡§™‡§ü‡§®‡§æ :  BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ...   \n",
       "2  ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä ,  ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞ ,  ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™...   \n",
       "3  ‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...   \n",
       "4  :  ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§Æ...   \n",
       "5  ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä ,...   \n",
       "6  #Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...   \n",
       "7  :  ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ‡§≤‡•á ‡§ï‡§æ ‡§∏‡§∞ ‡§´‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ ,  ,  ‡§ó‡§∞‡•ç‡§¶‡§®...   \n",
       "8  ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...   \n",
       "9  ‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç , ‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤ ‡§∞...   \n",
       "\n",
       "                                   hashtags smiley            emoji  \\\n",
       "0                                        []     []               []   \n",
       "1                                        []     []               []   \n",
       "2                                        []     []        [üôè, üòÇ, üëç]   \n",
       "3                                        []     []               []   \n",
       "4                                        []     []  [üëá, üòÇ, üòÇ, üòÇ, üòÇ]   \n",
       "5                    [#Maoist, #WestBengal]     []               []   \n",
       "6  [#Breaking, #Sushantsinghcase, #Kangana]     []               []   \n",
       "7                                        []     []               []   \n",
       "8                                        []     []               []   \n",
       "9                                        []     []               []   \n",
       "\n",
       "                         url                            mentions  \\\n",
       "0                         []                                  []   \n",
       "1  [https://t.co/Dq05hREifM]                   [@kumarprakash4u]   \n",
       "2                         []                                  []   \n",
       "3                         []                                  []   \n",
       "4                         []                       [@_Pb_swain_]   \n",
       "5  [https://t.co/pP1AOvOv0b]                                  []   \n",
       "6  [https://t.co/szOTZWq1hI]                                  []   \n",
       "7                         []  [@BasudebaTripat4, @Rajanspsingh1]   \n",
       "8                         []                                  []   \n",
       "9                         []                                  []   \n",
       "\n",
       "                numerals reserved_word  \\\n",
       "0     [20, 6, 10, 20, 6]            []   \n",
       "1                     []            []   \n",
       "2                     []            []   \n",
       "3                     []            []   \n",
       "4                     []          [RT]   \n",
       "5                     []            []   \n",
       "6                     []            []   \n",
       "7                     []            []   \n",
       "8                     []            []   \n",
       "9  [100, 100, 500, 1000]            []   \n",
       "\n",
       "                                             emotext  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [folded hands, face with tears of joy, thumbs up]   \n",
       "3                                                 []   \n",
       "4  [backhand index pointing down, face with tears...   \n",
       "5                                                 []   \n",
       "6                                                 []   \n",
       "7                                                 []   \n",
       "8                                                 []   \n",
       "9                                                 []   \n",
       "\n",
       "                          segmented_hash  \\\n",
       "0                                     []   \n",
       "1                                     []   \n",
       "2                                     []   \n",
       "3                                     []   \n",
       "4                                     []   \n",
       "5                  [maoist, west bengal]   \n",
       "6  [breaking, sushantsinghcase, kangana]   \n",
       "7                                     []   \n",
       "8                                     []   \n",
       "9                                     []   \n",
       "\n",
       "                                               clean  \n",
       "0  ‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...  \n",
       "1  ‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...  \n",
       "2  ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...  \n",
       "3  ‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...  \n",
       "4  RT  : ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ...  \n",
       "5  ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...  \n",
       "6   -‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ‡§ß‡§Æ‡§ï‡•Ä ‡§Æ‡§ø‡§≤...  \n",
       "7   :   ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ‡§≤‡•á ‡§ï‡§æ ‡§∏‡§∞ ‡§´‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ,, ‡§ó‡§∞‡•ç‡§¶‡§® ‡§§...  \n",
       "8  ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...  \n",
       "9  ‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train, valid])\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = labels[lab_num]\n",
    "# def label_encode(val):\n",
    "#     return labels.index(val)\n",
    "def label_encode(val):\n",
    "    val = val.split(',')\n",
    "    if lab_num == 4:\n",
    "        if lab in val:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        if lab in val:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = train.task_1.apply(label_encode)\n",
    "train['tweet'] = train.full_tweet\n",
    "test['tweet'] = test.full_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1915    0\n",
       "4       1\n",
       "4605    0\n",
       "2017    0\n",
       "2340    0\n",
       "573     0\n",
       "4920    0\n",
       "135     0\n",
       "1878    0\n",
       "589     0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "# train.tweet = train.tweet.apply(clean_text)\n",
    "# train.tweet = train.tweet.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.label = test.label.apply(label_encode)\n",
    "test = test.reset_index(drop=True)\n",
    "# test.tweet = test.tweet.apply(clean_text)\n",
    "# test.tweet = test.tweet.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370     ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§Ö‡§¨ ‡§Æ‡§ß‡•ç‡§Ø ‡§™‡•ç‡§∞‡§¶‡•á‡§∂ ‡§Æ‡•á‡§Ç ‡§´‡•ç‡§≤‡•à‡§ü ‡§π‡•Å‡§Ü...\n",
       "2173    ‡§∂‡§π‡•Ä‡§¶ ‡§µ‡•Ä‡§∞‡§™‡§æ‡§≤ ‡§∏‡§ø‡§Ç‡§π ‡§ú‡§Æ‡•ç‡§Æ‡•Ç-‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§ï‡•á ‡§∏‡•Å‡§Ç‡§¶‡§∞‡•Ä ‡§µ‡§® ‡§Æ‡•á‡§Ç...\n",
       "933     ‡§∏‡•Å‡§∂‡§æ‡§Ç‡§§ ‡§ï‡•Ä ‡§ë‡§ü‡•ã‡§™‡•ç‡§∏‡•Ä ‡§ï‡•á‡§∏ ‡§Æ‡•á‡§Ç ‡§∏‡§ñ‡•ç‡§§ MSHRC, ‡§ï‡•Ç‡§™‡§∞ ‡§Ö‡§∏‡•ç...\n",
       "468     ‡§Æ‡•à‡§Ç‡§®‡•á ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡•ã ‡§µ‡•ã‡§ü ‡§¶‡§ø‡§Ø‡§æ ‡§π‡•à ‡§ú‡§ø‡§∏ ‡§¨‡§æ‡§§ ‡§ï‡§æ ‡§°‡§∞ ‡§•‡§æ...\n",
       "1546    ‡§™‡•Ä ‡§ï‡•á ‡§è‡§ï ‡§ó‡§æ‡§Ç‡§µ ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§∞‡•ã‡§®‡§æ ‡§∏‡•ç‡§ï‡•ç‡§∞‡•Ä‡§®‡§ø‡§Ç‡§ó ‡§ï‡§∞‡§®‡•á ‡§ó‡§à‡§Ç ‡§°...\n",
       "983     ‡§á‡§≤‡§æ‡§π‡§æ‡§¨‡§æ‡§¶ ‡§π‡§æ‡§à‡§ï‡•ã‡§∞‡•ç‡§ü ‡§®‡•á ‡§¶‡§ø‡§Ø‡§æ ‡§°‡•â ‡§ï‡§´‡•Ä‡§≤ ‡§ñ‡§æ‡§® ‡§ï‡•Ä ‡§§‡•Å‡§∞‡§Ç‡§§...\n",
       "4224    ‡§ï‡§Æ‡•ç‡§Ø‡•Å‡§®‡§ø‡§∑‡•ç‡§ü‡•ã‡§Ç ‡§ï‡•Ä ‡§π‡§∞‡§ï‡§§ ‡§¶‡•á‡§ñ‡•ã: ‡§∂‡§æ‡§π‡•Ä‡§® ‡§¨‡§æ‡§ó ‡§ï‡•á ‡§á‡§µ‡•á‡§Ç‡§ü ...\n",
       "2734    ‡§Ö‡§Ç‡§§‡§∞‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Ø ‡§â‡§°‡§º‡§æ‡§®‡•ã‡§Ç ‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á ‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä, ‡§≤‡§ñ...\n",
       "4785    ‡§™‡•Å‡§≤‡§ø‡§∏ ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§ï‡§æ ‡§≠‡•Ä ‡§®‡§ø‡§ú‡§ø‡§ï‡§∞‡§£ ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è   ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø...\n",
       "4227     ‡§∏‡•Å‡§∞‡•á‡§∂ ‡§∞‡•à‡§®‡§æ ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§π‡§∞‡§≠‡§ú‡§® ‡§∏‡§ø‡§Ç‡§π ‡§®‡•á ‡§≠‡•Ä ‡§Ü‡§à‡§™‡•Ä‡§è‡§≤ 202...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tweet.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['tweet'], train['label'])\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train['tweet'], train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    try:\n",
    "        return len(text.split())\n",
    "    except:\n",
    "        print(text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30.687549721559268, 1990, 26, 5028)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "maxw = 0\n",
    "large_count = 0\n",
    "for i in train_x:\n",
    "    temp = count_words(i)\n",
    "    total += temp\n",
    "    maxw = temp if temp > maxw else maxw\n",
    "    large_count += 1 if temp > 120 else 0\n",
    "total/len(train_x), maxw, large_count, len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LENGTH = 50\n",
    "posts = train.values\n",
    "categories = train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.models as gsm\n",
    "e2v = gsm.KeyedVectors.load_word2vec_format('emoji2vec.bin', binary=True)\n",
    "# happy_vector = e2v['üòÇ']    # Produces an embedding vector of length 300\n",
    "\n",
    "# Download the bin file from here https://github.com/uclnlp/emoji2vec/blob/master/pre-trained/emoji2vec.bin\n",
    "\n",
    "def getEmojiEmbeddings(emojiList,dim=300,verbose = False):\n",
    "  \"\"\" Generates an emoji vector by averaging the emoji representation for each emoji. If no emoji returns an empty list of dimension dim\"\"\"\n",
    "  if dim < 300:\n",
    "    raise IndexError(\"Dim has to be greater than 300\")\n",
    "  result = np.zeros(dim)\n",
    "  if (len(emojiList) == 0):\n",
    "    return result\n",
    "  else:\n",
    "    embs = None\n",
    "    for i in emojiList:\n",
    "      if verbose:\n",
    "        if i not in e2v.vocab:\n",
    "          print(i)\n",
    "    embs = np.mean([e2v[i] for i in emojiList if i in e2v.vocab], axis=0)\n",
    "  if np.any(np.isnan(embs)):\n",
    "    return result\n",
    "  result[:300] = embs\n",
    "  return result\n",
    "getEmojiEmbeddings(valid.emoji.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2173: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([128]), torch.Size([300]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode_plus(\n",
    "            valid.full_tweet.values[0],\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )['input_ids']\n",
    "torch.tensor(ids, dtype=torch.long).shape, torch.tensor(getEmojiEmbeddings(valid.emoji.values[0]), dtype=torch.long).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len, t = False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.tweet\n",
    "        self.emoji = dataframe.emoji\n",
    "        self.hash = dataframe.segmented_hash\n",
    "        self.t = t\n",
    "        if not self.t:\n",
    "            self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        h_text = self.hash[index]\n",
    "        h_text = \" \".join(h_text)\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            h_text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        h_ids = inputs['input_ids']\n",
    "        h_mask = inputs['attention_mask']\n",
    "        h_token_type_ids = inputs[\"token_type_ids\"]\n",
    "#         h_inputs\n",
    "        emoji = getEmojiEmbeddings(self.emoji[index])\n",
    "        if self.t:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                'h_ids': torch.tensor(h_ids, dtype=torch.long),\n",
    "                'h_mask': torch.tensor(h_mask, dtype=torch.long),\n",
    "                'h_token_type_ids': torch.tensor(h_token_type_ids, dtype=torch.long),\n",
    "                'emoji' : torch.tensor(emoji, dtype=torch.long),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                'h_ids': torch.tensor(h_ids, dtype=torch.long),\n",
    "                'h_mask': torch.tensor(h_mask, dtype=torch.long),\n",
    "                'h_token_type_ids': torch.tensor(h_token_type_ids, dtype=torch.long),\n",
    "                'emoji' : torch.tensor(emoji, dtype=torch.long),\n",
    "                'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (6286, 16)\n",
      "TRAIN Dataset: (5343, 16)\n",
      "TEST Dataset: (943, 16)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.85\n",
    "train_data=train.sample(frac=train_size,random_state=200)\n",
    "test_data=train.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(train.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = MultiLabelDataset(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = MultiLabelDataset(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(200000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (l2): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(200000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (pre_classifier_1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (pre_classifier_2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (pre_classifier_3): Linear(in_features=1836, out_features=1836, bias=True)\n",
       "  (classifier): Linear(in_features=1836, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(models[model_num])\n",
    "        self.l2 = AutoModel.from_pretrained(models[model_num])\n",
    "        \n",
    "        self.pre_classifier_1 = torch.nn.Linear(768, 768)\n",
    "        self.pre_classifier_2 = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.pre_classifier_3 = torch.nn.Linear(1836, 1836)\n",
    "#         self.pre_classifier_3 = torch.nn.Linear(768, 100)\n",
    "        self.classifier = torch.nn.Linear(1836, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state_1 = output_1[0]\n",
    "        pooler_1 = hidden_state_1[:, 0]\n",
    "        pooler_1 = self.pre_classifier_1(pooler_1)\n",
    "        pooler_1 = torch.nn.Tanh()(pooler_1)\n",
    "        pooler_1 = self.dropout(pooler_1)\n",
    "        output_2 = self.l2(input_ids=h_ids, attention_mask=h_mask)\n",
    "        hidden_state_2 = output_2[0]\n",
    "        pooler_2 = hidden_state_2[:, 0]\n",
    "        pooler_2 = self.pre_classifier_2(pooler_2)\n",
    "        pooler_2 = torch.nn.Tanh()(pooler_2)\n",
    "        pooler_2 = self.dropout(pooler_2)\n",
    "        pooler_3 = torch.cat((pooler_1, pooler_2), 1)\n",
    "        pooler_3 = torch.cat((pooler_3, emoji), 1)\n",
    "#         print(pooler_1.shape,hidden_state_1.shape, pooler_2.shape, emoji.type(torch.FloatTensor).shape)\n",
    "#         pooler_3 = torch.nn.Tanh()(emoji.type(torch.FloatTensor))\n",
    "#         pooler_3 = self.dropout(pooler_3)\n",
    "#         print(pooler_3.shape)\n",
    "        pooler_3 = self.pre_classifier_3(pooler_3)\n",
    "#         pooler_3 = self.pre_classifier_3(pooler_2)\n",
    "        pooler_3 = torch.nn.Tanh()(pooler_3)\n",
    "        pooler_3 = self.dropout(pooler_3)\n",
    "        output = self.classifier(pooler_3)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# print(repr(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 58 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "l1.embeddings.word_embeddings.weight                    (200000, 128)\n",
      "l1.embeddings.position_embeddings.weight                  (512, 128)\n",
      "l1.embeddings.token_type_embeddings.weight                  (2, 128)\n",
      "l1.embeddings.LayerNorm.weight                                (128,)\n",
      "l1.embeddings.LayerNorm.bias                                  (128,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "l1.encoder.embedding_hidden_mapping_in.weight             (768, 128)\n",
      "l1.encoder.embedding_hidden_mapping_in.bias                   (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight   (768, 768)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight   (768, 768)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight   (768, 768)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight   (768, 768)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight  (3072, 768)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias      (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "pre_classifier_3.weight                                 (1836, 1836)\n",
      "pre_classifier_3.bias                                        (1836,)\n",
      "classifier.weight                                          (2, 1836)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    total_train_loss = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        h_ids = data['h_ids'].to(device, dtype = torch.long)\n",
    "        h_mask = data['h_mask'].to(device, dtype = torch.long)\n",
    "        h_token_type_ids = data['h_token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        emoji = data['emoji'].to(device, dtype = torch.long)\n",
    "        outputs = model(ids, mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji)\n",
    "        optimizer.zero_grad()\n",
    "#         loss = outputs.loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "#         if _%50==0:\n",
    "#             print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        total_train_loss += loss.item()\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    print(f'Epoch: {epoch}, Loss:  {total_train_loss/count}')\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            h_ids = data['h_ids'].to(device, dtype = torch.long)\n",
    "            h_mask = data['h_mask'].to(device, dtype = torch.long)\n",
    "            h_token_type_ids = data['h_token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            emoji = data['emoji'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    fin_outputs = list(np.argmax(np.array(fin_outputs), axis=1).flatten())\n",
    "    print(classification_report(fin_targets, fin_outputs))\n",
    "    torch.save(model, '../temp/finetuned/'+lab+'_epoch_'+str(epoch))\n",
    "    return fin_outputs, fin_targets\n",
    "#     final_outputs = np.array(fin_outputs) >=0.5\n",
    "#     final = []\n",
    "#     final_t = []\n",
    "#     final_fine = [[],[],[],[]]\n",
    "#     final_fine_t = [[],[],[],[]]\n",
    "#     for (i,j) in zip(final_outputs, fin_targets):\n",
    "#         output_sum = sum(i)\n",
    "#         target_sum = sum(j)\n",
    "#         if output_sum == 0:\n",
    "#             final.append(0)\n",
    "#         else:\n",
    "#             final.append(1)\n",
    "#         if target_sum == 0:\n",
    "#             final_t.append(0)\n",
    "#         else:\n",
    "#             final_t.append(1)\n",
    "#         for p in range(4):\n",
    "#             final_fine[p].append(int(i[p]))\n",
    "#             final_fine_t[p].append(int(j[p]))\n",
    "#     print(\"Coarse:\")\n",
    "#     print(classification_report(final, final_t))\n",
    "#     for i in range(4):\n",
    "#         print(\"Fine\", i)\n",
    "    \n",
    "#     return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  4.05it/s]/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "334it [01:21,  4.08it/s]\n",
      "1it [00:00,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.35052143896143595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:05,  5.60it/s]\n",
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93       812\n",
      "           1       0.00      0.00      0.00       131\n",
      "\n",
      "    accuracy                           0.86       943\n",
      "   macro avg       0.43      0.50      0.46       943\n",
      "weighted avg       0.74      0.86      0.80       943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    out, tar = train(epoch)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
