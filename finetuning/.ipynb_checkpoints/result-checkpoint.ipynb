{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertConfig, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sent_encoder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = ['fake','hate', 'defamation','offensive','non-hostile']\n",
    "lab_num = 4\n",
    "EPOCH_NUM = 0\n",
    "lab = labels[lab_num]\n",
    "epoch_name = '../temp/finetuned/'+lab+'_epoch_'+str(EPOCH_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['ai4bharat/indic-bert', 'distilbert-base-uncased-finetuned-sst-2-english', 'textattack/roberta-base-SST-2','roberta-base', 'google/electra-base-discriminator', 'xlnet-base-cased', 'xlm-roberta-base', '/scratch/indic-tapt','/scratch/indic-tapt2', '/scratch/indic-tapt/checkpoint-500']\n",
    "model_num = 0\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[model_num])\n",
    "src = '../temp/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../datasets/covid/Constraint_English_Train - Sheet1.csv')\n",
    "# test = pd.read_csv('../datasets/covid/Constraint_English_Val - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(src+'train.pickle','rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    train = pd.DataFrame.from_dict(train)\n",
    "    train.drop(train.head(1).index, inplace=True)\n",
    "with open(src+'valid.pickle','rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "    valid = pd.DataFrame.from_dict(valid)\n",
    "    valid.drop(valid.head(1).index, inplace=True)\n",
    "with open(src+'test.pickle','rb') as f:\n",
    "    test = pickle.load(f)\n",
    "#     del test['task_1']\n",
    "    test = pd.DataFrame.from_dict(test)\n",
    "#     test.drop(test.head(1).index, inplace=True)\n",
    "#     test = pd.DataFrame.from_dict(test)\n",
    "# test = pd.read_csv('data/valid.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, valid])\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = labels[lab_num]\n",
    "# def label_encode(val):\n",
    "#     return labels.index(val)\n",
    "def label_encode(val):\n",
    "    val = val.split(',')\n",
    "    if lab_num == 4:\n",
    "        if lab in val:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        if lab in val:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = train.task_1.apply(label_encode)\n",
    "train['tweet'] = train.full_tweet\n",
    "test['tweet'] = test.full_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.label.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "# train.tweet = train.tweet.apply(clean_text)\n",
    "# train.tweet = train.tweet.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.label = test.label.apply(label_encode)\n",
    "test = test.reset_index(drop=True)\n",
    "# test.tweet = test.tweet.apply(clean_text)\n",
    "# test.tweet = test.tweet.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tweet.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['tweet'], train['label'])\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train['tweet'], train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    try:\n",
    "        return len(text.split())\n",
    "    except:\n",
    "        print(text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "maxw = 0\n",
    "large_count = 0\n",
    "for i in train_x:\n",
    "    temp = count_words(i)\n",
    "    total += temp\n",
    "    maxw = temp if temp > maxw else maxw\n",
    "    large_count += 1 if temp > 120 else 0\n",
    "total/len(train_x), maxw, large_count, len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LENGTH = 50\n",
    "posts = train.values\n",
    "categories = train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models as gsm\n",
    "e2v = gsm.KeyedVectors.load_word2vec_format('emoji2vec.bin', binary=True)\n",
    "# happy_vector = e2v['ðŸ˜‚']    # Produces an embedding vector of length 300\n",
    "\n",
    "# Download the bin file from here https://github.com/uclnlp/emoji2vec/blob/master/pre-trained/emoji2vec.bin\n",
    "\n",
    "def getEmojiEmbeddings(emojiList,dim=300,verbose = False):\n",
    "  \"\"\" Generates an emoji vector by averaging the emoji representation for each emoji. If no emoji returns an empty list of dimension dim\"\"\"\n",
    "  if dim < 300:\n",
    "    raise IndexError(\"Dim has to be greater than 300\")\n",
    "  result = np.zeros(dim)\n",
    "  if (len(emojiList) == 0):\n",
    "    return result\n",
    "  else:\n",
    "    embs = None\n",
    "    for i in emojiList:\n",
    "      if verbose:\n",
    "        if i not in e2v.vocab:\n",
    "          print(i)\n",
    "    embs = np.mean([e2v[i] for i in emojiList if i in e2v.vocab], axis=0)\n",
    "  if np.any(np.isnan(embs)):\n",
    "    return result\n",
    "  result[:300] = embs\n",
    "  return result\n",
    "getEmojiEmbeddings(valid.emoji.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode_plus(\n",
    "            valid.full_tweet.values[0],\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )['input_ids']\n",
    "torch.tensor(ids, dtype=torch.long).shape, torch.tensor(getEmojiEmbeddings(valid.emoji.values[0]), dtype=torch.long).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len, t = False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.tweet\n",
    "        self.emoji = dataframe.emoji\n",
    "        self.hash = dataframe.segmented_hash\n",
    "        self.t = t\n",
    "        if not self.t:\n",
    "            self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        h_text = self.hash[index]\n",
    "        h_text = \" \".join(h_text)\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            h_text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        h_ids = inputs['input_ids']\n",
    "        h_mask = inputs['attention_mask']\n",
    "        h_token_type_ids = inputs[\"token_type_ids\"]\n",
    "#         h_inputs\n",
    "        emoji = getEmojiEmbeddings(self.emoji[index])\n",
    "        if self.t:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                'h_ids': torch.tensor(h_ids, dtype=torch.long),\n",
    "                'h_mask': torch.tensor(h_mask, dtype=torch.long),\n",
    "                'h_token_type_ids': torch.tensor(h_token_type_ids, dtype=torch.long),\n",
    "                'emoji' : torch.tensor(emoji, dtype=torch.long),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                'h_ids': torch.tensor(h_ids, dtype=torch.long),\n",
    "                'h_mask': torch.tensor(h_mask, dtype=torch.long),\n",
    "                'h_token_type_ids': torch.tensor(h_token_type_ids, dtype=torch.long),\n",
    "                'emoji' : torch.tensor(emoji, dtype=torch.long),\n",
    "                'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.85\n",
    "train_data=train.sample(frac=train_size,random_state=200)\n",
    "test_data=train.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(train.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = MultiLabelDataset(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = MultiLabelDataset(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(models[model_num])\n",
    "        self.l2 = AutoModel.from_pretrained(models[model_num])\n",
    "        \n",
    "        self.pre_classifier_1 = torch.nn.Linear(768, 768)\n",
    "        self.pre_classifier_2 = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.pre_classifier_3 = torch.nn.Linear(1836, 1836)\n",
    "#         self.pre_classifier_3 = torch.nn.Linear(768, 100)\n",
    "        self.classifier = torch.nn.Linear(1836, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state_1 = output_1[0]\n",
    "        pooler_1 = hidden_state_1[:, 0]\n",
    "        pooler_1 = self.pre_classifier_1(pooler_1)\n",
    "        pooler_1 = torch.nn.Tanh()(pooler_1)\n",
    "        pooler_1 = self.dropout(pooler_1)\n",
    "        output_2 = self.l2(input_ids=h_ids, attention_mask=h_mask)\n",
    "        hidden_state_2 = output_2[0]\n",
    "        pooler_2 = hidden_state_2[:, 0]\n",
    "        pooler_2 = self.pre_classifier_2(pooler_2)\n",
    "        pooler_2 = torch.nn.Tanh()(pooler_2)\n",
    "        pooler_2 = self.dropout(pooler_2)\n",
    "        pooler_3 = torch.cat((pooler_1, pooler_2), 1)\n",
    "        pooler_3 = torch.cat((pooler_3, emoji), 1)\n",
    "#         print(pooler_1.shape,hidden_state_1.shape, pooler_2.shape, emoji.type(torch.FloatTensor).shape)\n",
    "#         pooler_3 = torch.nn.Tanh()(emoji.type(torch.FloatTensor))\n",
    "#         pooler_3 = self.dropout(pooler_3)\n",
    "#         print(pooler_3.shape)\n",
    "        pooler_3 = self.pre_classifier_3(pooler_3)\n",
    "#         pooler_3 = self.pre_classifier_3(pooler_2)\n",
    "        pooler_3 = torch.nn.Tanh()(pooler_3)\n",
    "        pooler_3 = self.dropout(pooler_3)\n",
    "        output = self.classifier(pooler_3)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# print(repr(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    total_train_loss = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        h_ids = data['h_ids'].to(device, dtype = torch.long)\n",
    "        h_mask = data['h_mask'].to(device, dtype = torch.long)\n",
    "        h_token_type_ids = data['h_token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        emoji = data['emoji'].to(device, dtype = torch.long)\n",
    "        outputs = model(ids, mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji)\n",
    "        optimizer.zero_grad()\n",
    "#         loss = outputs.loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "#         if _%50==0:\n",
    "#             print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        total_train_loss += loss.item()\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    print(f'Epoch: {epoch}, Loss:  {total_train_loss/count}')\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            h_ids = data['h_ids'].to(device, dtype = torch.long)\n",
    "            h_mask = data['h_mask'].to(device, dtype = torch.long)\n",
    "            h_token_type_ids = data['h_token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            emoji = data['emoji'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    fin_outputs = list(np.argmax(np.array(fin_outputs), axis=1).flatten())\n",
    "    print(classification_report(fin_targets, fin_outputs))\n",
    "    torch.save(model, '../temp/finetuned/'+lab+'_epoch_'+str(epoch))\n",
    "    return fin_outputs, fin_targets\n",
    "#     final_outputs = np.array(fin_outputs) >=0.5\n",
    "#     final = []\n",
    "#     final_t = []\n",
    "#     final_fine = [[],[],[],[]]\n",
    "#     final_fine_t = [[],[],[],[]]\n",
    "#     for (i,j) in zip(final_outputs, fin_targets):\n",
    "#         output_sum = sum(i)\n",
    "#         target_sum = sum(j)\n",
    "#         if output_sum == 0:\n",
    "#             final.append(0)\n",
    "#         else:\n",
    "#             final.append(1)\n",
    "#         if target_sum == 0:\n",
    "#             final_t.append(0)\n",
    "#         else:\n",
    "#             final_t.append(1)\n",
    "#         for p in range(4):\n",
    "#             final_fine[p].append(int(i[p]))\n",
    "#             final_fine_t[p].append(int(j[p]))\n",
    "#     print(\"Coarse:\")\n",
    "#     print(classification_report(final, final_t))\n",
    "#     for i in range(4):\n",
    "#         print(\"Fine\", i)\n",
    "    \n",
    "#     return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(EPOCHS):\n",
    "#     out, tar = train(epoch)\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[0:10], tar[0:10]hjkhnk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "# train_size = 0.8\n",
    "# test_data=test.sample(frac=1,random_state=200)\n",
    "# test_data=train.drop(train_data.index).reset_index(drop=True)\n",
    "test_data = test.reset_index(drop=True)\n",
    "testing = MultiLabelDataset(test_data, tokenizer, MAX_LEN, t=True)\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "testing_loader = DataLoader(testing, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fin_targets=[]\n",
    "fin_outputs=[]\n",
    "# print(f'Epoch: {epoch}, Loss:  {total_train_loss/count}')\n",
    "with torch.no_grad():\n",
    "    model = torch.load(epoch_name, map_location=device)\n",
    "    model.eval()\n",
    "    for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        h_ids = data['h_ids'].to(device, dtype = torch.long)\n",
    "        h_mask = data['h_mask'].to(device, dtype = torch.long)\n",
    "        h_token_type_ids = data['h_token_type_ids'].to(device, dtype = torch.long)\n",
    "#         targets = data['targets'].to(device, dtype = torch.long)\n",
    "        emoji = data['emoji'].to(device, dtype = torch.long)\n",
    "        outputs = model(ids, mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji)\n",
    "#         fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "        fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "fin_outputs = list(np.argmax(np.array(fin_outputs), axis=1).flatten())\n",
    "# print(classification_report(fin_outputs, fin_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_outputs[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = np.array(fin_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fin_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test.full_tweet.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label_decode(val):\n",
    "#     return labels[val]\n",
    "# test.label = test.label.apply(label_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test.to_csv(path_or_buf='../temp/labels/'+lab+'.txt', index=False, columns = ['tweet_id', 'label'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
