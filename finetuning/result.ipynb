{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertConfig, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sent_encoder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = ['fake','hate', 'defamation','offensive','non-hostile']\n",
    "lab_num = 4\n",
    "EPOCH_NUM = 0\n",
    "lab = labels[lab_num]\n",
    "epoch_name = '../temp/finetuned/'+lab+'_epoch_'+str(EPOCH_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['ai4bharat/indic-bert', 'distilbert-base-uncased-finetuned-sst-2-english', 'textattack/roberta-base-SST-2','roberta-base', 'google/electra-base-discriminator', 'xlnet-base-cased', 'xlm-roberta-base', '/scratch/indic-tapt','/scratch/indic-tapt2', '/scratch/indic-tapt/checkpoint-500']\n",
    "model_num = 0\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[model_num])\n",
    "src = '../temp/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../datasets/covid/Constraint_English_Train - Sheet1.csv')\n",
    "# test = pd.read_csv('../datasets/covid/Constraint_English_Val - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(src+'train.pickle','rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    train = pd.DataFrame.from_dict(train)\n",
    "    train.drop(train.head(1).index, inplace=True)\n",
    "with open(src+'valid.pickle','rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "    valid = pd.DataFrame.from_dict(valid)\n",
    "    valid.drop(valid.head(1).index, inplace=True)\n",
    "with open(src+'test.pickle','rb') as f:\n",
    "    test = pickle.load(f)\n",
    "#     del test['task_1']\n",
    "    test = pd.DataFrame.from_dict(test)\n",
    "#     test.drop(test.head(1).index, inplace=True)\n",
    "#     test = pd.DataFrame.from_dict(test)\n",
    "# test = pd.read_csv('data/valid.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>full_tweet</th>\n",
       "      <th>tweet_raw_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>smiley</th>\n",
       "      <th>emoji</th>\n",
       "      <th>url</th>\n",
       "      <th>mentions</th>\n",
       "      <th>numerals</th>\n",
       "      <th>reserved_word</th>\n",
       "      <th>emotext</th>\n",
       "      <th>segmented_hash</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...</td>\n",
       "      <td>‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[20, 6, 10, 20, 6]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...</td>\n",
       "      <td>‡§™‡§ü‡§®‡§æ :  BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/Dq05hREifM]</td>\n",
       "      <td>[@kumarprakash4u]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...</td>\n",
       "      <td>‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä ,  ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞ ,  ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üôè, üòÇ, üëç]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[folded hands, face with tears of joy, thumbs up]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...</td>\n",
       "      <td>‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>RT @_Pb_swain_: ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ...</td>\n",
       "      <td>:  ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§Æ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üëá, üòÇ, üòÇ, üòÇ, üòÇ]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@_Pb_swain_]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[RT]</td>\n",
       "      <td>[backhand index pointing down, face with tears...</td>\n",
       "      <td>[]</td>\n",
       "      <td>RT  : ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...</td>\n",
       "      <td>‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä ,...</td>\n",
       "      <td>[#Maoist, #WestBengal]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/pP1AOvOv0b]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[maoist, west bengal]</td>\n",
       "      <td>‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>#Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...</td>\n",
       "      <td>#Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...</td>\n",
       "      <td>[#Breaking, #Sushantsinghcase, #Kangana]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/szOTZWq1hI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[breaking, sushantsinghcase, kangana]</td>\n",
       "      <td>-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ‡§ß‡§Æ‡§ï‡•Ä ‡§Æ‡§ø‡§≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>@BasudebaTripat4: @Rajanspsingh1 ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ...</td>\n",
       "      <td>:  ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ‡§≤‡•á ‡§ï‡§æ ‡§∏‡§∞ ‡§´‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ ,  ,  ‡§ó‡§∞‡•ç‡§¶‡§®...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@BasudebaTripat4, @Rajanspsingh1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>:   ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ‡§≤‡•á ‡§ï‡§æ ‡§∏‡§∞ ‡§´‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ,, ‡§ó‡§∞‡•ç‡§¶‡§® ‡§§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...</td>\n",
       "      <td>‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...</td>\n",
       "      <td>‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç , ‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤ ‡§∞...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[100, 100, 500, 1000]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tweet_id                                         full_tweet  \\\n",
       "0        1  ‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...   \n",
       "1        2  ‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...   \n",
       "2        3  ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...   \n",
       "3        4  ‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...   \n",
       "4        5  RT @_Pb_swain_: ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ...   \n",
       "5        6  ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...   \n",
       "6        7  #Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...   \n",
       "7        8  @BasudebaTripat4: @Rajanspsingh1 ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ...   \n",
       "8        9  ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...   \n",
       "9       10  ‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...   \n",
       "\n",
       "                                      tweet_raw_text  \\\n",
       "0  ‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...   \n",
       "1  ‡§™‡§ü‡§®‡§æ :  BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ...   \n",
       "2  ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä ,  ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞ ,  ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™...   \n",
       "3  ‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...   \n",
       "4  :  ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§Æ...   \n",
       "5  ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä ,...   \n",
       "6  #Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...   \n",
       "7  :  ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ‡§≤‡•á ‡§ï‡§æ ‡§∏‡§∞ ‡§´‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ ,  ,  ‡§ó‡§∞‡•ç‡§¶‡§®...   \n",
       "8  ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...   \n",
       "9  ‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç , ‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤ ‡§∞...   \n",
       "\n",
       "                                   hashtags smiley            emoji  \\\n",
       "0                                        []     []               []   \n",
       "1                                        []     []               []   \n",
       "2                                        []     []        [üôè, üòÇ, üëç]   \n",
       "3                                        []     []               []   \n",
       "4                                        []     []  [üëá, üòÇ, üòÇ, üòÇ, üòÇ]   \n",
       "5                    [#Maoist, #WestBengal]     []               []   \n",
       "6  [#Breaking, #Sushantsinghcase, #Kangana]     []               []   \n",
       "7                                        []     []               []   \n",
       "8                                        []     []               []   \n",
       "9                                        []     []               []   \n",
       "\n",
       "                         url                            mentions  \\\n",
       "0                         []                                  []   \n",
       "1  [https://t.co/Dq05hREifM]                   [@kumarprakash4u]   \n",
       "2                         []                                  []   \n",
       "3                         []                                  []   \n",
       "4                         []                       [@_Pb_swain_]   \n",
       "5  [https://t.co/pP1AOvOv0b]                                  []   \n",
       "6  [https://t.co/szOTZWq1hI]                                  []   \n",
       "7                         []  [@BasudebaTripat4, @Rajanspsingh1]   \n",
       "8                         []                                  []   \n",
       "9                         []                                  []   \n",
       "\n",
       "                numerals reserved_word  \\\n",
       "0     [20, 6, 10, 20, 6]            []   \n",
       "1                     []            []   \n",
       "2                     []            []   \n",
       "3                     []            []   \n",
       "4                     []          [RT]   \n",
       "5                     []            []   \n",
       "6                     []            []   \n",
       "7                     []            []   \n",
       "8                     []            []   \n",
       "9  [100, 100, 500, 1000]            []   \n",
       "\n",
       "                                             emotext  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [folded hands, face with tears of joy, thumbs up]   \n",
       "3                                                 []   \n",
       "4  [backhand index pointing down, face with tears...   \n",
       "5                                                 []   \n",
       "6                                                 []   \n",
       "7                                                 []   \n",
       "8                                                 []   \n",
       "9                                                 []   \n",
       "\n",
       "                          segmented_hash  \\\n",
       "0                                     []   \n",
       "1                                     []   \n",
       "2                                     []   \n",
       "3                                     []   \n",
       "4                                     []   \n",
       "5                  [maoist, west bengal]   \n",
       "6  [breaking, sushantsinghcase, kangana]   \n",
       "7                                     []   \n",
       "8                                     []   \n",
       "9                                     []   \n",
       "\n",
       "                                               clean  \n",
       "0  ‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...  \n",
       "1  ‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...  \n",
       "2  ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...  \n",
       "3  ‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...  \n",
       "4  RT  : ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ...  \n",
       "5  ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...  \n",
       "6   -‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ‡§ß‡§Æ‡§ï‡•Ä ‡§Æ‡§ø‡§≤...  \n",
       "7   :   ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ‡§≤‡•á ‡§ï‡§æ ‡§∏‡§∞ ‡§´‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ,, ‡§ó‡§∞‡•ç‡§¶‡§® ‡§§...  \n",
       "8  ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...  \n",
       "9  ‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train, valid])\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = labels[lab_num]\n",
    "# def label_encode(val):\n",
    "#     return labels.index(val)\n",
    "def label_encode(val):\n",
    "    val = val.split(',')\n",
    "    if lab_num == 4:\n",
    "        if lab in val:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        if lab in val:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = train.task_1.apply(label_encode)\n",
    "train['tweet'] = train.full_tweet\n",
    "test['tweet'] = test.full_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4172    0\n",
       "148     1\n",
       "3952    0\n",
       "3643    0\n",
       "866     1\n",
       "2338    1\n",
       "969     1\n",
       "2820    1\n",
       "5159    1\n",
       "2827    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "# train.tweet = train.tweet.apply(clean_text)\n",
    "# train.tweet = train.tweet.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.label = test.label.apply(label_encode)\n",
    "test = test.reset_index(drop=True)\n",
    "# test.tweet = test.tweet.apply(clean_text)\n",
    "# test.tweet = test.tweet.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4979    ‡§¶‡•á‡§∂ ‡§®‡•á ‡§∞‡§ï‡•ç‡§∑‡§æ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§¨‡§°‡§º‡•Ä ‡§∏‡§´‡§≤‡§§‡§æ ‡§π‡§æ‡§∏‡§ø‡§≤ ‡§ï‡•Ä ‡§π...\n",
       "3146    ‡§ï‡§Ç‡§ó‡§®‡§æ ‡§∞‡§®‡•å‡§§ ‡§î‡§∞ ‡§∂‡§ø‡§µ ‡§∏‡•á‡§®‡§æ ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§ú‡•Å‡§¨‡§æ‡§®‡•Ä ‡§ú‡§Ç‡§ó ‡•§ ‡§ï‡§Ç‡§ó...\n",
       "4966    ‡§Ö‡§¨ ‡§§‡§ï ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§Æ‡•á‡§Ç 41.07 ‡§≤‡§æ‡§ñ ‡§®‡§Æ‡•Ç‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§ú‡§æ‡§Ç‡§ö ‡§ï‡•Ä ‡§ú‡§æ...\n",
       "733     #RescueIndiaFromBJP ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡•á ‡§ï‡•Å‡§§‡•ç‡§§‡•ã‡§Ç ‡§®‡•á ‡§è‡§ï ...\n",
       "4642    BIHAR | ‡§ï‡•ã‡§∞‡•ã‡§®‡§æ ‡§Æ‡§π‡§æ‡§Æ‡§æ‡§∞‡•Ä ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§¨‡§ø‡§π‡§æ‡§∞ ‡§Æ‡•á‡§Ç 97 DSP...\n",
       "742     ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ ‡§™‡§∞‡•á‡§∂‡§æ‡§® ‡§®‡§æ ‡§π‡•ã‡§á‡§è ‡§´‡•ç‡§∞‡•Ä ‡§Æ‡•á‡§Ç ‡§Ö‡§ï‡§æ‡§â‡§Ç‡§ü ‡§Æ‡•á‡§Ç ‡§™‡•à‡§∏‡•á...\n",
       "2       ‡§∏‡•Å‡§∂‡§æ‡§Ç‡§§ ‡§®‡•á ‡§ú‡•ã ‡§¨‡§ø‡§ú‡§®‡•á‡§∏ ‡§°‡•Ä‡§≤ 9 ‡§ú‡•Ç‡§® ‡§ï‡•ã ‡§ï‡•Ä ‡§•‡•Ä, ‡§µ‡•ã ‡§°‡•Ä‡§≤...\n",
       "321     ‡§á‡§Ø‡•ã‡§® ‡§Æ‡•â‡§∞‡•ç‡§ó‡§® ‡§®‡•á ‡§â‡§Ç‡§ó‡§≤‡•Ä ‡§ü‡•Ç‡§ü‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§µ‡§ú‡•Ç‡§¶ ‡§ñ‡•á‡§≤‡§®‡§æ ‡§ú‡§æ‡§∞...\n",
       "477     ‡§∏‡•Å‡§®‡§ø‡§è ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§π‡§®‡§æ ‡§π‡•à ‡§â‡§∏ ‡§Æ‡•á‡§°‡§ø‡§∏‡§ø‡§® ‡§∏‡•ç‡§ü‡•ã‡§∞ ‡§ï‡•á ‡§Æ‡§æ‡§≤‡§ø‡§ï ‡§ï...\n",
       "4092    ‡§ú‡§ø‡§Ø‡•ã‡§Æ‡§æ‡§∞‡•ç‡§ü ‡§ï‡•Ä ‡§´‡•ç‡§∞‡•á‡§Ç‡§ö‡§æ‡§á‡§ú‡•Ä ‡§¶‡§ø‡§≤‡§µ‡§æ‡§®‡•á ‡§ï‡•á ‡§®‡§æ‡§Æ ‡§™‡§∞ ‡§ß‡•ã‡§ñ‡§æ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tweet.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['tweet'], train['label'])\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train['tweet'], train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    try:\n",
    "        return len(text.split())\n",
    "    except:\n",
    "        print(text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30.549323786793956, 2808, 23, 5028)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "maxw = 0\n",
    "large_count = 0\n",
    "for i in train_x:\n",
    "    temp = count_words(i)\n",
    "    total += temp\n",
    "    maxw = temp if temp > maxw else maxw\n",
    "    large_count += 1 if temp > 120 else 0\n",
    "total/len(train_x), maxw, large_count, len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_LENGTH = 50\n",
    "posts = train.values\n",
    "categories = train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.models as gsm\n",
    "e2v = gsm.KeyedVectors.load_word2vec_format('emoji2vec.bin', binary=True)\n",
    "# happy_vector = e2v['üòÇ']    # Produces an embedding vector of length 300\n",
    "\n",
    "# Download the bin file from here https://github.com/uclnlp/emoji2vec/blob/master/pre-trained/emoji2vec.bin\n",
    "\n",
    "def getEmojiEmbeddings(emojiList,dim=300,verbose = False):\n",
    "  \"\"\" Generates an emoji vector by averaging the emoji representation for each emoji. If no emoji returns an empty list of dimension dim\"\"\"\n",
    "  if dim < 300:\n",
    "    raise IndexError(\"Dim has to be greater than 300\")\n",
    "  result = np.zeros(dim)\n",
    "  if (len(emojiList) == 0):\n",
    "    return result\n",
    "  else:\n",
    "    embs = None\n",
    "    for i in emojiList:\n",
    "      if verbose:\n",
    "        if i not in e2v.vocab:\n",
    "          print(i)\n",
    "    embs = np.mean([e2v[i] for i in emojiList if i in e2v.vocab], axis=0)\n",
    "  if np.any(np.isnan(embs)):\n",
    "    return result\n",
    "  result[:300] = embs\n",
    "  return result\n",
    "getEmojiEmbeddings(valid.emoji.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2173: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([128]), torch.Size([300]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode_plus(\n",
    "            valid.full_tweet.values[0],\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )['input_ids']\n",
    "torch.tensor(ids, dtype=torch.long).shape, torch.tensor(getEmojiEmbeddings(valid.emoji.values[0]), dtype=torch.long).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len, t = False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.tweet\n",
    "        self.emoji = dataframe.emoji\n",
    "        self.hash = dataframe.segmented_hash\n",
    "        self.t = t\n",
    "        if not self.t:\n",
    "            self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        h_text = self.hash[index]\n",
    "        h_text = \" \".join(h_text)\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            h_text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        h_ids = inputs['input_ids']\n",
    "        h_mask = inputs['attention_mask']\n",
    "        h_token_type_ids = inputs[\"token_type_ids\"]\n",
    "#         h_inputs\n",
    "        emoji = getEmojiEmbeddings(self.emoji[index])\n",
    "        if self.t:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                'h_ids': torch.tensor(h_ids, dtype=torch.long),\n",
    "                'h_mask': torch.tensor(h_mask, dtype=torch.long),\n",
    "                'h_token_type_ids': torch.tensor(h_token_type_ids, dtype=torch.long),\n",
    "                'emoji' : torch.tensor(emoji, dtype=torch.long),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                'h_ids': torch.tensor(h_ids, dtype=torch.long),\n",
    "                'h_mask': torch.tensor(h_mask, dtype=torch.long),\n",
    "                'h_token_type_ids': torch.tensor(h_token_type_ids, dtype=torch.long),\n",
    "                'emoji' : torch.tensor(emoji, dtype=torch.long),\n",
    "                'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (6286, 16)\n",
      "TRAIN Dataset: (5343, 16)\n",
      "TEST Dataset: (943, 16)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.85\n",
    "train_data=train.sample(frac=train_size,random_state=200)\n",
    "test_data=train.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(train.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = MultiLabelDataset(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = MultiLabelDataset(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(200000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (l2): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(200000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (pre_classifier_1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (pre_classifier_2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (pre_classifier_3): Linear(in_features=1836, out_features=1836, bias=True)\n",
       "  (classifier): Linear(in_features=1836, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(models[model_num])\n",
    "        self.l2 = AutoModel.from_pretrained(models[model_num])\n",
    "        \n",
    "        self.pre_classifier_1 = torch.nn.Linear(768, 768)\n",
    "        self.pre_classifier_2 = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.pre_classifier_3 = torch.nn.Linear(1836, 1836)\n",
    "#         self.pre_classifier_3 = torch.nn.Linear(768, 100)\n",
    "        self.classifier = torch.nn.Linear(1836, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state_1 = output_1[0]\n",
    "        pooler_1 = hidden_state_1[:, 0]\n",
    "        pooler_1 = self.pre_classifier_1(pooler_1)\n",
    "        pooler_1 = torch.nn.Tanh()(pooler_1)\n",
    "        pooler_1 = self.dropout(pooler_1)\n",
    "        output_2 = self.l2(input_ids=h_ids, attention_mask=h_mask)\n",
    "        hidden_state_2 = output_2[0]\n",
    "        pooler_2 = hidden_state_2[:, 0]\n",
    "        pooler_2 = self.pre_classifier_2(pooler_2)\n",
    "        pooler_2 = torch.nn.Tanh()(pooler_2)\n",
    "        pooler_2 = self.dropout(pooler_2)\n",
    "        pooler_3 = torch.cat((pooler_1, pooler_2), 1)\n",
    "        pooler_3 = torch.cat((pooler_3, emoji), 1)\n",
    "#         print(pooler_1.shape,hidden_state_1.shape, pooler_2.shape, emoji.type(torch.FloatTensor).shape)\n",
    "#         pooler_3 = torch.nn.Tanh()(emoji.type(torch.FloatTensor))\n",
    "#         pooler_3 = self.dropout(pooler_3)\n",
    "#         print(pooler_3.shape)\n",
    "        pooler_3 = self.pre_classifier_3(pooler_3)\n",
    "#         pooler_3 = self.pre_classifier_3(pooler_2)\n",
    "        pooler_3 = torch.nn.Tanh()(pooler_3)\n",
    "        pooler_3 = self.dropout(pooler_3)\n",
    "        output = self.classifier(pooler_3)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# print(repr(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 58 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "l1.embeddings.word_embeddings.weight                    (200000, 128)\n",
      "l1.embeddings.position_embeddings.weight                  (512, 128)\n",
      "l1.embeddings.token_type_embeddings.weight                  (2, 128)\n",
      "l1.embeddings.LayerNorm.weight                                (128,)\n",
      "l1.embeddings.LayerNorm.bias                                  (128,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "l1.encoder.embedding_hidden_mapping_in.weight             (768, 128)\n",
      "l1.encoder.embedding_hidden_mapping_in.bias                   (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight   (768, 768)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight   (768, 768)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight   (768, 768)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight   (768, 768)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias       (768,)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight  (3072, 768)\n",
      "l1.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias      (3072,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "pre_classifier_3.weight                                 (1836, 1836)\n",
      "pre_classifier_3.bias                                        (1836,)\n",
      "classifier.weight                                          (2, 1836)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    total_train_loss = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        h_ids = data['h_ids'].to(device, dtype = torch.long)\n",
    "        h_mask = data['h_mask'].to(device, dtype = torch.long)\n",
    "        h_token_type_ids = data['h_token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        emoji = data['emoji'].to(device, dtype = torch.long)\n",
    "        outputs = model(ids, mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji)\n",
    "        optimizer.zero_grad()\n",
    "#         loss = outputs.loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "#         if _%50==0:\n",
    "#             print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        total_train_loss += loss.item()\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    print(f'Epoch: {epoch}, Loss:  {total_train_loss/count}')\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            h_ids = data['h_ids'].to(device, dtype = torch.long)\n",
    "            h_mask = data['h_mask'].to(device, dtype = torch.long)\n",
    "            h_token_type_ids = data['h_token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            emoji = data['emoji'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    fin_outputs = list(np.argmax(np.array(fin_outputs), axis=1).flatten())\n",
    "    print(classification_report(fin_targets, fin_outputs))\n",
    "    torch.save(model, '../temp/finetuned/'+lab+'_epoch_'+str(epoch))\n",
    "    return fin_outputs, fin_targets\n",
    "#     final_outputs = np.array(fin_outputs) >=0.5\n",
    "#     final = []\n",
    "#     final_t = []\n",
    "#     final_fine = [[],[],[],[]]\n",
    "#     final_fine_t = [[],[],[],[]]\n",
    "#     for (i,j) in zip(final_outputs, fin_targets):\n",
    "#         output_sum = sum(i)\n",
    "#         target_sum = sum(j)\n",
    "#         if output_sum == 0:\n",
    "#             final.append(0)\n",
    "#         else:\n",
    "#             final.append(1)\n",
    "#         if target_sum == 0:\n",
    "#             final_t.append(0)\n",
    "#         else:\n",
    "#             final_t.append(1)\n",
    "#         for p in range(4):\n",
    "#             final_fine[p].append(int(i[p]))\n",
    "#             final_fine_t[p].append(int(j[p]))\n",
    "#     print(\"Coarse:\")\n",
    "#     print(classification_report(final, final_t))\n",
    "#     for i in range(4):\n",
    "#         print(\"Fine\", i)\n",
    "    \n",
    "#     return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(EPOCHS):\n",
    "#     out, tar = train(epoch)\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[0:10], tar[0:10]hjkhnk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "# train_size = 0.8\n",
    "# test_data=test.sample(frac=1,random_state=200)\n",
    "# test_data=train.drop(train_data.index).reset_index(drop=True)\n",
    "test_data = test.reset_index(drop=True)\n",
    "testing = MultiLabelDataset(test_data, tokenizer, MAX_LEN, t=True)\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "testing_loader = DataLoader(testing, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan  3 18:04:43 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.95.01    Driver Version: 440.95.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 31%   24C    P2    45W / 250W |   6555MiB / 11019MiB |      4%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 29%   22C    P8    18W / 250W |     11MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 29%   22C    P8    28W / 250W |     11MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:83:00.0 Off |                  N/A |\n",
      "| 29%   23C    P8     5W / 250W |     11MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     14499      C   ...hagata.raha/anaconda/envs/p3/bin/python  5243MiB |\n",
      "|    0     15400      C   ...hagata.raha/anaconda/envs/p3/bin/python  1301MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.79it/s]/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "51it [00:08,  5.73it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fin_targets=[]\n",
    "fin_outputs=[]\n",
    "# print(f'Epoch: {epoch}, Loss:  {total_train_loss/count}')\n",
    "with torch.no_grad():\n",
    "    model = torch.load(epoch_name, map_location=device)\n",
    "    model.eval()\n",
    "    for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        h_ids = data['h_ids'].to(device, dtype = torch.long)\n",
    "        h_mask = data['h_mask'].to(device, dtype = torch.long)\n",
    "        h_token_type_ids = data['h_token_type_ids'].to(device, dtype = torch.long)\n",
    "#         targets = data['targets'].to(device, dtype = torch.long)\n",
    "        emoji = data['emoji'].to(device, dtype = torch.long)\n",
    "        outputs = model(ids, mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji)\n",
    "#         fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "        fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "fin_outputs = list(np.argmax(np.array(fin_outputs), axis=1).flatten())\n",
    "# print(classification_report(fin_outputs, fin_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_outputs[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>full_tweet</th>\n",
       "      <th>tweet_raw_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>smiley</th>\n",
       "      <th>emoji</th>\n",
       "      <th>url</th>\n",
       "      <th>mentions</th>\n",
       "      <th>numerals</th>\n",
       "      <th>reserved_word</th>\n",
       "      <th>emotext</th>\n",
       "      <th>segmented_hash</th>\n",
       "      <th>clean</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...</td>\n",
       "      <td>‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[20, 6, 10, 20, 6]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...</td>\n",
       "      <td>‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...</td>\n",
       "      <td>‡§™‡§ü‡§®‡§æ :  BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/Dq05hREifM]</td>\n",
       "      <td>[@kumarprakash4u]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...</td>\n",
       "      <td>‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...</td>\n",
       "      <td>‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä ,  ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞ ,  ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üôè, üòÇ, üëç]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[folded hands, face with tears of joy, thumbs up]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...</td>\n",
       "      <td>‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...</td>\n",
       "      <td>‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...</td>\n",
       "      <td>‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>RT @_Pb_swain_: ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ...</td>\n",
       "      <td>:  ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§Æ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üëá, üòÇ, üòÇ, üòÇ, üòÇ]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@_Pb_swain_]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[RT]</td>\n",
       "      <td>[backhand index pointing down, face with tears...</td>\n",
       "      <td>[]</td>\n",
       "      <td>RT  : ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ...</td>\n",
       "      <td>RT @_Pb_swain_: ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...</td>\n",
       "      <td>‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä ,...</td>\n",
       "      <td>[#Maoist, #WestBengal]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/pP1AOvOv0b]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[maoist, west bengal]</td>\n",
       "      <td>‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...</td>\n",
       "      <td>‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>#Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...</td>\n",
       "      <td>#Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...</td>\n",
       "      <td>[#Breaking, #Sushantsinghcase, #Kangana]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/szOTZWq1hI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[breaking, sushantsinghcase, kangana]</td>\n",
       "      <td>-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ‡§ß‡§Æ‡§ï‡•Ä ‡§Æ‡§ø‡§≤...</td>\n",
       "      <td>#Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>@BasudebaTripat4: @Rajanspsingh1 ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ...</td>\n",
       "      <td>:  ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ‡§≤‡•á ‡§ï‡§æ ‡§∏‡§∞ ‡§´‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ ,  ,  ‡§ó‡§∞‡•ç‡§¶‡§®...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@BasudebaTripat4, @Rajanspsingh1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>:   ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ‡§≤‡•á ‡§ï‡§æ ‡§∏‡§∞ ‡§´‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ,, ‡§ó‡§∞‡•ç‡§¶‡§® ‡§§...</td>\n",
       "      <td>@BasudebaTripat4: @Rajanspsingh1 ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...</td>\n",
       "      <td>‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...</td>\n",
       "      <td>‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...</td>\n",
       "      <td>‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç , ‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤ ‡§∞...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[100, 100, 500, 1000]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...</td>\n",
       "      <td>‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tweet_id                                         full_tweet  \\\n",
       "0        1  ‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...   \n",
       "1        2  ‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...   \n",
       "2        3  ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...   \n",
       "3        4  ‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...   \n",
       "4        5  RT @_Pb_swain_: ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ...   \n",
       "5        6  ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...   \n",
       "6        7  #Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...   \n",
       "7        8  @BasudebaTripat4: @Rajanspsingh1 ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ...   \n",
       "8        9  ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...   \n",
       "9       10  ‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...   \n",
       "\n",
       "                                      tweet_raw_text  \\\n",
       "0  ‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...   \n",
       "1  ‡§™‡§ü‡§®‡§æ :  BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ...   \n",
       "2  ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä ,  ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞ ,  ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™...   \n",
       "3  ‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...   \n",
       "4  :  ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§Æ...   \n",
       "5  ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä ,...   \n",
       "6  #Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...   \n",
       "7  :  ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ‡§≤‡•á ‡§ï‡§æ ‡§∏‡§∞ ‡§´‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ ,  ,  ‡§ó‡§∞‡•ç‡§¶‡§®...   \n",
       "8  ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...   \n",
       "9  ‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç , ‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤ ‡§∞...   \n",
       "\n",
       "                                   hashtags smiley            emoji  \\\n",
       "0                                        []     []               []   \n",
       "1                                        []     []               []   \n",
       "2                                        []     []        [üôè, üòÇ, üëç]   \n",
       "3                                        []     []               []   \n",
       "4                                        []     []  [üëá, üòÇ, üòÇ, üòÇ, üòÇ]   \n",
       "5                    [#Maoist, #WestBengal]     []               []   \n",
       "6  [#Breaking, #Sushantsinghcase, #Kangana]     []               []   \n",
       "7                                        []     []               []   \n",
       "8                                        []     []               []   \n",
       "9                                        []     []               []   \n",
       "\n",
       "                         url                            mentions  \\\n",
       "0                         []                                  []   \n",
       "1  [https://t.co/Dq05hREifM]                   [@kumarprakash4u]   \n",
       "2                         []                                  []   \n",
       "3                         []                                  []   \n",
       "4                         []                       [@_Pb_swain_]   \n",
       "5  [https://t.co/pP1AOvOv0b]                                  []   \n",
       "6  [https://t.co/szOTZWq1hI]                                  []   \n",
       "7                         []  [@BasudebaTripat4, @Rajanspsingh1]   \n",
       "8                         []                                  []   \n",
       "9                         []                                  []   \n",
       "\n",
       "                numerals reserved_word  \\\n",
       "0     [20, 6, 10, 20, 6]            []   \n",
       "1                     []            []   \n",
       "2                     []            []   \n",
       "3                     []            []   \n",
       "4                     []          [RT]   \n",
       "5                     []            []   \n",
       "6                     []            []   \n",
       "7                     []            []   \n",
       "8                     []            []   \n",
       "9  [100, 100, 500, 1000]            []   \n",
       "\n",
       "                                             emotext  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [folded hands, face with tears of joy, thumbs up]   \n",
       "3                                                 []   \n",
       "4  [backhand index pointing down, face with tears...   \n",
       "5                                                 []   \n",
       "6                                                 []   \n",
       "7                                                 []   \n",
       "8                                                 []   \n",
       "9                                                 []   \n",
       "\n",
       "                          segmented_hash  \\\n",
       "0                                     []   \n",
       "1                                     []   \n",
       "2                                     []   \n",
       "3                                     []   \n",
       "4                                     []   \n",
       "5                  [maoist, west bengal]   \n",
       "6  [breaking, sushantsinghcase, kangana]   \n",
       "7                                     []   \n",
       "8                                     []   \n",
       "9                                     []   \n",
       "\n",
       "                                               clean  \\\n",
       "0  ‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...   \n",
       "1  ‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...   \n",
       "2  ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...   \n",
       "3  ‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...   \n",
       "4  RT  : ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ...   \n",
       "5  ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...   \n",
       "6   -‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ‡§ß‡§Æ‡§ï‡•Ä ‡§Æ‡§ø‡§≤...   \n",
       "7   :   ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ‡§≤‡•á ‡§ï‡§æ ‡§∏‡§∞ ‡§´‡•ã‡§°‡§º ‡§¶‡§ø‡§Ø‡§æ,, ‡§ó‡§∞‡•ç‡§¶‡§® ‡§§...   \n",
       "8  ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...   \n",
       "9  ‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...   \n",
       "\n",
       "                                               tweet  \n",
       "0  ‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π...  \n",
       "1  ‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á...  \n",
       "2  ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä...  \n",
       "3  ‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ...  \n",
       "4  RT @_Pb_swain_: ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ...  \n",
       "5  ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§´‡§ø‡§∞ ‡§π‡•Å‡§à ‡§Æ‡§æ‡§ì‡§µ‡§æ‡§¶‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•Ä ‡§µ‡§æ‡§™‡§∏‡•Ä, ...  \n",
       "6  #Breaking-‡§ï‡§Ç‡§ó‡§®‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§™‡§∞ ‡§¨‡•ã‡§≤‡•á ‡§Æ‡§®‡•ã‡§ú ‡§§‡§ø‡§µ‡§æ‡§∞‡•Ä-‡§ï‡§π‡§æ ...  \n",
       "7  @BasudebaTripat4: @Rajanspsingh1 ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§ï‡§ø‡§Ø‡§æ ‡§∏‡§æ...  \n",
       "8  ‡§π‡•à‡§¶‡§∞‡§æ‡§¨‡§æ‡§¶ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§Ø‡§ï ‡§∞‡§æ‡§ú‡§æ ‡§∏‡§ø‡§Ç‡§π ‡§ï‡•Ä ‡§¨‡§π‡§® ‡§Æ‡§æ‡§Ø‡§æ ‡§¶...  \n",
       "9  ‡§ï‡§Æ‡§≤‡§®‡§æ‡§• ‡§ï‡•á ‡§∞‡§æ‡§ú ‡§Æ‡•á‡§Ç 100,‚Çπ ‡§Æ‡•á‡§Ç 100‡§Ø‡•Ç‡§®‡§ø‡§ü ‡§¨‡§ø‡§ú‡§≤‡•Ä ‡§Æ‡§ø‡§≤...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = np.array(fin_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1630"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fin_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1630"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.full_tweet.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>full_tweet</th>\n",
       "      <th>tweet_raw_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>smiley</th>\n",
       "      <th>emoji</th>\n",
       "      <th>url</th>\n",
       "      <th>mentions</th>\n",
       "      <th>numerals</th>\n",
       "      <th>reserved_word</th>\n",
       "      <th>emotext</th>\n",
       "      <th>segmented_hash</th>\n",
       "      <th>clean</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>569</td>\n",
       "      <td>‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§≠‡§æ‡§∞‡§§-‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§≠‡§æ‡§∞‡§§' ‡§ï‡•á ‡§∏‡§Ç‡§ï‡§≤‡•ç‡§™ ‡§ï‡•á ‡§∏‡§æ‡§• #Swa...</td>\n",
       "      <td>‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§≠‡§æ‡§∞‡§§-‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§≠‡§æ‡§∞‡§§' ‡§ï‡•á ‡§∏‡§Ç‡§ï‡§≤‡•ç‡§™ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Æ‡•á‡§Ç ...</td>\n",
       "      <td>[#SwachhSurvekshan2020]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@narendramodi]</td>\n",
       "      <td>[19]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[swachh survekshan 2020]</td>\n",
       "      <td>‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§≠‡§æ‡§∞‡§§-‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§≠‡§æ‡§∞‡§§' ‡§ï‡•á ‡§∏‡§Ç‡§ï‡§≤‡•ç‡§™ ‡§ï‡•á ‡§∏‡§æ‡§•   ‡§Æ‡•á...</td>\n",
       "      <td>‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§≠‡§æ‡§∞‡§§-‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§≠‡§æ‡§∞‡§§' ‡§ï‡•á ‡§∏‡§Ç‡§ï‡§≤‡•ç‡§™ ‡§ï‡•á ‡§∏‡§æ‡§• #Swa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>1215</td>\n",
       "      <td>‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä Jio ‡§∏‡§ø‡§Æ ‡§ï‡•á ‡§è‡§° ‡§∏‡•á ‡§∂‡§æ‡§π‡§∞‡•Å‡§ñ ‡§ñ‡§æ‡§® ‡§ï‡•ã ‡§®‡§ø‡§ï‡§æ‡§≤ ...</td>\n",
       "      <td>‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä Jio ‡§∏‡§ø‡§Æ ‡§ï‡•á ‡§è‡§° ‡§∏‡•á ‡§∂‡§æ‡§π‡§∞‡•Å‡§ñ ‡§ñ‡§æ‡§® ‡§ï‡•ã ‡§®‡§ø‡§ï‡§æ‡§≤ ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä Jio ‡§∏‡§ø‡§Æ ‡§ï‡•á ‡§è‡§° ‡§∏‡•á ‡§∂‡§æ‡§π‡§∞‡•Å‡§ñ ‡§ñ‡§æ‡§® ‡§ï‡•ã ‡§®‡§ø‡§ï‡§æ‡§≤ ...</td>\n",
       "      <td>‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä Jio ‡§∏‡§ø‡§Æ ‡§ï‡•á ‡§è‡§° ‡§∏‡•á ‡§∂‡§æ‡§π‡§∞‡•Å‡§ñ ‡§ñ‡§æ‡§® ‡§ï‡•ã ‡§®‡§ø‡§ï‡§æ‡§≤ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>269</td>\n",
       "      <td>@LakheraSatish: ‡§¶‡•á‡§ñ ‡§ö‡•Ä‡§®‡•Ä ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•Ä ‡§ï‡§¨‡•ç‡§∞, ‡§ï‡§æ‡§Ç‡§ó...</td>\n",
       "      <td>:  ‡§¶‡•á‡§ñ ‡§ö‡•Ä‡§®‡•Ä ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•Ä ‡§ï‡§¨‡•ç‡§∞ ,  ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§ü‡•Ç‡§ü...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@LakheraSatish]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>: ‡§¶‡•á‡§ñ ‡§ö‡•Ä‡§®‡•Ä ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•Ä ‡§ï‡§¨‡•ç‡§∞, ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§ü‡•Ç‡§ü ‡§∞...</td>\n",
       "      <td>@LakheraSatish: ‡§¶‡•á‡§ñ ‡§ö‡•Ä‡§®‡•Ä ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•Ä ‡§ï‡§¨‡•ç‡§∞, ‡§ï‡§æ‡§Ç‡§ó...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>197</td>\n",
       "      <td>‡§™‡§§‡•ç‡§∞ ‡§ï‡•á ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§∏‡•á‡§®‡§æ ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§®‡§µ ‡§µ‡§∞‡•ç‡§∑ ‡§ï‡•Ä ‡§∂‡•Å‡§≠...</td>\n",
       "      <td>‡§™‡§§‡•ç‡§∞ ‡§ï‡•á ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§∏‡•á‡§®‡§æ ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§®‡§µ ‡§µ‡§∞‡•ç‡§∑ ‡§ï‡•Ä ‡§∂‡•Å‡§≠...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§™‡§§‡•ç‡§∞ ‡§ï‡•á ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§∏‡•á‡§®‡§æ ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§®‡§µ ‡§µ‡§∞‡•ç‡§∑ ‡§ï‡•Ä ‡§∂‡•Å‡§≠...</td>\n",
       "      <td>‡§™‡§§‡•ç‡§∞ ‡§ï‡•á ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§∏‡•á‡§®‡§æ ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§®‡§µ ‡§µ‡§∞‡•ç‡§∑ ‡§ï‡•Ä ‡§∂‡•Å‡§≠...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>1140</td>\n",
       "      <td>Redmi 9 Prime ‡§î‡§∞ Redmi 9 ‡§ï‡•Ä ‡§∏‡•á‡§≤ ‡§Ü‡§ú ‡§¶‡•ã‡§™‡§π‡§∞ 12 ‡§¨‡§ú...</td>\n",
       "      <td>Redmi Prime ‡§î‡§∞ Redmi ‡§ï‡•Ä ‡§∏‡•á‡§≤ ‡§Ü‡§ú ‡§¶‡•ã‡§™‡§π‡§∞ ‡§¨‡§ú‡•á ,  ‡§ú‡§æ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/KzbthV3juh]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[9, 9, 12]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Redmi 9 Prime ‡§î‡§∞ Redmi 9 ‡§ï‡•Ä ‡§∏‡•á‡§≤ ‡§Ü‡§ú ‡§¶‡•ã‡§™‡§π‡§∞ 12 ‡§¨‡§ú...</td>\n",
       "      <td>Redmi 9 Prime ‡§î‡§∞ Redmi 9 ‡§ï‡•Ä ‡§∏‡•á‡§≤ ‡§Ü‡§ú ‡§¶‡•ã‡§™‡§π‡§∞ 12 ‡§¨‡§ú...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>‡§≠‡§æ‡§∞‡§§ ‡§®‡•á ‡§ö‡•Ä‡§® ‡§∏‡•á ‡§∏‡•Ä‡§Æ‡§æ ‡§™‡§∞ ‡§§‡•à‡§®‡§æ‡§§ ‡§â‡§∏‡§ï‡•á ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§Ö...</td>\n",
       "      <td>‡§≠‡§æ‡§∞‡§§ ‡§®‡•á ‡§ö‡•Ä‡§® ‡§∏‡•á ‡§∏‡•Ä‡§Æ‡§æ ‡§™‡§∞ ‡§§‡•à‡§®‡§æ‡§§ ‡§â‡§∏‡§ï‡•á ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§Ö...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§≠‡§æ‡§∞‡§§ ‡§®‡•á ‡§ö‡•Ä‡§® ‡§∏‡•á ‡§∏‡•Ä‡§Æ‡§æ ‡§™‡§∞ ‡§§‡•à‡§®‡§æ‡§§ ‡§â‡§∏‡§ï‡•á ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§Ö...</td>\n",
       "      <td>‡§≠‡§æ‡§∞‡§§ ‡§®‡•á ‡§ö‡•Ä‡§® ‡§∏‡•á ‡§∏‡•Ä‡§Æ‡§æ ‡§™‡§∞ ‡§§‡•à‡§®‡§æ‡§§ ‡§â‡§∏‡§ï‡•á ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§Ö...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>105</td>\n",
       "      <td>‡§¶‡§ø‡§≤ ‡§¶‡§π‡§≤‡§æ ‡§¶‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§ñ‡§¨‡§∞‡•§ 6 ‡§∏‡§æ‡§≤ ‡§ï‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ...</td>\n",
       "      <td>‡§¶‡§ø‡§≤ ‡§¶‡§π‡§≤‡§æ ‡§¶‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§ñ‡§¨‡§∞‡•§ ‡§∏‡§æ‡§≤ ‡§ï‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∞‡•á...</td>\n",
       "      <td>[#‡§ú]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[‡§ú]</td>\n",
       "      <td>‡§¶‡§ø‡§≤ ‡§¶‡§π‡§≤‡§æ ‡§¶‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§ñ‡§¨‡§∞‡•§ 6 ‡§∏‡§æ‡§≤ ‡§ï‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ...</td>\n",
       "      <td>‡§¶‡§ø‡§≤ ‡§¶‡§π‡§≤‡§æ ‡§¶‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§ñ‡§¨‡§∞‡•§ 6 ‡§∏‡§æ‡§≤ ‡§ï‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>1079</td>\n",
       "      <td>‡§Æ‡•â‡§Æ-‡§°‡•à‡§° ‡§¨‡§®‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§π‡•à‡§Ç ‡§µ‡§ø‡§∞‡•Å‡§∑‡•ç‡§ï‡§æ, ‡§µ‡§ø‡§∞‡§æ‡§ü ‡§¨‡•ã‡§≤‡•á- ‡§Ö‡§µ...</td>\n",
       "      <td>‡§Æ‡•â‡§Æ-‡§°‡•à‡§° ‡§¨‡§®‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§π‡•à‡§Ç ‡§µ‡§ø‡§∞‡•Å‡§∑‡•ç‡§ï‡§æ ,  ‡§µ‡§ø‡§∞‡§æ‡§ü ‡§¨‡•ã‡§≤‡•á- ...</td>\n",
       "      <td>[#virushka, #RCB, #TeamIndia, #ViratKohli]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/8x4k3Mkk1g, https://t.co/YJBZvv4...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[virus hk a, rcb, team india, virat kohli]</td>\n",
       "      <td>‡§Æ‡•â‡§Æ-‡§°‡•à‡§° ‡§¨‡§®‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§π‡•à‡§Ç ‡§µ‡§ø‡§∞‡•Å‡§∑‡•ç‡§ï‡§æ, ‡§µ‡§ø‡§∞‡§æ‡§ü ‡§¨‡•ã‡§≤‡•á- ‡§Ö‡§µ...</td>\n",
       "      <td>‡§Æ‡•â‡§Æ-‡§°‡•à‡§° ‡§¨‡§®‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§π‡•à‡§Ç ‡§µ‡§ø‡§∞‡•Å‡§∑‡•ç‡§ï‡§æ, ‡§µ‡§ø‡§∞‡§æ‡§ü ‡§¨‡•ã‡§≤‡•á- ‡§Ö‡§µ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>940</td>\n",
       "      <td>#CoronaUpdates : ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡§Ç‡§ï‡•ç‡§∞‡§Æ‡§ø‡§§‡•ã‡§Ç ‡§µ‡§æ‡§≤‡§æ ‡§¶‡•Ç‡§∏...</td>\n",
       "      <td>:  ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡§Ç‡§ï‡•ç‡§∞‡§Æ‡§ø‡§§‡•ã‡§Ç ‡§µ‡§æ‡§≤‡§æ ‡§¶‡•Ç‡§∏‡§∞‡§æ ‡§¶‡•á‡§∂ ‡§¨‡§®‡§æ ‡§≠‡§æ‡§∞...</td>\n",
       "      <td>[#CoronaUpdates, #CoronavirusIndia, #CoronaUpd...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/BRGQsNNtEe]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[corona updates, coronavirus india, corona upd...</td>\n",
       "      <td>: ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡§Ç‡§ï‡•ç‡§∞‡§Æ‡§ø‡§§‡•ã‡§Ç ‡§µ‡§æ‡§≤‡§æ ‡§¶‡•Ç‡§∏‡§∞‡§æ ‡§¶‡•á‡§∂ ‡§¨‡§®‡§æ ‡§≠‡§æ...</td>\n",
       "      <td>#CoronaUpdates : ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡§Ç‡§ï‡•ç‡§∞‡§Æ‡§ø‡§§‡•ã‡§Ç ‡§µ‡§æ‡§≤‡§æ ‡§¶‡•Ç‡§∏...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>494</td>\n",
       "      <td>@rashtrapatibhvn @narendramodi ‡§™‡§æ‡§™‡•Ä ‡§Ö‡§ß‡§∞‡•ç‡§Æ‡•Ä  ‡§Æ‡§®...</td>\n",
       "      <td>‡§™‡§æ‡§™‡•Ä ‡§Ö‡§ß‡§∞‡•ç‡§Æ‡•Ä  ‡§Æ‡§®‡§π‡•Ç‡§∏ ‡§ï‡§≤‡§Ç‡§ï ‡§ï‡•ã ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§ï‡•Ä ‡§¨‡§ß‡§æ‡§à ‡§¶‡•á‡§®...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@rashtrapatibhvn, @narendramodi]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>‡§™‡§æ‡§™‡•Ä ‡§Ö‡§ß‡§∞‡•ç‡§Æ‡•Ä  ‡§Æ‡§®‡§π‡•Ç‡§∏ ‡§ï‡§≤‡§Ç‡§ï ‡§ï‡•ã ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§ï‡•Ä ‡§¨‡§ß‡§æ‡§à...</td>\n",
       "      <td>@rashtrapatibhvn @narendramodi ‡§™‡§æ‡§™‡•Ä ‡§Ö‡§ß‡§∞‡•ç‡§Æ‡•Ä  ‡§Æ‡§®...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id                                         full_tweet  \\\n",
       "545       569  ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§≠‡§æ‡§∞‡§§-‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§≠‡§æ‡§∞‡§§' ‡§ï‡•á ‡§∏‡§Ç‡§ï‡§≤‡•ç‡§™ ‡§ï‡•á ‡§∏‡§æ‡§• #Swa...   \n",
       "1191     1215  ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä Jio ‡§∏‡§ø‡§Æ ‡§ï‡•á ‡§è‡§° ‡§∏‡•á ‡§∂‡§æ‡§π‡§∞‡•Å‡§ñ ‡§ñ‡§æ‡§® ‡§ï‡•ã ‡§®‡§ø‡§ï‡§æ‡§≤ ...   \n",
       "245       269  @LakheraSatish: ‡§¶‡•á‡§ñ ‡§ö‡•Ä‡§®‡•Ä ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•Ä ‡§ï‡§¨‡•ç‡§∞, ‡§ï‡§æ‡§Ç‡§ó...   \n",
       "173       197  ‡§™‡§§‡•ç‡§∞ ‡§ï‡•á ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§∏‡•á‡§®‡§æ ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§®‡§µ ‡§µ‡§∞‡•ç‡§∑ ‡§ï‡•Ä ‡§∂‡•Å‡§≠...   \n",
       "1116     1140  Redmi 9 Prime ‡§î‡§∞ Redmi 9 ‡§ï‡•Ä ‡§∏‡•á‡§≤ ‡§Ü‡§ú ‡§¶‡•ã‡§™‡§π‡§∞ 12 ‡§¨‡§ú...   \n",
       "78         79  ‡§≠‡§æ‡§∞‡§§ ‡§®‡•á ‡§ö‡•Ä‡§® ‡§∏‡•á ‡§∏‡•Ä‡§Æ‡§æ ‡§™‡§∞ ‡§§‡•à‡§®‡§æ‡§§ ‡§â‡§∏‡§ï‡•á ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§Ö...   \n",
       "104       105  ‡§¶‡§ø‡§≤ ‡§¶‡§π‡§≤‡§æ ‡§¶‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§ñ‡§¨‡§∞‡•§ 6 ‡§∏‡§æ‡§≤ ‡§ï‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ...   \n",
       "1055     1079  ‡§Æ‡•â‡§Æ-‡§°‡•à‡§° ‡§¨‡§®‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§π‡•à‡§Ç ‡§µ‡§ø‡§∞‡•Å‡§∑‡•ç‡§ï‡§æ, ‡§µ‡§ø‡§∞‡§æ‡§ü ‡§¨‡•ã‡§≤‡•á- ‡§Ö‡§µ...   \n",
       "916       940  #CoronaUpdates : ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡§Ç‡§ï‡•ç‡§∞‡§Æ‡§ø‡§§‡•ã‡§Ç ‡§µ‡§æ‡§≤‡§æ ‡§¶‡•Ç‡§∏...   \n",
       "470       494  @rashtrapatibhvn @narendramodi ‡§™‡§æ‡§™‡•Ä ‡§Ö‡§ß‡§∞‡•ç‡§Æ‡•Ä  ‡§Æ‡§®...   \n",
       "\n",
       "                                         tweet_raw_text  \\\n",
       "545   ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§≠‡§æ‡§∞‡§§-‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§≠‡§æ‡§∞‡§§' ‡§ï‡•á ‡§∏‡§Ç‡§ï‡§≤‡•ç‡§™ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Æ‡•á‡§Ç ...   \n",
       "1191  ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä Jio ‡§∏‡§ø‡§Æ ‡§ï‡•á ‡§è‡§° ‡§∏‡•á ‡§∂‡§æ‡§π‡§∞‡•Å‡§ñ ‡§ñ‡§æ‡§® ‡§ï‡•ã ‡§®‡§ø‡§ï‡§æ‡§≤ ...   \n",
       "245   :  ‡§¶‡•á‡§ñ ‡§ö‡•Ä‡§®‡•Ä ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•Ä ‡§ï‡§¨‡•ç‡§∞ ,  ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§ü‡•Ç‡§ü...   \n",
       "173   ‡§™‡§§‡•ç‡§∞ ‡§ï‡•á ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§∏‡•á‡§®‡§æ ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§®‡§µ ‡§µ‡§∞‡•ç‡§∑ ‡§ï‡•Ä ‡§∂‡•Å‡§≠...   \n",
       "1116  Redmi Prime ‡§î‡§∞ Redmi ‡§ï‡•Ä ‡§∏‡•á‡§≤ ‡§Ü‡§ú ‡§¶‡•ã‡§™‡§π‡§∞ ‡§¨‡§ú‡•á ,  ‡§ú‡§æ...   \n",
       "78    ‡§≠‡§æ‡§∞‡§§ ‡§®‡•á ‡§ö‡•Ä‡§® ‡§∏‡•á ‡§∏‡•Ä‡§Æ‡§æ ‡§™‡§∞ ‡§§‡•à‡§®‡§æ‡§§ ‡§â‡§∏‡§ï‡•á ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§Ö...   \n",
       "104   ‡§¶‡§ø‡§≤ ‡§¶‡§π‡§≤‡§æ ‡§¶‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§ñ‡§¨‡§∞‡•§ ‡§∏‡§æ‡§≤ ‡§ï‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∞‡•á...   \n",
       "1055  ‡§Æ‡•â‡§Æ-‡§°‡•à‡§° ‡§¨‡§®‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§π‡•à‡§Ç ‡§µ‡§ø‡§∞‡•Å‡§∑‡•ç‡§ï‡§æ ,  ‡§µ‡§ø‡§∞‡§æ‡§ü ‡§¨‡•ã‡§≤‡•á- ...   \n",
       "916   :  ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡§Ç‡§ï‡•ç‡§∞‡§Æ‡§ø‡§§‡•ã‡§Ç ‡§µ‡§æ‡§≤‡§æ ‡§¶‡•Ç‡§∏‡§∞‡§æ ‡§¶‡•á‡§∂ ‡§¨‡§®‡§æ ‡§≠‡§æ‡§∞...   \n",
       "470   ‡§™‡§æ‡§™‡•Ä ‡§Ö‡§ß‡§∞‡•ç‡§Æ‡•Ä  ‡§Æ‡§®‡§π‡•Ç‡§∏ ‡§ï‡§≤‡§Ç‡§ï ‡§ï‡•ã ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§ï‡•Ä ‡§¨‡§ß‡§æ‡§à ‡§¶‡•á‡§®...   \n",
       "\n",
       "                                               hashtags smiley emoji  \\\n",
       "545                             [#SwachhSurvekshan2020]     []    []   \n",
       "1191                                                 []     []    []   \n",
       "245                                                  []     []    []   \n",
       "173                                                  []     []    []   \n",
       "1116                                                 []     []    []   \n",
       "78                                                   []     []    []   \n",
       "104                                                [#‡§ú]     []    []   \n",
       "1055         [#virushka, #RCB, #TeamIndia, #ViratKohli]     []    []   \n",
       "916   [#CoronaUpdates, #CoronavirusIndia, #CoronaUpd...     []    []   \n",
       "470                                                  []     []    []   \n",
       "\n",
       "                                                    url  \\\n",
       "545                                                  []   \n",
       "1191                                                 []   \n",
       "245                                                  []   \n",
       "173                                                  []   \n",
       "1116                          [https://t.co/KzbthV3juh]   \n",
       "78                                                   []   \n",
       "104                                                  []   \n",
       "1055  [https://t.co/8x4k3Mkk1g, https://t.co/YJBZvv4...   \n",
       "916                           [https://t.co/BRGQsNNtEe]   \n",
       "470                                                  []   \n",
       "\n",
       "                               mentions    numerals reserved_word emotext  \\\n",
       "545                     [@narendramodi]        [19]            []      []   \n",
       "1191                                 []          []            []      []   \n",
       "245                    [@LakheraSatish]          []            []      []   \n",
       "173                                  []          []            []      []   \n",
       "1116                                 []  [9, 9, 12]            []      []   \n",
       "78                                   []          []            []      []   \n",
       "104                                  []         [6]            []      []   \n",
       "1055                                 []          []            []      []   \n",
       "916                                  []          []            []      []   \n",
       "470   [@rashtrapatibhvn, @narendramodi]       [100]            []      []   \n",
       "\n",
       "                                         segmented_hash  \\\n",
       "545                            [swachh survekshan 2020]   \n",
       "1191                                                 []   \n",
       "245                                                  []   \n",
       "173                                                  []   \n",
       "1116                                                 []   \n",
       "78                                                   []   \n",
       "104                                                 [‡§ú]   \n",
       "1055         [virus hk a, rcb, team india, virat kohli]   \n",
       "916   [corona updates, coronavirus india, corona upd...   \n",
       "470                                                  []   \n",
       "\n",
       "                                                  clean  \\\n",
       "545   ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§≠‡§æ‡§∞‡§§-‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§≠‡§æ‡§∞‡§§' ‡§ï‡•á ‡§∏‡§Ç‡§ï‡§≤‡•ç‡§™ ‡§ï‡•á ‡§∏‡§æ‡§•   ‡§Æ‡•á...   \n",
       "1191  ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä Jio ‡§∏‡§ø‡§Æ ‡§ï‡•á ‡§è‡§° ‡§∏‡•á ‡§∂‡§æ‡§π‡§∞‡•Å‡§ñ ‡§ñ‡§æ‡§® ‡§ï‡•ã ‡§®‡§ø‡§ï‡§æ‡§≤ ...   \n",
       "245    : ‡§¶‡•á‡§ñ ‡§ö‡•Ä‡§®‡•Ä ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•Ä ‡§ï‡§¨‡•ç‡§∞, ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡§æ ‡§ü‡•Ç‡§ü ‡§∞...   \n",
       "173   ‡§™‡§§‡•ç‡§∞ ‡§ï‡•á ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§∏‡•á‡§®‡§æ ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§®‡§µ ‡§µ‡§∞‡•ç‡§∑ ‡§ï‡•Ä ‡§∂‡•Å‡§≠...   \n",
       "1116  Redmi 9 Prime ‡§î‡§∞ Redmi 9 ‡§ï‡•Ä ‡§∏‡•á‡§≤ ‡§Ü‡§ú ‡§¶‡•ã‡§™‡§π‡§∞ 12 ‡§¨‡§ú...   \n",
       "78    ‡§≠‡§æ‡§∞‡§§ ‡§®‡•á ‡§ö‡•Ä‡§® ‡§∏‡•á ‡§∏‡•Ä‡§Æ‡§æ ‡§™‡§∞ ‡§§‡•à‡§®‡§æ‡§§ ‡§â‡§∏‡§ï‡•á ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§Ö...   \n",
       "104   ‡§¶‡§ø‡§≤ ‡§¶‡§π‡§≤‡§æ ‡§¶‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§ñ‡§¨‡§∞‡•§ 6 ‡§∏‡§æ‡§≤ ‡§ï‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ...   \n",
       "1055  ‡§Æ‡•â‡§Æ-‡§°‡•à‡§° ‡§¨‡§®‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§π‡•à‡§Ç ‡§µ‡§ø‡§∞‡•Å‡§∑‡•ç‡§ï‡§æ, ‡§µ‡§ø‡§∞‡§æ‡§ü ‡§¨‡•ã‡§≤‡•á- ‡§Ö‡§µ...   \n",
       "916     : ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡§Ç‡§ï‡•ç‡§∞‡§Æ‡§ø‡§§‡•ã‡§Ç ‡§µ‡§æ‡§≤‡§æ ‡§¶‡•Ç‡§∏‡§∞‡§æ ‡§¶‡•á‡§∂ ‡§¨‡§®‡§æ ‡§≠‡§æ...   \n",
       "470       ‡§™‡§æ‡§™‡•Ä ‡§Ö‡§ß‡§∞‡•ç‡§Æ‡•Ä  ‡§Æ‡§®‡§π‡•Ç‡§∏ ‡§ï‡§≤‡§Ç‡§ï ‡§ï‡•ã ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§ï‡•Ä ‡§¨‡§ß‡§æ‡§à...   \n",
       "\n",
       "                                                  tweet  label  \n",
       "545   ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§≠‡§æ‡§∞‡§§-‡§∏‡•ç‡§µ‡§∏‡•ç‡§• ‡§≠‡§æ‡§∞‡§§' ‡§ï‡•á ‡§∏‡§Ç‡§ï‡§≤‡•ç‡§™ ‡§ï‡•á ‡§∏‡§æ‡§• #Swa...      0  \n",
       "1191  ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä Jio ‡§∏‡§ø‡§Æ ‡§ï‡•á ‡§è‡§° ‡§∏‡•á ‡§∂‡§æ‡§π‡§∞‡•Å‡§ñ ‡§ñ‡§æ‡§® ‡§ï‡•ã ‡§®‡§ø‡§ï‡§æ‡§≤ ...      1  \n",
       "245   @LakheraSatish: ‡§¶‡•á‡§ñ ‡§ö‡•Ä‡§®‡•Ä ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•Ä ‡§ï‡§¨‡•ç‡§∞, ‡§ï‡§æ‡§Ç‡§ó...      1  \n",
       "173   ‡§™‡§§‡•ç‡§∞ ‡§ï‡•á ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§∏‡•á‡§®‡§æ ‡§ï‡•á ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§®‡§µ ‡§µ‡§∞‡•ç‡§∑ ‡§ï‡•Ä ‡§∂‡•Å‡§≠...      1  \n",
       "1116  Redmi 9 Prime ‡§î‡§∞ Redmi 9 ‡§ï‡•Ä ‡§∏‡•á‡§≤ ‡§Ü‡§ú ‡§¶‡•ã‡§™‡§π‡§∞ 12 ‡§¨‡§ú...      0  \n",
       "78    ‡§≠‡§æ‡§∞‡§§ ‡§®‡•á ‡§ö‡•Ä‡§® ‡§∏‡•á ‡§∏‡•Ä‡§Æ‡§æ ‡§™‡§∞ ‡§§‡•à‡§®‡§æ‡§§ ‡§â‡§∏‡§ï‡•á ‡§∏‡•à‡§®‡§ø‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§Ö...      1  \n",
       "104   ‡§¶‡§ø‡§≤ ‡§¶‡§π‡§≤‡§æ ‡§¶‡•á‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§ñ‡§¨‡§∞‡•§ 6 ‡§∏‡§æ‡§≤ ‡§ï‡•Ä ‡§¨‡§ö‡•ç‡§ö‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ...      1  \n",
       "1055  ‡§Æ‡•â‡§Æ-‡§°‡•à‡§° ‡§¨‡§®‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§π‡•à‡§Ç ‡§µ‡§ø‡§∞‡•Å‡§∑‡•ç‡§ï‡§æ, ‡§µ‡§ø‡§∞‡§æ‡§ü ‡§¨‡•ã‡§≤‡•á- ‡§Ö‡§µ...      0  \n",
       "916   #CoronaUpdates : ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡§Ç‡§ï‡•ç‡§∞‡§Æ‡§ø‡§§‡•ã‡§Ç ‡§µ‡§æ‡§≤‡§æ ‡§¶‡•Ç‡§∏...      0  \n",
       "470   @rashtrapatibhvn @narendramodi ‡§™‡§æ‡§™‡•Ä ‡§Ö‡§ß‡§∞‡•ç‡§Æ‡•Ä  ‡§Æ‡§®...      1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label_decode(val):\n",
    "#     return labels[val]\n",
    "# test.label = test.label.apply(label_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test.to_csv(path_or_buf='../temp/labels/'+lab+'.txt', index=False, columns = ['tweet_id', 'label'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
