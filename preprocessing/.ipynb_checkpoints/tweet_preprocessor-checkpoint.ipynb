{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Hate-Speech-Detection/blob/master/tweet_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:27:13.198398Z",
     "iopub.status.busy": "2020-12-05T23:27:13.198195Z",
     "iopub.status.idle": "2020-12-05T23:27:13.660184Z",
     "shell.execute_reply": "2020-12-05T23:27:13.659440Z",
     "shell.execute_reply.started": "2020-12-05T23:27:13.198345Z"
    },
    "id": "5SzZYXq3ER6b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlrd\n",
    "import re\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:27:13.673917Z",
     "iopub.status.busy": "2020-12-05T23:27:13.673759Z",
     "iopub.status.idle": "2020-12-05T23:27:20.948500Z",
     "shell.execute_reply": "2020-12-05T23:27:20.947662Z",
     "shell.execute_reply.started": "2020-12-05T23:27:13.673897Z"
    },
    "id": "cOqUHRECEX9o",
    "outputId": "0911749c-b53f-4f9e-bb3e-512d1b1d4be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: ekphrasis in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (0.5.1)\n",
      "Requirement already satisfied: matplotlib in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (3.3.1)\n",
      "Requirement already satisfied: termcolor in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (4.51.0)\n",
      "Requirement already satisfied: numpy in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (1.19.2)\n",
      "Requirement already satisfied: colorama in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (0.4.3)\n",
      "Requirement already satisfied: ftfy in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (5.8)\n",
      "Requirement already satisfied: ujson in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (3.2.0)\n",
      "Requirement already satisfied: nltk in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (3.5)\n",
      "Requirement already satisfied: wcwidth in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ftfy->ekphrasis) (0.2.5)\n",
      "Requirement already satisfied: numpy in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (1.19.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from matplotlib->ekphrasis) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from matplotlib->ekphrasis) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from matplotlib->ekphrasis) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from matplotlib->ekphrasis) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from matplotlib->ekphrasis) (1.3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from matplotlib->ekphrasis) (7.2.0)\n",
      "Requirement already satisfied: six in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->ekphrasis) (1.15.0)\n",
      "Requirement already satisfied: click in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from nltk->ekphrasis) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from nltk->ekphrasis) (0.17.0)\n",
      "Requirement already satisfied: regex in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from nltk->ekphrasis) (2020.7.14)\n",
      "Requirement already satisfied: tqdm in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (4.51.0)\n",
      "Requirement already satisfied: six in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->ekphrasis) (1.15.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-pasta: [Errno 2] No such file or directory: '/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/google_pasta-0.2.0.dist-info/METADATA'\u001b[0m\n",
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "!pip install ekphrasis\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "# to leverage word statistics from Twitter\n",
    "seg_tw = Segmenter(corpus = \"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:27:20.950824Z",
     "iopub.status.busy": "2020-12-05T23:27:20.950589Z",
     "iopub.status.idle": "2020-12-05T23:27:22.803123Z",
     "shell.execute_reply": "2020-12-05T23:27:22.802513Z",
     "shell.execute_reply.started": "2020-12-05T23:27:20.950779Z"
    },
    "id": "cvfnKs-dEZwN",
    "outputId": "0d6002e5-ae50-4bd0-8f2d-6d4ac61ffd32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: tweet-preprocessor in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (0.6.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-pasta: [Errno 2] No such file or directory: '/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/google_pasta-0.2.0.dist-info/METADATA'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "!pip install tweet-preprocessor\n",
    "import preprocessor as tweet_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:27:22.804628Z",
     "iopub.status.busy": "2020-12-05T23:27:22.804446Z",
     "iopub.status.idle": "2020-12-05T23:27:24.627084Z",
     "shell.execute_reply": "2020-12-05T23:27:24.626479Z",
     "shell.execute_reply.started": "2020-12-05T23:27:22.804606Z"
    },
    "id": "Wk2BUZTuEdN1",
    "outputId": "ae9779a1-b187-4648-a971-5a82e12ce5fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: emot in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (2.1)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-pasta: [Errno 2] No such file or directory: '/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/google_pasta-0.2.0.dist-info/METADATA'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "!pip install emot\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqibDmdYEjIK"
   },
   "source": [
    "#### *Raw Datasets are hosted [here](https://drive.google.com/drive/folders/1TuHRQQ41lK9oXJhlhsRiMjMnczuKn_kF?usp=sharing).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:27:24.628559Z",
     "iopub.status.busy": "2020-12-05T23:27:24.628377Z",
     "iopub.status.idle": "2020-12-05T23:27:24.631000Z",
     "shell.execute_reply": "2020-12-05T23:27:24.630506Z",
     "shell.execute_reply.started": "2020-12-05T23:27:24.628534Z"
    },
    "id": "SmRsC-wmEhdx",
    "outputId": "d867c2c1-b442-4578-8046-429c47068dc6"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:33:37.337080Z",
     "iopub.status.busy": "2020-12-05T23:33:37.336836Z",
     "iopub.status.idle": "2020-12-05T23:33:37.341871Z",
     "shell.execute_reply": "2020-12-05T23:33:37.341286Z",
     "shell.execute_reply.started": "2020-12-05T23:33:37.337052Z"
    },
    "id": "fD2Ksnv6EmaS"
   },
   "outputs": [],
   "source": [
    "def make_list(proc_obj):\n",
    "  if proc_obj == None:\n",
    "    return []\n",
    "  \n",
    "  store = []\n",
    "  for unit in proc_obj:\n",
    "    store.append(unit.match)\n",
    "  \n",
    "  return store\n",
    "\n",
    "def emotext(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:38:22.625412Z",
     "iopub.status.busy": "2020-12-05T23:38:22.625126Z",
     "iopub.status.idle": "2020-12-05T23:38:22.628676Z",
     "shell.execute_reply": "2020-12-05T23:38:22.628203Z",
     "shell.execute_reply.started": "2020-12-05T23:38:22.625385Z"
    },
    "id": "PI524oqVE026"
   },
   "outputs": [],
   "source": [
    "# For 2020 Datasets\n",
    "\n",
    "is_hindi = 1\n",
    "\n",
    "# For Train Data\n",
    "# datatype = \"train\"\n",
    "# For English\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/english.xlsx\"\n",
    "\n",
    "# For Hindi\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/hindi.xlsx\"\n",
    "# is_hindi = 1\n",
    "\n",
    "# For German\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/german.xlsx\"\n",
    "\n",
    "# For Test Data\n",
    "datatype = \"train\"\n",
    "# For English\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/english_test_1509.csv\"\n",
    "\n",
    "# For Hindi\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/hindi_test_1509.csv\"\n",
    "# is_hindi = 1\n",
    "\n",
    "# For German\n",
    "file_name = \"data/valid.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:38:22.860705Z",
     "iopub.status.busy": "2020-12-05T23:38:22.860512Z",
     "iopub.status.idle": "2020-12-05T23:38:22.865938Z",
     "shell.execute_reply": "2020-12-05T23:38:22.865455Z",
     "shell.execute_reply.started": "2020-12-05T23:38:22.860683Z"
    },
    "id": "nNNoGyv8E5th"
   },
   "outputs": [],
   "source": [
    "# Initializing Lists\n",
    "datapoints_count = 0\n",
    "see_index = True\n",
    "\n",
    "tweets = []\n",
    "raw_tweet_texts = []\n",
    "tokenized_tweets = []\n",
    "hashtags = []\n",
    "smileys = []\n",
    "emojis = []\n",
    "urls = []\n",
    "mentions = []\n",
    "numbers = []\n",
    "reserveds = []\n",
    "clean = []\n",
    "task_1_labels = []\n",
    "task_2_labels = []\n",
    "tweet_ids = []\n",
    "hasoc_ID = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:38:23.021192Z",
     "iopub.status.busy": "2020-12-05T23:38:23.021028Z",
     "iopub.status.idle": "2020-12-05T23:38:23.026535Z",
     "shell.execute_reply": "2020-12-05T23:38:23.026008Z",
     "shell.execute_reply.started": "2020-12-05T23:38:23.021172Z"
    },
    "id": "nsKZHtQ-FvOw"
   },
   "outputs": [],
   "source": [
    "def strip_list(listie):\n",
    "  stripped = []\n",
    "  for item in listie:\n",
    "    stripped.append(item.strip())\n",
    "  return stripped\n",
    "\n",
    "def hindi_clean(line, parse_obj):\n",
    "  # beta\n",
    "  tokens = line.replace(\":\", \" : \").replace(\",\", \" , \").replace(\";\", \" ; \").split(\" \")\n",
    "  valid_stri = \"\"\n",
    "\n",
    "  for raw_token in tokens:\n",
    "    token = raw_token.strip()\n",
    "    if token in strip_list(make_list(parse_obj.hashtags)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.smileys)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.emojis)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.urls)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.mentions)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.numbers)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.reserved)):\n",
    "      continue\n",
    "    valid_stri = valid_stri + \" \" + token\n",
    "  return valid_stri.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:38:23.231231Z",
     "iopub.status.busy": "2020-12-05T23:38:23.230995Z",
     "iopub.status.idle": "2020-12-05T23:38:23.529538Z",
     "shell.execute_reply": "2020-12-05T23:38:23.529028Z",
     "shell.execute_reply.started": "2020-12-05T23:38:23.231206Z"
    },
    "id": "0TuJvskdE95H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Datapoints: 760\n"
     ]
    }
   ],
   "source": [
    "if datatype == 'train':\n",
    "#     workbook = xlrd.open_workbook(file_name)\n",
    "#     sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "#     for row in range(sheet.nrows):\n",
    "#         line = sheet.row_values(row)\n",
    "\n",
    "    file = open(file_name, 'r')\n",
    "    count = 0\n",
    "    file_reader = csv.reader(file, delimiter = \"\\t\")\n",
    "    for line in file_reader:\n",
    "#         if see_index == True:\n",
    "#             see_index = False\n",
    "#             continue\n",
    "\n",
    "        datapoints_count += 1\n",
    "        tweet_ids.append(line[0])\n",
    "        task_1_labels.append(line[2])\n",
    "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
    "        temp = line[1].replace(\"\\n\", \" \")\n",
    "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
    "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
    "        tag = strip_list(make_list(parse_obj.hashtags))\n",
    "        hashtags.append(tag)\n",
    "        smiley = strip_list(make_list(parse_obj.smileys))\n",
    "        smileys.append(smiley)\n",
    "        emoji = strip_list(make_list(parse_obj.emojis))\n",
    "        emojis.append(emoji)\n",
    "        url = strip_list(make_list(parse_obj.urls))\n",
    "        urls.append(url)\n",
    "        mention = strip_list(make_list(parse_obj.mentions))\n",
    "        mentions.append(mention)\n",
    "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "        for i in url + mention + smiley + emoji + tag:\n",
    "#             print(i, tem)\n",
    "            temp = temp.replace(i, \" \")\n",
    "        clean.append(temp)\n",
    "        if is_hindi == 0:\n",
    "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
    "        else:\n",
    "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
    "\n",
    "    print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T23:38:23.530585Z",
     "iopub.status.busy": "2020-12-05T23:38:23.530411Z",
     "iopub.status.idle": "2020-12-05T23:38:23.533764Z",
     "shell.execute_reply": "2020-12-05T23:38:23.533315Z",
     "shell.execute_reply.started": "2020-12-05T23:38:23.530565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoints_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:38:23.558567Z",
     "iopub.status.busy": "2020-12-05T23:38:23.558401Z",
     "iopub.status.idle": "2020-12-05T23:38:23.564440Z",
     "shell.execute_reply": "2020-12-05T23:38:23.563951Z",
     "shell.execute_reply.started": "2020-12-05T23:38:23.558547Z"
    },
    "id": "9gtRGkIkNmIv",
    "outputId": "9e46fa44-4a8f-44af-884f-9086be627719"
   },
   "outputs": [],
   "source": [
    "if datatype == 'test':\n",
    "    file = open(file_name, 'r')\n",
    "    file_reader = csv.reader(file, delimiter = \"\\t\")\n",
    "    for line in file_reader:\n",
    "        if see_index == True:\n",
    "            see_index = False\n",
    "            continue\n",
    "#         print(line)\n",
    "        datapoints_count += 1\n",
    "        tweet_ids.append(line[0])\n",
    "        task_1_labels.append(line[2])\n",
    "#         task_2_labels.append(line[3])\n",
    "#         hasoc_ID.append(line[4])\n",
    "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
    "\n",
    "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
    "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
    "        hashtags.append(strip_list(make_list(parse_obj.hashtags)))\n",
    "        smileys.append(strip_list(make_list(parse_obj.smileys)))\n",
    "        emojis.append(strip_list(make_list(parse_obj.emojis)))\n",
    "        urls.append(strip_list(make_list(parse_obj.urls)))\n",
    "        mentions.append(strip_list(make_list(parse_obj.mentions)))\n",
    "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "\n",
    "        if is_hindi == 0:\n",
    "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
    "        else:\n",
    "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
    "\n",
    "    print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:38:24.122119Z",
     "iopub.status.busy": "2020-12-05T23:38:24.121900Z",
     "iopub.status.idle": "2020-12-05T23:38:24.129387Z",
     "shell.execute_reply": "2020-12-05T23:38:24.128687Z",
     "shell.execute_reply.started": "2020-12-05T23:38:24.122096Z"
    },
    "id": "Gd_fy5REFxUX",
    "outputId": "31bd67fc-95aa-4699-8df2-678b0e6b06d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets:\n",
      "['Post', 'दृढ़ इच्छा शक्ति से परिपूर्ण प्रणबदा के लिए देशहित सर्वोच्च रहा।  उनका निधन हम सब के लिए अपूरणीय क्षति है। ईश्वर दिवंगत आत्मा को अपने श्रीचरणों में स्थान दें। शोक संतप्त परिजनों के प्रति संवेदनाएं। ऊं शांति!!!', 'भारतीय जनता पार्टी rss वाले इतने गिरे हुए हैं जहां मैं रहती हूं वहां मेरी जासूसी  करा रहें है उसकी जासूस की पहचान मुझे अच्छी तरह है rss बीजेपी वाले की जासूस दिल्ली में कौन है उत्तर प्रदेश में कौन है हरियाणा राजस्थान में कौन है सबकी पहचान है मुझे मेरी नजर से बच नहीं सकते हो', 'कोरोना से निपटने की तैयारी / दिल्ली में 10 हजार बेड वाला दुनिया का सबसे बड़ा कोविड केयर सेंटर शुरू, राजनाथ-शाह ने डीआरडीओ के 1 हजार बेड वाले सेंटर का भी उद्घाटन किया https://t.co/9rlQowAsFh #Delhi @ArvindKejriwal  @rajnathsingh @AmitShah @DRDO_India @WHO @crpfindia @ITBP_official', 'गवर्नर कॉन्फ्रेंस में PM मोदी बोले- शिक्षा नीति में सरकार का दखल कम होना चाहिए https://t.co/ZvKgxk6dbd']\n",
      "Raw Texts:\n",
      "['Post', 'दृढ़ इच्छा शक्ति से परिपूर्ण प्रणबदा के लिए देशहित सर्वोच्च रहा।  उनका निधन हम सब के लिए अपूरणीय क्षति है। ईश्वर दिवंगत आत्मा को अपने श्रीचरणों में स्थान दें। शोक संतप्त परिजनों के प्रति संवेदनाएं। ऊं शांति!!!', 'भारतीय जनता पार्टी rss वाले इतने गिरे हुए हैं जहां मैं रहती हूं वहां मेरी जासूसी  करा रहें है उसकी जासूस की पहचान मुझे अच्छी तरह है rss बीजेपी वाले की जासूस दिल्ली में कौन है उत्तर प्रदेश में कौन है हरियाणा राजस्थान में कौन है सबकी पहचान है मुझे मेरी नजर से बच नहीं सकते हो', 'कोरोना से निपटने की तैयारी / दिल्ली में हजार बेड वाला दुनिया का सबसे बड़ा कोविड केयर सेंटर शुरू ,  राजनाथ-शाह ने डीआरडीओ के हजार बेड वाले सेंटर का भी उद्घाटन किया https : //t.co/9rlQowAsFh', 'गवर्नर कॉन्फ्रेंस में PM मोदी बोले- शिक्षा नीति में सरकार का दखल कम होना चाहिए https : //t.co/ZvKgxk6dbd']\n",
      "Hashtags:\n",
      "[[], [], [], ['#Delhi'], []]\n",
      "Smileys:\n",
      "[[], [], [], [], []]\n",
      "Emojis:\n",
      "[[], [], [], [], []]\n",
      "Urls:\n",
      "[[], [], [], ['https://t.co/9rlQowAsFh'], ['https://t.co/ZvKgxk6dbd']]\n",
      "Mentions:\n",
      "[[], [], [], ['@ArvindKejriwal', '@rajnathsingh', '@AmitShah', '@DRDO_India', '@WHO', '@crpfindia', '@ITBP_official'], []]\n",
      "Numbers:\n",
      "[[], [], [], ['10', '1'], []]\n",
      "Reserved Words:\n",
      "[[], [], [], [], []]\n",
      "Cleaned\n",
      "['Post', 'दृढ़ इच्छा शक्ति से परिपूर्ण प्रणबदा के लिए देशहित सर्वोच्च रहा।  उनका निधन हम सब के लिए अपूरणीय क्षति है। ईश्वर दिवंगत आत्मा को अपने श्रीचरणों में स्थान दें। शोक संतप्त परिजनों के प्रति संवेदनाएं। ऊं शांति!!!', 'भारतीय जनता पार्टी rss वाले इतने गिरे हुए हैं जहां मैं रहती हूं वहां मेरी जासूसी  करा रहें है उसकी जासूस की पहचान मुझे अच्छी तरह है rss बीजेपी वाले की जासूस दिल्ली में कौन है उत्तर प्रदेश में कौन है हरियाणा राजस्थान में कौन है सबकी पहचान है मुझे मेरी नजर से बच नहीं सकते हो', 'कोरोना से निपटने की तैयारी / दिल्ली में 10 हजार बेड वाला दुनिया का सबसे बड़ा कोविड केयर सेंटर शुरू, राजनाथ-शाह ने डीआरडीओ के 1 हजार बेड वाले सेंटर का भी उद्घाटन किया                   ', 'गवर्नर कॉन्फ्रेंस में PM मोदी बोले- शिक्षा नीति में सरकार का दखल कम होना चाहिए  ']\n",
      "Task Labels:\n",
      "['Labels Set', 'non-hostile', 'defamation', 'non-hostile', 'non-hostile']\n"
     ]
    }
   ],
   "source": [
    "# Viewing Created Dataset\n",
    "\n",
    "print(\"Tweets:\")\n",
    "print(tweets[0: 5])\n",
    "\n",
    "print(\"Raw Texts:\")\n",
    "print(raw_tweet_texts[0: 5])\n",
    "\n",
    "print(\"Hashtags:\")\n",
    "print(hashtags[0: 5])\n",
    "\n",
    "print(\"Smileys:\")\n",
    "print(smileys[0: 5])\n",
    "\n",
    "print(\"Emojis:\")\n",
    "print(emojis[0: 5])\n",
    "\n",
    "print(\"Urls:\")\n",
    "print(urls[0: 5])\n",
    "\n",
    "print(\"Mentions:\")\n",
    "print(mentions[0: 5])\n",
    "\n",
    "print(\"Numbers:\")\n",
    "print(numbers[0: 5])\n",
    "\n",
    "print(\"Reserved Words:\")\n",
    "print(reserveds[0: 5])\n",
    "print(\"Cleaned\")\n",
    "print(clean[0:5])\n",
    "print(\"Task Labels:\")\n",
    "print(task_1_labels[0: 5])\n",
    "# print(task_2_labels[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:38:24.341196Z",
     "iopub.status.busy": "2020-12-05T23:38:24.341003Z",
     "iopub.status.idle": "2020-12-05T23:38:24.518363Z",
     "shell.execute_reply": "2020-12-05T23:38:24.517760Z",
     "shell.execute_reply.started": "2020-12-05T23:38:24.341173Z"
    },
    "id": "LUrX_FPcIVQR",
    "outputId": "138a1162-3919-43e3-c396-a1745c15b244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Descriptions:\n",
      "[[], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Generating Emoji Texts\n",
    "emoji_texts = []\n",
    "\n",
    "for emo_list in emojis:\n",
    "  texts = []\n",
    "  for emoji in emo_list:\n",
    "    text = emotext(emoji)\n",
    "    texts.append(text.replace(\"_\", \" \"))\n",
    "  emoji_texts.append(texts)\n",
    "\n",
    "print(\"Emoji Descriptions:\")\n",
    "print(emoji_texts[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:38:24.700645Z",
     "iopub.status.busy": "2020-12-05T23:38:24.700476Z",
     "iopub.status.idle": "2020-12-05T23:38:24.705483Z",
     "shell.execute_reply": "2020-12-05T23:38:24.704948Z",
     "shell.execute_reply.started": "2020-12-05T23:38:24.700624Z"
    },
    "id": "9PWvCf65IXQT",
    "outputId": "d3d1cb70-d883-4adf-8536-9302bfcd126e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented Hashtags:\n",
      "[[], [], [], ['delhi'], []]\n"
     ]
    }
   ],
   "source": [
    "# Segmenting Hashtags\n",
    "segmented_hashtags = []\n",
    "\n",
    "for hashset in hashtags:\n",
    "  segmented_set = []\n",
    "  for tag in hashset:\n",
    "    word = tag[1: ]\n",
    "    # removing the hash symbol\n",
    "    segmented_set.append(seg_tw.segment(word))\n",
    "  segmented_hashtags.append(segmented_set)\n",
    "\n",
    "print(\"Segmented Hashtags:\")\n",
    "print(segmented_hashtags[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:38:25.207296Z",
     "iopub.status.busy": "2020-12-05T23:38:25.207033Z",
     "iopub.status.idle": "2020-12-05T23:38:25.250977Z",
     "shell.execute_reply": "2020-12-05T23:38:25.250395Z",
     "shell.execute_reply.started": "2020-12-05T23:38:25.207271Z"
    },
    "id": "UGPepLXoJsqR"
   },
   "outputs": [],
   "source": [
    "name = 'valid.pickle'\n",
    "dickie = {}\n",
    "dickie['tweet_id'] = tweet_ids\n",
    "dickie['task_1'] = task_1_labels\n",
    "dickie['full_tweet'] = tweets\n",
    "dickie['tweet_raw_text'] = raw_tweet_texts\n",
    "dickie['hashtags'] = hashtags\n",
    "dickie['smiley'] = smileys\n",
    "dickie['emoji'] = emojis\n",
    "dickie['url'] = urls\n",
    "dickie['mentions'] = mentions\n",
    "dickie['numerals'] = numbers\n",
    "dickie['reserved_word'] = reserveds\n",
    "dickie['emotext'] = emoji_texts\n",
    "dickie['segmented_hash'] = segmented_hashtags\n",
    "dickie['clean'] = clean\n",
    "with open(name, 'wb') as f:\n",
    "  pickle.dump(dickie, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:38:26.454103Z",
     "iopub.status.busy": "2020-12-05T23:38:26.453893Z",
     "iopub.status.idle": "2020-12-05T23:38:26.468376Z",
     "shell.execute_reply": "2020-12-05T23:38:26.467662Z",
     "shell.execute_reply.started": "2020-12-05T23:38:26.454081Z"
    },
    "id": "FK4uO1J1LeIC",
    "outputId": "d2839abd-0419-4ba5-e422-cf2914318b4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[760, 760, 760, 760, 760, 760, 760, 760, 760, 760, 760, 760, 760, 760]\n"
     ]
    }
   ],
   "source": [
    "with open(name, 'rb') as f:\n",
    "  try_dict = pickle.load(f)\n",
    "\n",
    "sizes = []\n",
    "for key in try_dict.keys():\n",
    "  sizes.append(len(try_dict[key]))\n",
    "\n",
    "# Verifying if all sizes are equal\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3FUMYDP9IaXe"
   },
   "outputs": [],
   "source": [
    "# ^_^ Thank You"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxDfY6jG78kns9fmcJq+Qh",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tweet_processor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
