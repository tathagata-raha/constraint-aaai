{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Hate-Speech-Detection/blob/master/tweet_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SzZYXq3ER6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: xlrd in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip install xlrd\n",
    "import xlrd\n",
    "import re\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "cOqUHRECEX9o",
    "outputId": "0911749c-b53f-4f9e-bb3e-512d1b1d4be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: ekphrasis in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (0.5.1)\n",
      "Requirement already satisfied: numpy in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (1.19.4)\n",
      "Requirement already satisfied: matplotlib in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (3.3.3)\n",
      "Requirement already satisfied: tqdm in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (4.55.0)\n",
      "Requirement already satisfied: nltk in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (3.5)\n",
      "Requirement already satisfied: ftfy in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (5.8)\n",
      "Requirement already satisfied: ujson in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (4.0.1)\n",
      "Requirement already satisfied: colorama in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (0.4.4)\n",
      "Requirement already satisfied: termcolor in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (1.1.0)\n",
      "Requirement already satisfied: wcwidth in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ftfy->ekphrasis) (0.2.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from matplotlib->ekphrasis) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from matplotlib->ekphrasis) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from matplotlib->ekphrasis) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from matplotlib->ekphrasis) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from matplotlib->ekphrasis) (1.3.1)\n",
      "Requirement already satisfied: six in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib->ekphrasis) (1.15.0)\n",
      "Requirement already satisfied: click in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from nltk->ekphrasis) (7.1.2)\n",
      "Requirement already satisfied: regex in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from nltk->ekphrasis) (2020.11.13)\n",
      "Requirement already satisfied: joblib in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from nltk->ekphrasis) (1.0.0)\n",
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "!pip install ekphrasis\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "# to leverage word statistics from Twitter\n",
    "seg_tw = Segmenter(corpus = \"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cvfnKs-dEZwN",
    "outputId": "0d6002e5-ae50-4bd0-8f2d-6d4ac61ffd32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: tweet-preprocessor in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "!pip install tweet-preprocessor\n",
    "import preprocessor as tweet_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Wk2BUZTuEdN1",
    "outputId": "ae9779a1-b187-4648-a971-5a82e12ce5fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: emot in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (2.1)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "!pip install emot\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqibDmdYEjIK"
   },
   "source": [
    "#### *Raw Datasets are hosted [here](https://drive.google.com/drive/folders/1TuHRQQ41lK9oXJhlhsRiMjMnczuKn_kF?usp=sharing).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SmRsC-wmEhdx",
    "outputId": "d867c2c1-b442-4578-8046-429c47068dc6"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fD2Ksnv6EmaS"
   },
   "outputs": [],
   "source": [
    "def make_list(proc_obj):\n",
    "  if proc_obj == None:\n",
    "    return []\n",
    "  \n",
    "  store = []\n",
    "  for unit in proc_obj:\n",
    "    store.append(unit.match)\n",
    "  \n",
    "  return store\n",
    "\n",
    "def emotext(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PI524oqVE026"
   },
   "outputs": [],
   "source": [
    "is_hindi = 1\n",
    "# change to test while running on the test set\n",
    "datatype = \"test\"\n",
    "# location of the tsv file\n",
    "file_name = \"../data/\"+datatype+\".tsv\"\n",
    "dest = '../temp/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNNoGyv8E5th"
   },
   "outputs": [],
   "source": [
    "# Initializing Lists\n",
    "datapoints_count = 0\n",
    "see_index = True\n",
    "\n",
    "tweets = []\n",
    "raw_tweet_texts = []\n",
    "tokenized_tweets = []\n",
    "hashtags = []\n",
    "smileys = []\n",
    "emojis = []\n",
    "urls = []\n",
    "mentions = []\n",
    "numbers = []\n",
    "reserveds = []\n",
    "clean = []\n",
    "task_1_labels = []\n",
    "task_2_labels = []\n",
    "tweet_ids = []\n",
    "hasoc_ID = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsKZHtQ-FvOw"
   },
   "outputs": [],
   "source": [
    "def strip_list(listie):\n",
    "  stripped = []\n",
    "  for item in listie:\n",
    "    stripped.append(item.strip())\n",
    "  return stripped\n",
    "\n",
    "def hindi_clean(line, parse_obj):\n",
    "  # beta\n",
    "  tokens = line.replace(\":\", \" : \").replace(\",\", \" , \").replace(\";\", \" ; \").split(\" \")\n",
    "  valid_stri = \"\"\n",
    "\n",
    "  for raw_token in tokens:\n",
    "    token = raw_token.strip()\n",
    "    if token in strip_list(make_list(parse_obj.hashtags)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.smileys)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.emojis)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.urls)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.mentions)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.numbers)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.reserved)):\n",
    "      continue\n",
    "    valid_stri = valid_stri + \" \" + token\n",
    "  return valid_stri.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TuJvskdE95H"
   },
   "outputs": [],
   "source": [
    "if datatype == 'train' or datatype=='valid':\n",
    "#     workbook = xlrd.open_workbook(file_name)\n",
    "#     sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "#     for row in range(sheet.nrows):\n",
    "#         line = sheet.row_values(row)\n",
    "\n",
    "    file = open(file_name, 'r')\n",
    "    count = 0\n",
    "    file_reader = csv.reader(file, delimiter = \"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for line in file_reader:\n",
    "#         if see_index == True:\n",
    "#             see_index = False\n",
    "#             continue\n",
    "\n",
    "        datapoints_count += 1\n",
    "        tweet_ids.append(line[0])\n",
    "        task_1_labels.append(line[2])\n",
    "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
    "        temp = line[1].replace(\"\\n\", \" \")\n",
    "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
    "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
    "        tag = strip_list(make_list(parse_obj.hashtags))\n",
    "        hashtags.append(tag)\n",
    "        smiley = strip_list(make_list(parse_obj.smileys))\n",
    "        smileys.append(smiley)\n",
    "        emoji = strip_list(make_list(parse_obj.emojis))\n",
    "        emojis.append(emoji)\n",
    "        url = strip_list(make_list(parse_obj.urls))\n",
    "        urls.append(url)\n",
    "        mention = strip_list(make_list(parse_obj.mentions))\n",
    "        mentions.append(mention)\n",
    "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "        for i in url + mention + smiley + emoji + tag:\n",
    "#             print(i, tem)\n",
    "            temp = temp.replace(i, \" \")\n",
    "        clean.append(temp)\n",
    "        if is_hindi == 0:\n",
    "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
    "        else:\n",
    "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
    "\n",
    "    print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoints_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9gtRGkIkNmIv",
    "outputId": "9e46fa44-4a8f-44af-884f-9086be627719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Datapoints: 1653\n"
     ]
    }
   ],
   "source": [
    "if datatype == 'test':\n",
    "    file = open(file_name, 'r')\n",
    "    file_reader = csv.reader(file, delimiter = \"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for line in file_reader:\n",
    "        if see_index == True:\n",
    "            see_index = False\n",
    "            continue\n",
    "#         print(line)\n",
    "        datapoints_count += 1\n",
    "        tweet_ids.append(line[0])\n",
    "#         task_1_labels.append(line[2])\n",
    "#         task_2_labels.append(line[3])\n",
    "#         hasoc_ID.append(line[4])\n",
    "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
    "\n",
    "        temp = line[1].replace(\"\\n\", \" \")\n",
    "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
    "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
    "        tag = strip_list(make_list(parse_obj.hashtags))\n",
    "        hashtags.append(tag)\n",
    "        smiley = strip_list(make_list(parse_obj.smileys))\n",
    "        smileys.append(smiley)\n",
    "        emoji = strip_list(make_list(parse_obj.emojis))\n",
    "        emojis.append(emoji)\n",
    "        url = strip_list(make_list(parse_obj.urls))\n",
    "        urls.append(url)\n",
    "        mention = strip_list(make_list(parse_obj.mentions))\n",
    "        mentions.append(mention)\n",
    "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "        for i in url + mention + smiley + emoji + tag:\n",
    "#             print(i, tem)\n",
    "            temp = temp.replace(i, \" \")\n",
    "        clean.append(temp)\n",
    "        if is_hindi == 0:\n",
    "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
    "        else:\n",
    "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
    "\n",
    "    print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "Gd_fy5REFxUX",
    "outputId": "31bd67fc-95aa-4699-8df2-678b0e6b06d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets:\n",
      "['कीस की को रोजगार चाहिए फिर नहीं कहना रोजगार नहीं मिलता है 20 करोड को रोजगार दे दिया है वह भी मात्र 6 साँल में चार साल अभी बाकी है और हर साल दो करोड़ रोजगार देने का ही वादा था 10 साल में देना था 20 करोड को लोगो को रोजगार जो मात्र 6 साल में लक्ष्य को प्राप्त करने वाली पहली सरकार है', 'पटना: BMP कैंप में पुरुष और महिला कांस्टेबल ने गोली मारकर की खुदकुशी, जांच में जुटी पुलिस  @kumarprakash4u की रिपोर्ट  https://t.co/Dq05hREifM', 'कोई भी कांग्रेसी, ऊंची छत पर, रेलवे लाइन पर, ऊंची बिल्डिंग पर, एकांत जगह पर, कुए के पास दिखाई दे, तुरंत पुलिस को सूचित करें,🙏 😂👍 विलुप्त होती हुई प्रजातियो को बचाना हमारा फर्ज है।', 'अंडरवर्ल्ड डॉन छोटा राजन के भाई को बीजेपी द्वारा टिकट मिला है।  ', 'RT @_Pb_swain_: इन पंचर छापों को कोन समझाए कि उनके रोजगार में कमी का कारण मोदी नहीं  👇 ट्यूब लैस टायर है.😂😂😂😂']\n",
      "Raw Texts:\n",
      "['कीस की को रोजगार चाहिए फिर नहीं कहना रोजगार नहीं मिलता है करोड को रोजगार दे दिया है वह भी मात्र साँल में चार साल अभी बाकी है और हर साल दो करोड़ रोजगार देने का ही वादा था साल में देना था करोड को लोगो को रोजगार जो मात्र साल में लक्ष्य को प्राप्त करने वाली पहली सरकार है', 'पटना :  BMP कैंप में पुरुष और महिला कांस्टेबल ने गोली मारकर की खुदकुशी ,  जांच में जुटी पुलिस  की रिपोर्ट  https : //t.co/Dq05hREifM', 'कोई भी कांग्रेसी ,  ऊंची छत पर ,  रेलवे लाइन पर ,  ऊंची बिल्डिंग पर ,  एकांत जगह पर ,  कुए के पास दिखाई दे ,  तुरंत पुलिस को सूचित करें , 😂👍 विलुप्त होती हुई प्रजातियो को बचाना हमारा फर्ज है।', 'अंडरवर्ल्ड डॉन छोटा राजन के भाई को बीजेपी द्वारा टिकट मिला है।', ':  इन पंचर छापों को कोन समझाए कि उनके रोजगार में कमी का कारण मोदी नहीं  ट्यूब लैस टायर है.😂😂😂😂']\n",
      "Hashtags:\n",
      "[[], [], [], [], []]\n",
      "Smileys:\n",
      "[[], [], [], [], []]\n",
      "Emojis:\n",
      "[[], [], ['🙏', '😂', '👍'], [], ['👇', '😂', '😂', '😂', '😂']]\n",
      "Urls:\n",
      "[[], ['https://t.co/Dq05hREifM'], [], [], []]\n",
      "Mentions:\n",
      "[[], ['@kumarprakash4u'], [], [], ['@_Pb_swain_']]\n",
      "Numbers:\n",
      "[['20', '6', '10', '20', '6'], [], [], [], []]\n",
      "Reserved Words:\n",
      "[[], [], [], [], ['RT']]\n",
      "Cleaned\n",
      "['कीस की को रोजगार चाहिए फिर नहीं कहना रोजगार नहीं मिलता है 20 करोड को रोजगार दे दिया है वह भी मात्र 6 साँल में चार साल अभी बाकी है और हर साल दो करोड़ रोजगार देने का ही वादा था 10 साल में देना था 20 करोड को लोगो को रोजगार जो मात्र 6 साल में लक्ष्य को प्राप्त करने वाली पहली सरकार है', 'पटना: BMP कैंप में पुरुष और महिला कांस्टेबल ने गोली मारकर की खुदकुशी, जांच में जुटी पुलिस    की रिपोर्ट   ', 'कोई भी कांग्रेसी, ऊंची छत पर, रेलवे लाइन पर, ऊंची बिल्डिंग पर, एकांत जगह पर, कुए के पास दिखाई दे, तुरंत पुलिस को सूचित करें,     विलुप्त होती हुई प्रजातियो को बचाना हमारा फर्ज है।', 'अंडरवर्ल्ड डॉन छोटा राजन के भाई को बीजेपी द्वारा टिकट मिला है।  ', 'RT  : इन पंचर छापों को कोन समझाए कि उनके रोजगार में कमी का कारण मोदी नहीं    ट्यूब लैस टायर है.    ']\n",
      "Task Labels:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Viewing Created Dataset\n",
    "\n",
    "print(\"Tweets:\")\n",
    "print(tweets[0: 5])\n",
    "\n",
    "print(\"Raw Texts:\")\n",
    "print(raw_tweet_texts[0: 5])\n",
    "\n",
    "print(\"Hashtags:\")\n",
    "print(hashtags[0: 5])\n",
    "\n",
    "print(\"Smileys:\")\n",
    "print(smileys[0: 5])\n",
    "\n",
    "print(\"Emojis:\")\n",
    "print(emojis[0: 5])\n",
    "\n",
    "print(\"Urls:\")\n",
    "print(urls[0: 5])\n",
    "\n",
    "print(\"Mentions:\")\n",
    "print(mentions[0: 5])\n",
    "\n",
    "print(\"Numbers:\")\n",
    "print(numbers[0: 5])\n",
    "\n",
    "print(\"Reserved Words:\")\n",
    "print(reserveds[0: 5])\n",
    "print(\"Cleaned\")\n",
    "print(clean[0:5])\n",
    "print(\"Task Labels:\")\n",
    "print(task_1_labels[0: 5])\n",
    "# print(task_2_labels[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "LUrX_FPcIVQR",
    "outputId": "138a1162-3919-43e3-c396-a1745c15b244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Descriptions:\n",
      "[[], [], ['folded hands', 'face with tears of joy', 'thumbs up'], [], ['backhand index pointing down', 'face with tears of joy', 'face with tears of joy', 'face with tears of joy', 'face with tears of joy']]\n"
     ]
    }
   ],
   "source": [
    "# Generating Emoji Texts\n",
    "emoji_texts = []\n",
    "\n",
    "for emo_list in emojis:\n",
    "  texts = []\n",
    "  for emoji in emo_list:\n",
    "    text = emotext(emoji)\n",
    "    texts.append(text.replace(\"_\", \" \"))\n",
    "  emoji_texts.append(texts)\n",
    "\n",
    "print(\"Emoji Descriptions:\")\n",
    "print(emoji_texts[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "9PWvCf65IXQT",
    "outputId": "d3d1cb70-d883-4adf-8536-9302bfcd126e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented Hashtags:\n",
      "[[], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Segmenting Hashtags\n",
    "segmented_hashtags = []\n",
    "\n",
    "for hashset in hashtags:\n",
    "  segmented_set = []\n",
    "  for tag in hashset:\n",
    "    word = tag[1: ]\n",
    "    # removing the hash symbol\n",
    "    segmented_set.append(seg_tw.segment(word))\n",
    "  segmented_hashtags.append(segmented_set)\n",
    "\n",
    "print(\"Segmented Hashtags:\")\n",
    "print(segmented_hashtags[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGPepLXoJsqR"
   },
   "outputs": [],
   "source": [
    "name = dest + datatype+'.pickle'\n",
    "dickie = {}\n",
    "dickie['tweet_id'] = tweet_ids\n",
    "if datatype != 'test':\n",
    "    dickie['task_1'] = task_1_labels\n",
    "dickie['full_tweet'] = tweets\n",
    "dickie['tweet_raw_text'] = raw_tweet_texts\n",
    "dickie['hashtags'] = hashtags\n",
    "dickie['smiley'] = smileys\n",
    "dickie['emoji'] = emojis\n",
    "dickie['url'] = urls\n",
    "dickie['mentions'] = mentions\n",
    "dickie['numerals'] = numbers\n",
    "dickie['reserved_word'] = reserveds\n",
    "dickie['emotext'] = emoji_texts\n",
    "dickie['segmented_hash'] = segmented_hashtags\n",
    "dickie['clean'] = clean\n",
    "with open(name, 'wb') as f:\n",
    "  pickle.dump(dickie, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FK4uO1J1LeIC",
    "outputId": "d2839abd-0419-4ba5-e422-cf2914318b4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653]\n"
     ]
    }
   ],
   "source": [
    "with open(name, 'rb') as f:\n",
    "  try_dict = pickle.load(f)\n",
    "\n",
    "sizes = []\n",
    "for key in try_dict.keys():\n",
    "  sizes.append(len(try_dict[key]))\n",
    "\n",
    "# Verifying if all sizes are equal\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3FUMYDP9IaXe"
   },
   "outputs": [],
   "source": [
    "# ^_^ Thank You"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxDfY6jG78kns9fmcJq+Qh",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tweet_processor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
