{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Hate-Speech-Detection/blob/master/tweet_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SzZYXq3ER6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: xlrd in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip install xlrd\n",
    "import xlrd\n",
    "import re\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "cOqUHRECEX9o",
    "outputId": "0911749c-b53f-4f9e-bb3e-512d1b1d4be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: ekphrasis in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (0.5.1)\n",
      "Requirement already satisfied: numpy in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (1.19.4)\n",
      "Requirement already satisfied: matplotlib in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (3.3.3)\n",
      "Requirement already satisfied: tqdm in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (4.55.0)\n",
      "Requirement already satisfied: nltk in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (3.5)\n",
      "Requirement already satisfied: ftfy in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (5.8)\n",
      "Requirement already satisfied: ujson in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (4.0.1)\n",
      "Requirement already satisfied: colorama in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (0.4.4)\n",
      "Requirement already satisfied: termcolor in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ekphrasis) (1.1.0)\n",
      "Requirement already satisfied: wcwidth in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from ftfy->ekphrasis) (0.2.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from matplotlib->ekphrasis) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from matplotlib->ekphrasis) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from matplotlib->ekphrasis) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from matplotlib->ekphrasis) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from matplotlib->ekphrasis) (1.3.1)\n",
      "Requirement already satisfied: six in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib->ekphrasis) (1.15.0)\n",
      "Requirement already satisfied: click in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from nltk->ekphrasis) (7.1.2)\n",
      "Requirement already satisfied: regex in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from nltk->ekphrasis) (2020.11.13)\n",
      "Requirement already satisfied: joblib in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (from nltk->ekphrasis) (1.0.0)\n",
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "!pip install ekphrasis\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "# to leverage word statistics from Twitter\n",
    "seg_tw = Segmenter(corpus = \"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cvfnKs-dEZwN",
    "outputId": "0d6002e5-ae50-4bd0-8f2d-6d4ac61ffd32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: tweet-preprocessor in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "!pip install tweet-preprocessor\n",
    "import preprocessor as tweet_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Wk2BUZTuEdN1",
    "outputId": "ae9779a1-b187-4648-a971-5a82e12ce5fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: emot in /home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages (2.1)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "!pip install emot\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqibDmdYEjIK"
   },
   "source": [
    "#### *Raw Datasets are hosted [here](https://drive.google.com/drive/folders/1TuHRQQ41lK9oXJhlhsRiMjMnczuKn_kF?usp=sharing).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SmRsC-wmEhdx",
    "outputId": "d867c2c1-b442-4578-8046-429c47068dc6"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fD2Ksnv6EmaS"
   },
   "outputs": [],
   "source": [
    "def make_list(proc_obj):\n",
    "  if proc_obj == None:\n",
    "    return []\n",
    "  \n",
    "  store = []\n",
    "  for unit in proc_obj:\n",
    "    store.append(unit.match)\n",
    "  \n",
    "  return store\n",
    "\n",
    "def emotext(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PI524oqVE026"
   },
   "outputs": [],
   "source": [
    "is_hindi = 1\n",
    "# change to test while running on the test set\n",
    "datatype = \"test\"\n",
    "# location of the tsv file\n",
    "file_name = \"../data/\"+datatype+\".tsv\"\n",
    "dest = '../temp/preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNNoGyv8E5th"
   },
   "outputs": [],
   "source": [
    "# Initializing Lists\n",
    "datapoints_count = 0\n",
    "see_index = True\n",
    "\n",
    "tweets = []\n",
    "raw_tweet_texts = []\n",
    "tokenized_tweets = []\n",
    "hashtags = []\n",
    "smileys = []\n",
    "emojis = []\n",
    "urls = []\n",
    "mentions = []\n",
    "numbers = []\n",
    "reserveds = []\n",
    "clean = []\n",
    "task_1_labels = []\n",
    "task_2_labels = []\n",
    "tweet_ids = []\n",
    "hasoc_ID = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsKZHtQ-FvOw"
   },
   "outputs": [],
   "source": [
    "def strip_list(listie):\n",
    "  stripped = []\n",
    "  for item in listie:\n",
    "    stripped.append(item.strip())\n",
    "  return stripped\n",
    "\n",
    "def hindi_clean(line, parse_obj):\n",
    "  # beta\n",
    "  tokens = line.replace(\":\", \" : \").replace(\",\", \" , \").replace(\";\", \" ; \").split(\" \")\n",
    "  valid_stri = \"\"\n",
    "\n",
    "  for raw_token in tokens:\n",
    "    token = raw_token.strip()\n",
    "    if token in strip_list(make_list(parse_obj.hashtags)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.smileys)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.emojis)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.urls)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.mentions)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.numbers)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.reserved)):\n",
    "      continue\n",
    "    valid_stri = valid_stri + \" \" + token\n",
    "  return valid_stri.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TuJvskdE95H"
   },
   "outputs": [],
   "source": [
    "if datatype == 'train' or datatype=='valid':\n",
    "#     workbook = xlrd.open_workbook(file_name)\n",
    "#     sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "#     for row in range(sheet.nrows):\n",
    "#         line = sheet.row_values(row)\n",
    "\n",
    "    file = open(file_name, 'r')\n",
    "    count = 0\n",
    "    file_reader = csv.reader(file, delimiter = \"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for line in file_reader:\n",
    "#         if see_index == True:\n",
    "#             see_index = False\n",
    "#             continue\n",
    "\n",
    "        datapoints_count += 1\n",
    "        tweet_ids.append(line[0])\n",
    "        task_1_labels.append(line[2])\n",
    "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
    "        temp = line[1].replace(\"\\n\", \" \")\n",
    "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
    "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
    "        tag = strip_list(make_list(parse_obj.hashtags))\n",
    "        hashtags.append(tag)\n",
    "        smiley = strip_list(make_list(parse_obj.smileys))\n",
    "        smileys.append(smiley)\n",
    "        emoji = strip_list(make_list(parse_obj.emojis))\n",
    "        emojis.append(emoji)\n",
    "        url = strip_list(make_list(parse_obj.urls))\n",
    "        urls.append(url)\n",
    "        mention = strip_list(make_list(parse_obj.mentions))\n",
    "        mentions.append(mention)\n",
    "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "        for i in url + mention + smiley + emoji + tag:\n",
    "#             print(i, tem)\n",
    "            temp = temp.replace(i, \" \")\n",
    "        clean.append(temp)\n",
    "        if is_hindi == 0:\n",
    "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
    "        else:\n",
    "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
    "\n",
    "    print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoints_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9gtRGkIkNmIv",
    "outputId": "9e46fa44-4a8f-44af-884f-9086be627719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Datapoints: 1653\n"
     ]
    }
   ],
   "source": [
    "if datatype == 'test':\n",
    "    file = open(file_name, 'r')\n",
    "    file_reader = csv.reader(file, delimiter = \"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for line in file_reader:\n",
    "        if see_index == True:\n",
    "            see_index = False\n",
    "            continue\n",
    "#         print(line)\n",
    "        datapoints_count += 1\n",
    "        tweet_ids.append(line[0])\n",
    "#         task_1_labels.append(line[2])\n",
    "#         task_2_labels.append(line[3])\n",
    "#         hasoc_ID.append(line[4])\n",
    "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
    "\n",
    "        temp = line[1].replace(\"\\n\", \" \")\n",
    "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
    "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
    "        tag = strip_list(make_list(parse_obj.hashtags))\n",
    "        hashtags.append(tag)\n",
    "        smiley = strip_list(make_list(parse_obj.smileys))\n",
    "        smileys.append(smiley)\n",
    "        emoji = strip_list(make_list(parse_obj.emojis))\n",
    "        emojis.append(emoji)\n",
    "        url = strip_list(make_list(parse_obj.urls))\n",
    "        urls.append(url)\n",
    "        mention = strip_list(make_list(parse_obj.mentions))\n",
    "        mentions.append(mention)\n",
    "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "        for i in url + mention + smiley + emoji + tag:\n",
    "#             print(i, tem)\n",
    "            temp = temp.replace(i, \" \")\n",
    "        clean.append(temp)\n",
    "        if is_hindi == 0:\n",
    "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
    "        else:\n",
    "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
    "\n",
    "    print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "Gd_fy5REFxUX",
    "outputId": "31bd67fc-95aa-4699-8df2-678b0e6b06d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets:\n",
      "['‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§§‡§æ ‡§π‡•à 20 ‡§ï‡§∞‡•ã‡§° ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§¶‡•á ‡§¶‡§ø‡§Ø‡§æ ‡§π‡•à ‡§µ‡§π ‡§≠‡•Ä ‡§Æ‡§æ‡§§‡•ç‡§∞ 6 ‡§∏‡§æ‡§Å‡§≤ ‡§Æ‡•á‡§Ç ‡§ö‡§æ‡§∞ ‡§∏‡§æ‡§≤ ‡§Ö‡§≠‡•Ä ‡§¨‡§æ‡§ï‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§π‡§∞ ‡§∏‡§æ‡§≤ ‡§¶‡•ã ‡§ï‡§∞‡•ã‡§°‡§º ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§¶‡•á‡§®‡•á ‡§ï‡§æ ‡§π‡•Ä ‡§µ‡§æ‡§¶‡§æ ‡§•‡§æ 10 ‡§∏‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§®‡§æ ‡§•‡§æ 20 ‡§ï‡§∞‡•ã‡§° ‡§ï‡•ã ‡§≤‡•ã‡§ó‡•ã ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ú‡•ã ‡§Æ‡§æ‡§§‡•ç‡§∞ 6 ‡§∏‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§ï‡•ã ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§™‡§π‡§≤‡•Ä ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§π‡•à', '‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á ‡§ó‡•ã‡§≤‡•Ä ‡§Æ‡§æ‡§∞‡§ï‡§∞ ‡§ï‡•Ä ‡§ñ‡•Å‡§¶‡§ï‡•Å‡§∂‡•Ä, ‡§ú‡§æ‡§Ç‡§ö ‡§Æ‡•á‡§Ç ‡§ú‡•Å‡§ü‡•Ä ‡§™‡•Å‡§≤‡§ø‡§∏  @kumarprakash4u ‡§ï‡•Ä ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü  https://t.co/Dq05hREifM', '‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä‡§Ç‡§ö‡•Ä ‡§¨‡§ø‡§≤‡•ç‡§°‡§ø‡§Ç‡§ó ‡§™‡§∞, ‡§è‡§ï‡§æ‡§Ç‡§§ ‡§ú‡§ó‡§π ‡§™‡§∞, ‡§ï‡•Å‡§è ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§¶‡§ø‡§ñ‡§æ‡§à ‡§¶‡•á, ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§™‡•Å‡§≤‡§ø‡§∏ ‡§ï‡•ã ‡§∏‡•Ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç,üôè üòÇüëç ‡§µ‡§ø‡§≤‡•Å‡§™‡•ç‡§§ ‡§π‡•ã‡§§‡•Ä ‡§π‡•Å‡§à ‡§™‡•ç‡§∞‡§ú‡§æ‡§§‡§ø‡§Ø‡•ã ‡§ï‡•ã ‡§¨‡§ö‡§æ‡§®‡§æ ‡§π‡§Æ‡§æ‡§∞‡§æ ‡§´‡§∞‡•ç‡§ú ‡§π‡•à‡•§', '‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ü‡§ø‡§ï‡§ü ‡§Æ‡§ø‡§≤‡§æ ‡§π‡•à‡•§  ', 'RT @_Pb_swain_: ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§ï‡§Æ‡•Ä ‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§Æ‡•ã‡§¶‡•Ä ‡§®‡§π‡•Ä‡§Ç  üëá ‡§ü‡•ç‡§Ø‡•Ç‡§¨ ‡§≤‡•à‡§∏ ‡§ü‡§æ‡§Ø‡§∞ ‡§π‡•à.üòÇüòÇüòÇüòÇ']\n",
      "Raw Texts:\n",
      "['‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§§‡§æ ‡§π‡•à ‡§ï‡§∞‡•ã‡§° ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§¶‡•á ‡§¶‡§ø‡§Ø‡§æ ‡§π‡•à ‡§µ‡§π ‡§≠‡•Ä ‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§∏‡§æ‡§Å‡§≤ ‡§Æ‡•á‡§Ç ‡§ö‡§æ‡§∞ ‡§∏‡§æ‡§≤ ‡§Ö‡§≠‡•Ä ‡§¨‡§æ‡§ï‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§π‡§∞ ‡§∏‡§æ‡§≤ ‡§¶‡•ã ‡§ï‡§∞‡•ã‡§°‡§º ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§¶‡•á‡§®‡•á ‡§ï‡§æ ‡§π‡•Ä ‡§µ‡§æ‡§¶‡§æ ‡§•‡§æ ‡§∏‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§®‡§æ ‡§•‡§æ ‡§ï‡§∞‡•ã‡§° ‡§ï‡•ã ‡§≤‡•ã‡§ó‡•ã ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ú‡•ã ‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§∏‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§ï‡•ã ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§™‡§π‡§≤‡•Ä ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§π‡•à', '‡§™‡§ü‡§®‡§æ :  BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á ‡§ó‡•ã‡§≤‡•Ä ‡§Æ‡§æ‡§∞‡§ï‡§∞ ‡§ï‡•Ä ‡§ñ‡•Å‡§¶‡§ï‡•Å‡§∂‡•Ä ,  ‡§ú‡§æ‡§Ç‡§ö ‡§Æ‡•á‡§Ç ‡§ú‡•Å‡§ü‡•Ä ‡§™‡•Å‡§≤‡§ø‡§∏  ‡§ï‡•Ä ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü  https : //t.co/Dq05hREifM', '‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä ,  ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞ ,  ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞ ,  ‡§ä‡§Ç‡§ö‡•Ä ‡§¨‡§ø‡§≤‡•ç‡§°‡§ø‡§Ç‡§ó ‡§™‡§∞ ,  ‡§è‡§ï‡§æ‡§Ç‡§§ ‡§ú‡§ó‡§π ‡§™‡§∞ ,  ‡§ï‡•Å‡§è ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§¶‡§ø‡§ñ‡§æ‡§à ‡§¶‡•á ,  ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§™‡•Å‡§≤‡§ø‡§∏ ‡§ï‡•ã ‡§∏‡•Ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç , üòÇüëç ‡§µ‡§ø‡§≤‡•Å‡§™‡•ç‡§§ ‡§π‡•ã‡§§‡•Ä ‡§π‡•Å‡§à ‡§™‡•ç‡§∞‡§ú‡§æ‡§§‡§ø‡§Ø‡•ã ‡§ï‡•ã ‡§¨‡§ö‡§æ‡§®‡§æ ‡§π‡§Æ‡§æ‡§∞‡§æ ‡§´‡§∞‡•ç‡§ú ‡§π‡•à‡•§', '‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ü‡§ø‡§ï‡§ü ‡§Æ‡§ø‡§≤‡§æ ‡§π‡•à‡•§', ':  ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§ï‡§Æ‡•Ä ‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§Æ‡•ã‡§¶‡•Ä ‡§®‡§π‡•Ä‡§Ç  ‡§ü‡•ç‡§Ø‡•Ç‡§¨ ‡§≤‡•à‡§∏ ‡§ü‡§æ‡§Ø‡§∞ ‡§π‡•à.üòÇüòÇüòÇüòÇ']\n",
      "Hashtags:\n",
      "[[], [], [], [], []]\n",
      "Smileys:\n",
      "[[], [], [], [], []]\n",
      "Emojis:\n",
      "[[], [], ['üôè', 'üòÇ', 'üëç'], [], ['üëá', 'üòÇ', 'üòÇ', 'üòÇ', 'üòÇ']]\n",
      "Urls:\n",
      "[[], ['https://t.co/Dq05hREifM'], [], [], []]\n",
      "Mentions:\n",
      "[[], ['@kumarprakash4u'], [], [], ['@_Pb_swain_']]\n",
      "Numbers:\n",
      "[['20', '6', '10', '20', '6'], [], [], [], []]\n",
      "Reserved Words:\n",
      "[[], [], [], [], ['RT']]\n",
      "Cleaned\n",
      "['‡§ï‡•Ä‡§∏ ‡§ï‡•Ä ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è ‡§´‡§ø‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§π‡§®‡§æ ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§§‡§æ ‡§π‡•à 20 ‡§ï‡§∞‡•ã‡§° ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§¶‡•á ‡§¶‡§ø‡§Ø‡§æ ‡§π‡•à ‡§µ‡§π ‡§≠‡•Ä ‡§Æ‡§æ‡§§‡•ç‡§∞ 6 ‡§∏‡§æ‡§Å‡§≤ ‡§Æ‡•á‡§Ç ‡§ö‡§æ‡§∞ ‡§∏‡§æ‡§≤ ‡§Ö‡§≠‡•Ä ‡§¨‡§æ‡§ï‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§π‡§∞ ‡§∏‡§æ‡§≤ ‡§¶‡•ã ‡§ï‡§∞‡•ã‡§°‡§º ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§¶‡•á‡§®‡•á ‡§ï‡§æ ‡§π‡•Ä ‡§µ‡§æ‡§¶‡§æ ‡§•‡§æ 10 ‡§∏‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§®‡§æ ‡§•‡§æ 20 ‡§ï‡§∞‡•ã‡§° ‡§ï‡•ã ‡§≤‡•ã‡§ó‡•ã ‡§ï‡•ã ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§ú‡•ã ‡§Æ‡§æ‡§§‡•ç‡§∞ 6 ‡§∏‡§æ‡§≤ ‡§Æ‡•á‡§Ç ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§ï‡•ã ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§™‡§π‡§≤‡•Ä ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§π‡•à', '‡§™‡§ü‡§®‡§æ: BMP ‡§ï‡•à‡§Ç‡§™ ‡§Æ‡•á‡§Ç ‡§™‡•Å‡§∞‡•Å‡§∑ ‡§î‡§∞ ‡§Æ‡§π‡§ø‡§≤‡§æ ‡§ï‡§æ‡§Ç‡§∏‡•ç‡§ü‡•á‡§¨‡§≤ ‡§®‡•á ‡§ó‡•ã‡§≤‡•Ä ‡§Æ‡§æ‡§∞‡§ï‡§∞ ‡§ï‡•Ä ‡§ñ‡•Å‡§¶‡§ï‡•Å‡§∂‡•Ä, ‡§ú‡§æ‡§Ç‡§ö ‡§Æ‡•á‡§Ç ‡§ú‡•Å‡§ü‡•Ä ‡§™‡•Å‡§≤‡§ø‡§∏    ‡§ï‡•Ä ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü   ', '‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏‡•Ä, ‡§ä‡§Ç‡§ö‡•Ä ‡§õ‡§§ ‡§™‡§∞, ‡§∞‡•á‡§≤‡§µ‡•á ‡§≤‡§æ‡§á‡§® ‡§™‡§∞, ‡§ä‡§Ç‡§ö‡•Ä ‡§¨‡§ø‡§≤‡•ç‡§°‡§ø‡§Ç‡§ó ‡§™‡§∞, ‡§è‡§ï‡§æ‡§Ç‡§§ ‡§ú‡§ó‡§π ‡§™‡§∞, ‡§ï‡•Å‡§è ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§¶‡§ø‡§ñ‡§æ‡§à ‡§¶‡•á, ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§™‡•Å‡§≤‡§ø‡§∏ ‡§ï‡•ã ‡§∏‡•Ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç,     ‡§µ‡§ø‡§≤‡•Å‡§™‡•ç‡§§ ‡§π‡•ã‡§§‡•Ä ‡§π‡•Å‡§à ‡§™‡•ç‡§∞‡§ú‡§æ‡§§‡§ø‡§Ø‡•ã ‡§ï‡•ã ‡§¨‡§ö‡§æ‡§®‡§æ ‡§π‡§Æ‡§æ‡§∞‡§æ ‡§´‡§∞‡•ç‡§ú ‡§π‡•à‡•§', '‡§Ö‡§Ç‡§°‡§∞‡§µ‡§∞‡•ç‡§≤‡•ç‡§° ‡§°‡•â‡§® ‡§õ‡•ã‡§ü‡§æ ‡§∞‡§æ‡§ú‡§® ‡§ï‡•á ‡§≠‡§æ‡§à ‡§ï‡•ã ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§ü‡§ø‡§ï‡§ü ‡§Æ‡§ø‡§≤‡§æ ‡§π‡•à‡•§  ', 'RT  : ‡§á‡§® ‡§™‡§Ç‡§ö‡§∞ ‡§õ‡§æ‡§™‡•ã‡§Ç ‡§ï‡•ã ‡§ï‡•ã‡§® ‡§∏‡§Æ‡§ù‡§æ‡§è ‡§ï‡§ø ‡§â‡§®‡§ï‡•á ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§ï‡§Æ‡•Ä ‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§Æ‡•ã‡§¶‡•Ä ‡§®‡§π‡•Ä‡§Ç    ‡§ü‡•ç‡§Ø‡•Ç‡§¨ ‡§≤‡•à‡§∏ ‡§ü‡§æ‡§Ø‡§∞ ‡§π‡•à.    ']\n",
      "Task Labels:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Viewing Created Dataset\n",
    "\n",
    "print(\"Tweets:\")\n",
    "print(tweets[0: 5])\n",
    "\n",
    "print(\"Raw Texts:\")\n",
    "print(raw_tweet_texts[0: 5])\n",
    "\n",
    "print(\"Hashtags:\")\n",
    "print(hashtags[0: 5])\n",
    "\n",
    "print(\"Smileys:\")\n",
    "print(smileys[0: 5])\n",
    "\n",
    "print(\"Emojis:\")\n",
    "print(emojis[0: 5])\n",
    "\n",
    "print(\"Urls:\")\n",
    "print(urls[0: 5])\n",
    "\n",
    "print(\"Mentions:\")\n",
    "print(mentions[0: 5])\n",
    "\n",
    "print(\"Numbers:\")\n",
    "print(numbers[0: 5])\n",
    "\n",
    "print(\"Reserved Words:\")\n",
    "print(reserveds[0: 5])\n",
    "print(\"Cleaned\")\n",
    "print(clean[0:5])\n",
    "print(\"Task Labels:\")\n",
    "print(task_1_labels[0: 5])\n",
    "# print(task_2_labels[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "LUrX_FPcIVQR",
    "outputId": "138a1162-3919-43e3-c396-a1745c15b244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Descriptions:\n",
      "[[], [], ['folded hands', 'face with tears of joy', 'thumbs up'], [], ['backhand index pointing down', 'face with tears of joy', 'face with tears of joy', 'face with tears of joy', 'face with tears of joy']]\n"
     ]
    }
   ],
   "source": [
    "# Generating Emoji Texts\n",
    "emoji_texts = []\n",
    "\n",
    "for emo_list in emojis:\n",
    "  texts = []\n",
    "  for emoji in emo_list:\n",
    "    text = emotext(emoji)\n",
    "    texts.append(text.replace(\"_\", \" \"))\n",
    "  emoji_texts.append(texts)\n",
    "\n",
    "print(\"Emoji Descriptions:\")\n",
    "print(emoji_texts[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "9PWvCf65IXQT",
    "outputId": "d3d1cb70-d883-4adf-8536-9302bfcd126e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented Hashtags:\n",
      "[[], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Segmenting Hashtags\n",
    "segmented_hashtags = []\n",
    "\n",
    "for hashset in hashtags:\n",
    "  segmented_set = []\n",
    "  for tag in hashset:\n",
    "    word = tag[1: ]\n",
    "    # removing the hash symbol\n",
    "    segmented_set.append(seg_tw.segment(word))\n",
    "  segmented_hashtags.append(segmented_set)\n",
    "\n",
    "print(\"Segmented Hashtags:\")\n",
    "print(segmented_hashtags[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGPepLXoJsqR"
   },
   "outputs": [],
   "source": [
    "name = dest + datatype+'.pickle'\n",
    "dickie = {}\n",
    "dickie['tweet_id'] = tweet_ids\n",
    "if datatype != 'test':\n",
    "    dickie['task_1'] = task_1_labels\n",
    "dickie['full_tweet'] = tweets\n",
    "dickie['tweet_raw_text'] = raw_tweet_texts\n",
    "dickie['hashtags'] = hashtags\n",
    "dickie['smiley'] = smileys\n",
    "dickie['emoji'] = emojis\n",
    "dickie['url'] = urls\n",
    "dickie['mentions'] = mentions\n",
    "dickie['numerals'] = numbers\n",
    "dickie['reserved_word'] = reserveds\n",
    "dickie['emotext'] = emoji_texts\n",
    "dickie['segmented_hash'] = segmented_hashtags\n",
    "dickie['clean'] = clean\n",
    "with open(name, 'wb') as f:\n",
    "  pickle.dump(dickie, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FK4uO1J1LeIC",
    "outputId": "d2839abd-0419-4ba5-e422-cf2914318b4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653]\n"
     ]
    }
   ],
   "source": [
    "with open(name, 'rb') as f:\n",
    "  try_dict = pickle.load(f)\n",
    "\n",
    "sizes = []\n",
    "for key in try_dict.keys():\n",
    "  sizes.append(len(try_dict[key]))\n",
    "\n",
    "# Verifying if all sizes are equal\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3FUMYDP9IaXe"
   },
   "outputs": [],
   "source": [
    "# ^_^ Thank You"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxDfY6jG78kns9fmcJq+Qh",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tweet_processor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
