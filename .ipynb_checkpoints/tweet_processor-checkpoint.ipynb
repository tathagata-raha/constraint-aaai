{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Hate-Speech-Detection/blob/master/tweet_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:01:20.136074Z",
     "iopub.status.busy": "2020-12-05T23:01:20.135789Z",
     "iopub.status.idle": "2020-12-05T23:01:20.871077Z",
     "shell.execute_reply": "2020-12-05T23:01:20.870559Z",
     "shell.execute_reply.started": "2020-12-05T23:01:20.136001Z"
    },
    "id": "5SzZYXq3ER6b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlrd\n",
    "import re\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-12-05T23:01:24.681141Z",
     "iopub.status.busy": "2020-12-05T23:01:24.680946Z"
    },
    "id": "cOqUHRECEX9o",
    "outputId": "0911749c-b53f-4f9e-bb3e-512d1b1d4be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: ekphrasis in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (0.5.1)\n",
      "Requirement already satisfied: numpy in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (1.19.2)\n",
      "Requirement already satisfied: matplotlib in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (3.3.1)\n",
      "Requirement already satisfied: colorama in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (0.4.3)\n",
      "Requirement already satisfied: ftfy in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (5.8)\n",
      "Requirement already satisfied: ujson in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (3.2.0)\n",
      "Requirement already satisfied: nltk in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (3.5)\n",
      "Requirement already satisfied: tqdm in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (4.51.0)\n",
      "Requirement already satisfied: termcolor in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (1.1.0)\n",
      "Requirement already satisfied: wcwidth in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ftfy->ekphrasis) (0.2.5)\n",
      "Requirement already satisfied: numpy in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (1.19.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from matplotlib->ekphrasis) (1.3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from matplotlib->ekphrasis) (7.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from matplotlib->ekphrasis) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from matplotlib->ekphrasis) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from matplotlib->ekphrasis) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from matplotlib->ekphrasis) (0.10.0)\n",
      "Requirement already satisfied: six in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->ekphrasis) (1.15.0)\n",
      "Requirement already satisfied: click in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from nltk->ekphrasis) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from nltk->ekphrasis) (0.17.0)\n",
      "Requirement already satisfied: regex in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from nltk->ekphrasis) (2020.7.14)\n",
      "Requirement already satisfied: tqdm in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from ekphrasis) (4.51.0)\n",
      "Requirement already satisfied: six in /home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->ekphrasis) (1.15.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for google-pasta: [Errno 2] No such file or directory: '/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/google_pasta-0.2.0.dist-info/METADATA'\u001b[0m\n",
      "Word statistics files not found!\n",
      "Downloading... "
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "!pip install ekphrasis\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "# to leverage word statistics from Twitter\n",
    "seg_tw = Segmenter(corpus = \"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cvfnKs-dEZwN",
    "outputId": "0d6002e5-ae50-4bd0-8f2d-6d4ac61ffd32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "!pip install tweet-preprocessor\n",
    "import preprocessor as tweet_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Wk2BUZTuEdN1",
    "outputId": "ae9779a1-b187-4648-a971-5a82e12ce5fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in /usr/local/lib/python3.6/dist-packages (2.1)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "!pip install emot\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqibDmdYEjIK"
   },
   "source": [
    "#### *Raw Datasets are hosted [here](https://drive.google.com/drive/folders/1TuHRQQ41lK9oXJhlhsRiMjMnczuKn_kF?usp=sharing).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SmRsC-wmEhdx",
    "outputId": "d867c2c1-b442-4578-8046-429c47068dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fD2Ksnv6EmaS"
   },
   "outputs": [],
   "source": [
    "def make_list(proc_obj):\n",
    "  if proc_obj == None:\n",
    "    return []\n",
    "  \n",
    "  store = []\n",
    "  for unit in proc_obj:\n",
    "    store.append(unit.match)\n",
    "  \n",
    "  return store\n",
    "\n",
    "def emotext(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PI524oqVE026"
   },
   "outputs": [],
   "source": [
    "# For 2020 Datasets\n",
    "\n",
    "is_hindi = 0\n",
    "\n",
    "# For Train Data\n",
    "# datatype = \"train\"\n",
    "# For English\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/english.xlsx\"\n",
    "\n",
    "# For Hindi\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/hindi.xlsx\"\n",
    "# is_hindi = 1\n",
    "\n",
    "# For German\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/german.xlsx\"\n",
    "\n",
    "# For Test Data\n",
    "datatype = \"test\"\n",
    "# For English\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/english_test_1509.csv\"\n",
    "\n",
    "# For Hindi\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/hindi_test_1509.csv\"\n",
    "# is_hindi = 1\n",
    "\n",
    "# For German\n",
    "file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/german_test_1509.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNNoGyv8E5th"
   },
   "outputs": [],
   "source": [
    "# Initializing Lists\n",
    "datapoints_count = 0\n",
    "see_index = True\n",
    "\n",
    "tweets = []\n",
    "raw_tweet_texts = []\n",
    "tokenized_tweets = []\n",
    "hashtags = []\n",
    "smileys = []\n",
    "emojis = []\n",
    "urls = []\n",
    "mentions = []\n",
    "numbers = []\n",
    "reserveds = []\n",
    "\n",
    "task_1_labels = []\n",
    "task_2_labels = []\n",
    "tweet_ids = []\n",
    "hasoc_ID = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsKZHtQ-FvOw"
   },
   "outputs": [],
   "source": [
    "def strip_list(listie):\n",
    "  stripped = []\n",
    "  for item in listie:\n",
    "    stripped.append(item.strip())\n",
    "  return stripped\n",
    "\n",
    "def hindi_clean(line, parse_obj):\n",
    "  # beta\n",
    "  tokens = line.replace(\":\", \" : \").replace(\",\", \" , \").replace(\";\", \" ; \").split(\" \")\n",
    "  valid_stri = \"\"\n",
    "\n",
    "  for raw_token in tokens:\n",
    "    token = raw_token.strip()\n",
    "    if token in strip_list(make_list(parse_obj.hashtags)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.smileys)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.emojis)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.urls)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.mentions)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.numbers)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.reserved)):\n",
    "      continue\n",
    "    valid_stri = valid_stri + \" \" + token\n",
    "  return valid_stri.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TuJvskdE95H"
   },
   "outputs": [],
   "source": [
    "if datatype == 'train':\n",
    "    workbook = xlrd.open_workbook(file_name)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    for row in range(sheet.nrows):\n",
    "        line = sheet.row_values(row)\n",
    "\n",
    "    file = open(file_name, 'r')\n",
    "    file_reader = csv.reader(file, delimiter = \",\")\n",
    "    for line in file_reader:\n",
    "        if see_index == True:\n",
    "            see_index = False\n",
    "            continue\n",
    "\n",
    "        datapoints_count += 1\n",
    "        tweet_ids.append(line[0])\n",
    "        task_1_labels.append(line[2])\n",
    "        task_2_labels.append(line[3])\n",
    "        hasoc_ID.append(line[4])\n",
    "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
    "\n",
    "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
    "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
    "        hashtags.append(strip_list(make_list(parse_obj.hashtags)))\n",
    "        smileys.append(strip_list(make_list(parse_obj.smileys)))\n",
    "        emojis.append(strip_list(make_list(parse_obj.emojis)))\n",
    "        urls.append(strip_list(make_list(parse_obj.urls)))\n",
    "        mentions.append(strip_list(make_list(parse_obj.mentions)))\n",
    "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "\n",
    "        if is_hindi == 0:\n",
    "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
    "        else:\n",
    "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
    "\n",
    "    print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9gtRGkIkNmIv",
    "outputId": "9e46fa44-4a8f-44af-884f-9086be627719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Datapoints: 526\n"
     ]
    }
   ],
   "source": [
    "if datatype == 'test':\n",
    "    file = open(file_name, 'r')\n",
    "    file_reader = csv.reader(file, delimiter = \",\")\n",
    "    for line in file_reader:\n",
    "        if see_index == True:\n",
    "            see_index = False\n",
    "            continue\n",
    "\n",
    "        datapoints_count += 1\n",
    "        tweet_ids.append(line[0])\n",
    "        task_1_labels.append(line[2])\n",
    "        task_2_labels.append(line[3])\n",
    "        hasoc_ID.append(line[4])\n",
    "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
    "\n",
    "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
    "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
    "        hashtags.append(strip_list(make_list(parse_obj.hashtags)))\n",
    "        smileys.append(strip_list(make_list(parse_obj.smileys)))\n",
    "        emojis.append(strip_list(make_list(parse_obj.emojis)))\n",
    "        urls.append(strip_list(make_list(parse_obj.urls)))\n",
    "        mentions.append(strip_list(make_list(parse_obj.mentions)))\n",
    "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "\n",
    "        if is_hindi == 0:\n",
    "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
    "        else:\n",
    "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
    "\n",
    "    print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "Gd_fy5REFxUX",
    "outputId": "31bd67fc-95aa-4699-8df2-678b0e6b06d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets:\n",
      "['@derCarsti Boykottieren hÃ¶rt sich besser an. ðŸ’™ðŸ’™', 'RT @ibikus31: Es wird spekuliert, ob Merkel ein Amt in BrÃ¼ssel Ã¼bernimmt. WÃ¤re es so, wie schÃ¤tzen Sie dann die Zukunft der Mitgliedstaatenâ€¦', 'Hat #Hitler wirklich den Krieg in der WÃ¼ste verloren? Nach der #Welt Schlagzeile hat die #Tagesschau nicht reagiert.', 'RT @Beatrix_vStorch: #May tritt in UK unter TrÃ¤nen zurÃ¼ck. Wenn #Merkel zurÃ¼cktritt, dann auch unter TrÃ¤nen â€“ unter den FreudentrÃ¤nen von Mâ€¦', '@justmeDoro Eher nicht. Das GÃ¤nse hauen wieder ab in ihre warmen Ãœberwinterungsquartiere. ðŸ˜…ðŸ˜‚']\n",
      "Raw Texts:\n",
      "['Boykottieren hrt sich besser an.', ': Es wird spekuliert, ob Merkel ein Amt in Brssel bernimmt. Wre es so, wie schtzen Sie dann die Zukunft der Mitgliedstaaten', 'Hat wirklich den Krieg in der Wste verloren? Nach der Schlagzeile hat die nicht reagiert.', ': tritt in UK unter Trnen zurck. Wenn zurcktritt, dann auch unter Trnen unter den Freudentrnen von M', 'Eher nicht. Das Gnse hauen wieder ab in ihre warmen berwinterungsquartiere.']\n",
      "Hashtags:\n",
      "[[], [], ['#Hitler', '#Welt', '#Tagesschau'], ['#May', '#Merkel'], []]\n",
      "Smileys:\n",
      "[[], [], [], [], []]\n",
      "Emojis:\n",
      "[['ðŸ’™', 'ðŸ’™'], [], [], [], ['ðŸ˜…', 'ðŸ˜‚']]\n",
      "Urls:\n",
      "[[], [], [], [], []]\n",
      "Mentions:\n",
      "[['@derCarsti'], ['@ibikus31'], [], ['@Beatrix_vStorch'], ['@justmeDoro']]\n",
      "Numbers:\n",
      "[[], [], [], [], []]\n",
      "Reserved Words:\n",
      "[[], ['RT'], [], ['RT'], []]\n",
      "Task Labels:\n",
      "['NOT', 'NOT', 'NOT', 'NOT', 'NOT']\n",
      "['NONE', 'NONE', 'NONE', 'NONE', 'NONE']\n"
     ]
    }
   ],
   "source": [
    "# Viewing Created Dataset\n",
    "\n",
    "print(\"Tweets:\")\n",
    "print(tweets[0: 5])\n",
    "\n",
    "print(\"Raw Texts:\")\n",
    "print(raw_tweet_texts[0: 5])\n",
    "\n",
    "print(\"Hashtags:\")\n",
    "print(hashtags[0: 5])\n",
    "\n",
    "print(\"Smileys:\")\n",
    "print(smileys[0: 5])\n",
    "\n",
    "print(\"Emojis:\")\n",
    "print(emojis[0: 5])\n",
    "\n",
    "print(\"Urls:\")\n",
    "print(urls[0: 5])\n",
    "\n",
    "print(\"Mentions:\")\n",
    "print(mentions[0: 5])\n",
    "\n",
    "print(\"Numbers:\")\n",
    "print(numbers[0: 5])\n",
    "\n",
    "print(\"Reserved Words:\")\n",
    "print(reserveds[0: 5])\n",
    "\n",
    "print(\"Task Labels:\")\n",
    "print(task_1_labels[0: 5])\n",
    "print(task_2_labels[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "LUrX_FPcIVQR",
    "outputId": "138a1162-3919-43e3-c396-a1745c15b244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Descriptions:\n",
      "[['blue heart', 'blue heart'], [], [], [], ['smiling face with open mouth & cold sweat', 'face with tears of joy']]\n"
     ]
    }
   ],
   "source": [
    "# Generating Emoji Texts\n",
    "emoji_texts = []\n",
    "\n",
    "for emo_list in emojis:\n",
    "  texts = []\n",
    "  for emoji in emo_list:\n",
    "    text = emotext(emoji)\n",
    "    texts.append(text.replace(\"_\", \" \"))\n",
    "  emoji_texts.append(texts)\n",
    "\n",
    "print(\"Emoji Descriptions:\")\n",
    "print(emoji_texts[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "9PWvCf65IXQT",
    "outputId": "d3d1cb70-d883-4adf-8536-9302bfcd126e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented Hashtags:\n",
      "[[], [], ['hitler', 'welt', 'tagesschau'], ['may', 'merkel'], []]\n"
     ]
    }
   ],
   "source": [
    "# Segmenting Hashtags\n",
    "segmented_hashtags = []\n",
    "\n",
    "for hashset in hashtags:\n",
    "  segmented_set = []\n",
    "  for tag in hashset:\n",
    "    word = tag[1: ]\n",
    "    # removing the hash symbol\n",
    "    segmented_set.append(seg_tw.segment(word))\n",
    "  segmented_hashtags.append(segmented_set)\n",
    "\n",
    "print(\"Segmented Hashtags:\")\n",
    "print(segmented_hashtags[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGPepLXoJsqR"
   },
   "outputs": [],
   "source": [
    "name = 'ge_test.pickle'\n",
    "dickie = {}\n",
    "dickie['tweet_id'] = tweet_ids\n",
    "dickie['task_1'] = task_1_labels\n",
    "dickie['task_2'] = task_2_labels\n",
    "dickie['hasoc_id'] = hasoc_ID\n",
    "dickie['full_tweet'] = tweets\n",
    "dickie['tweet_raw_text'] = raw_tweet_texts\n",
    "dickie['hashtags'] = hashtags\n",
    "dickie['smiley'] = smileys\n",
    "dickie['emoji'] = emojis\n",
    "dickie['url'] = urls\n",
    "dickie['mentions'] = mentions\n",
    "dickie['numerals'] = numbers\n",
    "dickie['reserved_word'] = reserveds\n",
    "dickie['emotext'] = emoji_texts\n",
    "dickie['segmented_hash'] = segmented_hashtags\n",
    "\n",
    "with open(name, 'wb') as f:\n",
    "  pickle.dump(dickie, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FK4uO1J1LeIC",
    "outputId": "d2839abd-0419-4ba5-e422-cf2914318b4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[526, 526, 526, 526, 526, 526, 526, 526, 526, 526, 526, 526, 526, 526, 526]\n"
     ]
    }
   ],
   "source": [
    "with open(name, 'rb') as f:\n",
    "  try_dict = pickle.load(f)\n",
    "\n",
    "sizes = []\n",
    "for key in try_dict.keys():\n",
    "  sizes.append(len(try_dict[key]))\n",
    "\n",
    "# Verifying if all sizes are equal\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3FUMYDP9IaXe"
   },
   "outputs": [],
   "source": [
    "# ^_^ Thank You"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxDfY6jG78kns9fmcJq+Qh",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tweet_processor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
