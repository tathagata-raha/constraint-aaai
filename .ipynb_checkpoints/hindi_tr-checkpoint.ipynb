{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:23.219362Z",
     "iopub.status.busy": "2020-11-13T20:26:23.219023Z",
     "iopub.status.idle": "2020-11-13T20:26:29.304847Z",
     "shell.execute_reply": "2020-11-13T20:26:29.304251Z",
     "shell.execute_reply.started": "2020-11-13T20:26:23.219264Z"
    }
   },
   "outputs": [],
   "source": [
    "# Insert code here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertConfig\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sent_encoder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:29.306048Z",
     "iopub.status.busy": "2020-11-13T20:26:29.305865Z",
     "iopub.status.idle": "2020-11-13T20:26:29.522304Z",
     "shell.execute_reply": "2020-11-13T20:26:29.521782Z",
     "shell.execute_reply.started": "2020-11-13T20:26:29.306027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:1\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:29.525617Z",
     "iopub.status.busy": "2020-11-13T20:26:29.525469Z",
     "iopub.status.idle": "2020-11-13T20:26:31.959643Z",
     "shell.execute_reply": "2020-11-13T20:26:31.958876Z",
     "shell.execute_reply.started": "2020-11-13T20:26:29.525598Z"
    }
   },
   "outputs": [],
   "source": [
    "models = ['bert-base-multilingual-cased', 'xlm-roberta-base', 'sagorsarker/bangla-bert-base', 'ai4bharat/indic-bert']\n",
    "model_num = 1\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[model_num])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:31.960846Z",
     "iopub.status.busy": "2020-11-13T20:26:31.960676Z",
     "iopub.status.idle": "2020-11-13T20:26:32.456017Z",
     "shell.execute_reply": "2020-11-13T20:26:32.455524Z",
     "shell.execute_reply.started": "2020-11-13T20:26:31.960824Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train/sub-task-1d-train-hi.tsv', sep='\\t')\n",
    "test = pd.read_csv('data/dev/sub-task-1d-dev-hi.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:32.456933Z",
     "iopub.status.busy": "2020-11-13T20:26:32.456769Z",
     "iopub.status.idle": "2020-11-13T20:26:32.472116Z",
     "shell.execute_reply": "2020-11-13T20:26:32.471701Z",
     "shell.execute_reply.started": "2020-11-13T20:26:32.456912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100425</th>\n",
       "      <td>तो , यही िह अंशहैर्ो आप यहां अंशमेंदेखतेहैंिीए...</td>\n",
       "      <td>phy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14011</th>\n",
       "      <td>फिर C ऑफ 3 का मान 1 से बढ़ा दिया जाता है ।</td>\n",
       "      <td>cse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32349</th>\n",
       "      <td>तो , सीओ • 2 िन्यू आयन है ।</td>\n",
       "      <td>phy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46703</th>\n",
       "      <td>तो , येव्यापक रूप सेहमारे सीखनेके उद्देश्यहैं ...</td>\n",
       "      <td>phy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74887</th>\n",
       "      <td>इसलिए , उदाहरण के लिए , मैं संख्याओं पर इस तरह...</td>\n",
       "      <td>mgmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10238</th>\n",
       "      <td>&amp;nbsp ; तो छीलने की इसी तरह की प्रक्रियाओं को ...</td>\n",
       "      <td>mgmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35838</th>\n",
       "      <td>पहली विशेषता यह है कि किसी को एक डिस - प्रोपोर...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22831</th>\n",
       "      <td>हम कैपिटल लेटर के साथ हाई वोल्टेज साइड के टर्म...</td>\n",
       "      <td>phy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13742</th>\n",
       "      <td>साइट  पर काम की प्रणाली क्या है ?</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92965</th>\n",
       "      <td>इसलिए घटक जैसे : चेतावनी  सिद्धांत है : जहां ए...</td>\n",
       "      <td>com_tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61761</th>\n",
       "      <td>तो , यह हमें पाठकीयता प्रदान करता है और यह हमे...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85953</th>\n",
       "      <td>अब मैं एकप्रसिद्ध प्रसंग का उल्लेखकरना चाहूंगा...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114419</th>\n",
       "      <td>यह सब ठीक है ।</td>\n",
       "      <td>phy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104503</th>\n",
       "      <td>इसलिए , विशेष प्रभावों का न्यायसंगत उपयोग एक अ...</td>\n",
       "      <td>com_tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83370</th>\n",
       "      <td>तो , यह है कि , यही कारण है कि एक QFD का उपयोग...</td>\n",
       "      <td>mgmt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label\n",
       "100425  तो , यही िह अंशहैर्ो आप यहां अंशमेंदेखतेहैंिीए...       phy\n",
       "14011          फिर C ऑफ 3 का मान 1 से बढ़ा दिया जाता है ।       cse\n",
       "32349                         तो , सीओ • 2 िन्यू आयन है ।       phy\n",
       "46703   तो , येव्यापक रूप सेहमारे सीखनेके उद्देश्यहैं ...       phy\n",
       "74887   इसलिए , उदाहरण के लिए , मैं संख्याओं पर इस तरह...      mgmt\n",
       "10238   &nbsp ; तो छीलने की इसी तरह की प्रक्रियाओं को ...      mgmt\n",
       "35838   पहली विशेषता यह है कि किसी को एक डिस - प्रोपोर...     other\n",
       "22831   हम कैपिटल लेटर के साथ हाई वोल्टेज साइड के टर्म...       phy\n",
       "13742                   साइट  पर काम की प्रणाली क्या है ?     other\n",
       "92965   इसलिए घटक जैसे : चेतावनी  सिद्धांत है : जहां ए...  com_tech\n",
       "61761   तो , यह हमें पाठकीयता प्रदान करता है और यह हमे...     other\n",
       "85953   अब मैं एकप्रसिद्ध प्रसंग का उल्लेखकरना चाहूंगा...     other\n",
       "114419                                     यह सब ठीक है ।       phy\n",
       "104503  इसलिए , विशेष प्रभावों का न्यायसंगत उपयोग एक अ...  com_tech\n",
       "83370   तो , यह है कि , यही कारण है कि एक QFD का उपयोग...      mgmt"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:32.472990Z",
     "iopub.status.busy": "2020-11-13T20:26:32.472822Z",
     "iopub.status.idle": "2020-11-13T20:26:32.481840Z",
     "shell.execute_reply": "2020-11-13T20:26:32.481415Z",
     "shell.execute_reply.started": "2020-11-13T20:26:32.472970Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = list(train.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:32.482669Z",
     "iopub.status.busy": "2020-11-13T20:26:32.482509Z",
     "iopub.status.idle": "2020-11-13T20:26:32.485057Z",
     "shell.execute_reply": "2020-11-13T20:26:32.484649Z",
     "shell.execute_reply.started": "2020-11-13T20:26:32.482649Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_encode(val):\n",
    "    return labels.index(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:32.485859Z",
     "iopub.status.busy": "2020-11-13T20:26:32.485705Z",
     "iopub.status.idle": "2020-11-13T20:26:32.536591Z",
     "shell.execute_reply": "2020-11-13T20:26:32.536124Z",
     "shell.execute_reply.started": "2020-11-13T20:26:32.485841Z"
    }
   },
   "outputs": [],
   "source": [
    "train.label = train.label.apply(label_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:32.538544Z",
     "iopub.status.busy": "2020-11-13T20:26:32.538396Z",
     "iopub.status.idle": "2020-11-13T20:26:33.865876Z",
     "shell.execute_reply": "2020-11-13T20:26:33.865354Z",
     "shell.execute_reply.started": "2020-11-13T20:26:32.538525Z"
    }
   },
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "STOPWORDS = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "#     text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "train.text = train.text.apply(clean_text)\n",
    "train.text = train.text.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:33.867203Z",
     "iopub.status.busy": "2020-11-13T20:26:33.867052Z",
     "iopub.status.idle": "2020-11-13T20:26:34.004843Z",
     "shell.execute_reply": "2020-11-13T20:26:34.004356Z",
     "shell.execute_reply.started": "2020-11-13T20:26:33.867183Z"
    }
   },
   "outputs": [],
   "source": [
    "test.label = test.label.apply(label_encode)\n",
    "test = test.reset_index(drop=True)\n",
    "test.text = test.text.apply(clean_text)\n",
    "test.text = test.text.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.005659Z",
     "iopub.status.busy": "2020-11-13T20:26:34.005513Z",
     "iopub.status.idle": "2020-11-13T20:26:34.020767Z",
     "shell.execute_reply": "2020-11-13T20:26:34.020354Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.005640Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['tweet'], train['label'])\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train.text, train.label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.021536Z",
     "iopub.status.busy": "2020-11-13T20:26:34.021391Z",
     "iopub.status.idle": "2020-11-13T20:26:34.024248Z",
     "shell.execute_reply": "2020-11-13T20:26:34.023797Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.021518Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    try:\n",
    "        return len(text.split())\n",
    "    except:\n",
    "        print(text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.025229Z",
     "iopub.status.busy": "2020-11-13T20:26:34.024969Z",
     "iopub.status.idle": "2020-11-13T20:26:34.276336Z",
     "shell.execute_reply": "2020-11-13T20:26:34.275856Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.025209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28.413262516597673, 53046, 136)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "maxw = 0\n",
    "large_count = 0\n",
    "for i in train_x:\n",
    "    temp = count_words(i)\n",
    "    total += temp\n",
    "    maxw = temp if temp > maxw else maxw\n",
    "    large_count += 1 if temp > 128 else 0\n",
    "total/len(train_x), maxw, large_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.277191Z",
     "iopub.status.busy": "2020-11-13T20:26:34.276987Z",
     "iopub.status.idle": "2020-11-13T20:26:34.279974Z",
     "shell.execute_reply": "2020-11-13T20:26:34.279423Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.277171Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "posts = train_x.values\n",
    "categories = train_y.values\n",
    "best_model_path = '/scratch/best_model_hi.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.280789Z",
     "iopub.status.busy": "2020-11-13T20:26:34.280641Z",
     "iopub.status.idle": "2020-11-13T20:26:34.285565Z",
     "shell.execute_reply": "2020-11-13T20:26:34.285097Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.280770Z"
    }
   },
   "outputs": [],
   "source": [
    "posts_test = valid_x.values\n",
    "categories_test = valid_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.286366Z",
     "iopub.status.busy": "2020-11-13T20:26:34.286207Z",
     "iopub.status.idle": "2020-11-13T20:26:34.292376Z",
     "shell.execute_reply": "2020-11-13T20:26:34.291973Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.286347Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_tokenizer(posts, categories):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in posts:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            truncation=True,\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(categories)\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.293156Z",
     "iopub.status.busy": "2020-11-13T20:26:34.293010Z",
     "iopub.status.idle": "2020-11-13T20:26:34.299705Z",
     "shell.execute_reply": "2020-11-13T20:26:34.299303Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.293138Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_dataset(input_ids, attention_masks, labels):\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    train_size = int(0.875 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    # Divide the dataset by randomly selecting samples.\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    print('{:>5,} training samples'.format(train_size))\n",
    "    print('{:>5,} validation samples'.format(val_size))\n",
    "    batch_size = BATCH_SIZE\n",
    "    train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "    validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "    return train_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.300472Z",
     "iopub.status.busy": "2020-11-13T20:26:34.300325Z",
     "iopub.status.idle": "2020-11-13T20:26:34.307774Z",
     "shell.execute_reply": "2020-11-13T20:26:34.307373Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.300453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     models[model_num], # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#     num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "#                     # You can increase this for multi-class tasks.\n",
    "#     output_attentions = False, # Whether the model returns attentions weights.\n",
    "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "# )\n",
    "def build_model(name):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        models[model_num], # Use the 12-layer BERT model, with an uncased vocab.\n",
    "        num_labels = len(labels), # The number of output labels--2 for binary classification.\n",
    "                        # You can increase this for multi-class tasks.\n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "    model.to(device)\n",
    "    params = list(model.named_parameters())\n",
    "    print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "    for p in params[-4:]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.308532Z",
     "iopub.status.busy": "2020-11-13T20:26:34.308385Z",
     "iopub.status.idle": "2020-11-13T20:26:34.313541Z",
     "shell.execute_reply": "2020-11-13T20:26:34.313139Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.308513Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_optimizer(model, train_dataloader):\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.314309Z",
     "iopub.status.busy": "2020-11-13T20:26:34.314164Z",
     "iopub.status.idle": "2020-11-13T20:26:34.319273Z",
     "shell.execute_reply": "2020-11-13T20:26:34.318754Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.314290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.320064Z",
     "iopub.status.busy": "2020-11-13T20:26:34.319917Z",
     "iopub.status.idle": "2020-11-13T20:26:34.325021Z",
     "shell.execute_reply": "2020-11-13T20:26:34.324616Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.320045Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.325892Z",
     "iopub.status.busy": "2020-11-13T20:26:34.325738Z",
     "iopub.status.idle": "2020-11-13T20:26:34.339843Z",
     "shell.execute_reply": "2020-11-13T20:26:34.339439Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.325873Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, train_dataloader, validation_dataloader):\n",
    "    seed_val = 42\n",
    "    torch.cuda.empty_cache()\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    # For each epoch...\n",
    "    prev_loss = 0\n",
    "    prev_acc = 0\n",
    "    first_epoch = 1\n",
    "    for epoch_i in range(0, EPOCHS):\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because \n",
    "            # accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            model.zero_grad()        \n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "            outputs = model(b_input_ids, \n",
    "                                 attention_mask=b_input_mask, \n",
    "                                 labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                outputs = model(b_input_ids, \n",
    "                                       attention_mask=b_input_mask,\n",
    "                                       labels=b_labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "\n",
    "\n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "        if not first_epoch:\n",
    "            if prev_loss < avg_val_loss and prev_acc > avg_val_accuracy:\n",
    "                print(\"Backtracking\")\n",
    "                model = torch.load(best_model_path)\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] /= 2\n",
    "            else:\n",
    "                print(\"Going on track\")\n",
    "                prev_loss = avg_val_loss\n",
    "                prev_acc = avg_val_accuracy\n",
    "                torch.save(model, best_model_path)\n",
    "        else:\n",
    "            print(\"Going on track\")\n",
    "            first_epoch = 0\n",
    "            prev_loss = avg_val_loss\n",
    "            prev_acc = avg_val_accuracy\n",
    "            torch.save(model, best_model_path)\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "#         if epoch_i >= 0:\n",
    "#             inp = input()\n",
    "#             if inp.startswith('y'):\n",
    "#                 break\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    model = torch.load(best_model_path)\n",
    "    return model, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.340679Z",
     "iopub.status.busy": "2020-11-13T20:26:34.340532Z",
     "iopub.status.idle": "2020-11-13T20:26:34.347075Z",
     "shell.execute_reply": "2020-11-13T20:26:34.346669Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.340661Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(posts, categories, model):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in posts:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            truncation=True,\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(categories)\n",
    "    # Set the batch size.  \n",
    "    batch_size = 32\n",
    "    # Create the DataLoader.\n",
    "    prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "    print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "      # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and \n",
    "        # speeding up prediction\n",
    "        with torch.no_grad():\n",
    "          # Forward pass, calculate logit predictions\n",
    "          outputs = model(b_input_ids, \n",
    "                          attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "    print('    DONE.')\n",
    "    flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "    # For each sample, pick the label (0 or 1) with the higher score.\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "    # Combine the correct labels for each batch into a single list.\n",
    "    flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "    print(classification_report(flat_true_labels, flat_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.347834Z",
     "iopub.status.busy": "2020-11-13T20:26:34.347688Z",
     "iopub.status.idle": "2020-11-13T20:26:34.353445Z",
     "shell.execute_reply": "2020-11-13T20:26:34.353044Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.347816Z"
    }
   },
   "outputs": [],
   "source": [
    "def classifier(name):\n",
    "    posts = train_x.values\n",
    "    categories = train_y.values\n",
    "    input_ids, attention_masks, labels = train_tokenizer(posts, categories)\n",
    "    train_dataloader, validation_dataloader = build_dataset(input_ids, attention_masks, labels)\n",
    "    model = build_model(name)\n",
    "    optimizer, scheduler = build_optimizer(model, train_dataloader)\n",
    "    model, training_stats = train(model, optimizer, scheduler, train_dataloader, validation_dataloader)\n",
    "    posts = valid_x.values\n",
    "    categories = valid_y.values\n",
    "    test_model(posts, categories, model)\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-13T20:26:34.354214Z",
     "iopub.status.busy": "2020-11-13T20:26:34.354069Z",
     "iopub.status.idle": "2020-11-13T20:48:08.693902Z",
     "shell.execute_reply": "2020-11-13T20:48:08.692814Z",
     "shell.execute_reply.started": "2020-11-13T20:26:34.354196Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1764: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89,621 training samples\n",
      "12,803 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 203 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "roberta.embeddings.word_embeddings.weight               (250002, 768)\n",
      "roberta.embeddings.position_embeddings.weight             (514, 768)\n",
      "roberta.embeddings.token_type_embeddings.weight             (1, 768)\n",
      "roberta.embeddings.LayerNorm.weight                           (768,)\n",
      "roberta.embeddings.LayerNorm.bias                             (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.query.bias             (768,)\n",
      "roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.key.bias               (768,)\n",
      "roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.value.bias             (768,)\n",
      "roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
      "roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
      "roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
      "roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
      "roberta.encoder.layer.0.output.dense.bias                     (768,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "classifier.dense.weight                                   (768, 768)\n",
      "classifier.dense.bias                                         (768,)\n",
      "classifier.out_proj.weight                                  (7, 768)\n",
      "classifier.out_proj.bias                                        (7,)\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of  5,602.    Elapsed: 0:00:07.\n",
      "  Batch    80  of  5,602.    Elapsed: 0:00:14.\n",
      "  Batch   120  of  5,602.    Elapsed: 0:00:20.\n",
      "  Batch   160  of  5,602.    Elapsed: 0:00:27.\n",
      "  Batch   200  of  5,602.    Elapsed: 0:00:34.\n",
      "  Batch   240  of  5,602.    Elapsed: 0:00:41.\n",
      "  Batch   280  of  5,602.    Elapsed: 0:00:48.\n",
      "  Batch   320  of  5,602.    Elapsed: 0:00:54.\n",
      "  Batch   360  of  5,602.    Elapsed: 0:01:01.\n",
      "  Batch   400  of  5,602.    Elapsed: 0:01:08.\n",
      "  Batch   440  of  5,602.    Elapsed: 0:01:15.\n",
      "  Batch   480  of  5,602.    Elapsed: 0:01:22.\n",
      "  Batch   520  of  5,602.    Elapsed: 0:01:29.\n",
      "  Batch   560  of  5,602.    Elapsed: 0:01:36.\n",
      "  Batch   600  of  5,602.    Elapsed: 0:01:42.\n",
      "  Batch   640  of  5,602.    Elapsed: 0:01:49.\n",
      "  Batch   680  of  5,602.    Elapsed: 0:01:56.\n",
      "  Batch   720  of  5,602.    Elapsed: 0:02:03.\n",
      "  Batch   760  of  5,602.    Elapsed: 0:02:10.\n",
      "  Batch   800  of  5,602.    Elapsed: 0:02:17.\n",
      "  Batch   840  of  5,602.    Elapsed: 0:02:24.\n",
      "  Batch   880  of  5,602.    Elapsed: 0:02:31.\n",
      "  Batch   920  of  5,602.    Elapsed: 0:02:38.\n",
      "  Batch   960  of  5,602.    Elapsed: 0:02:45.\n",
      "  Batch 1,000  of  5,602.    Elapsed: 0:02:52.\n",
      "  Batch 1,040  of  5,602.    Elapsed: 0:02:59.\n",
      "  Batch 1,160  of  5,602.    Elapsed: 0:03:20.\n",
      "  Batch 1,200  of  5,602.    Elapsed: 0:03:27.\n",
      "  Batch 1,240  of  5,602.    Elapsed: 0:03:34.\n",
      "  Batch 1,280  of  5,602.    Elapsed: 0:03:40.\n",
      "  Batch 1,320  of  5,602.    Elapsed: 0:03:47.\n",
      "  Batch 1,360  of  5,602.    Elapsed: 0:03:54.\n",
      "  Batch 1,400  of  5,602.    Elapsed: 0:04:01.\n",
      "  Batch 1,440  of  5,602.    Elapsed: 0:04:08.\n",
      "  Batch 1,480  of  5,602.    Elapsed: 0:04:16.\n",
      "  Batch 1,520  of  5,602.    Elapsed: 0:04:23.\n",
      "  Batch 1,560  of  5,602.    Elapsed: 0:04:30.\n",
      "  Batch 1,600  of  5,602.    Elapsed: 0:04:37.\n",
      "  Batch 1,640  of  5,602.    Elapsed: 0:04:44.\n",
      "  Batch 1,680  of  5,602.    Elapsed: 0:04:51.\n",
      "  Batch 1,720  of  5,602.    Elapsed: 0:04:58.\n",
      "  Batch 1,760  of  5,602.    Elapsed: 0:05:05.\n",
      "  Batch 1,800  of  5,602.    Elapsed: 0:05:12.\n",
      "  Batch 1,840  of  5,602.    Elapsed: 0:05:19.\n",
      "  Batch 1,880  of  5,602.    Elapsed: 0:05:26.\n",
      "  Batch 1,920  of  5,602.    Elapsed: 0:05:33.\n",
      "  Batch 1,960  of  5,602.    Elapsed: 0:05:40.\n",
      "  Batch 2,000  of  5,602.    Elapsed: 0:05:47.\n",
      "  Batch 2,040  of  5,602.    Elapsed: 0:05:54.\n",
      "  Batch 2,080  of  5,602.    Elapsed: 0:06:01.\n",
      "  Batch 2,120  of  5,602.    Elapsed: 0:06:08.\n",
      "  Batch 2,160  of  5,602.    Elapsed: 0:06:15.\n",
      "  Batch 2,200  of  5,602.    Elapsed: 0:06:22.\n",
      "  Batch 2,240  of  5,602.    Elapsed: 0:06:29.\n",
      "  Batch 2,280  of  5,602.    Elapsed: 0:06:36.\n",
      "  Batch 2,320  of  5,602.    Elapsed: 0:06:43.\n",
      "  Batch 2,360  of  5,602.    Elapsed: 0:06:51.\n",
      "  Batch 2,400  of  5,602.    Elapsed: 0:06:58.\n",
      "  Batch 2,440  of  5,602.    Elapsed: 0:07:05.\n",
      "  Batch 2,480  of  5,602.    Elapsed: 0:07:12.\n",
      "  Batch 2,520  of  5,602.    Elapsed: 0:07:19.\n",
      "  Batch 2,560  of  5,602.    Elapsed: 0:07:26.\n",
      "  Batch 2,600  of  5,602.    Elapsed: 0:07:33.\n",
      "  Batch 2,640  of  5,602.    Elapsed: 0:07:40.\n",
      "  Batch 2,680  of  5,602.    Elapsed: 0:07:47.\n",
      "  Batch 2,720  of  5,602.    Elapsed: 0:07:54.\n",
      "  Batch 2,760  of  5,602.    Elapsed: 0:08:01.\n",
      "  Batch 2,800  of  5,602.    Elapsed: 0:08:08.\n",
      "  Batch 2,840  of  5,602.    Elapsed: 0:08:15.\n",
      "  Batch 2,880  of  5,602.    Elapsed: 0:08:22.\n",
      "  Batch 2,920  of  5,602.    Elapsed: 0:08:29.\n",
      "  Batch 2,960  of  5,602.    Elapsed: 0:08:36.\n",
      "  Batch 3,000  of  5,602.    Elapsed: 0:08:43.\n",
      "  Batch 3,040  of  5,602.    Elapsed: 0:08:50.\n",
      "  Batch 3,080  of  5,602.    Elapsed: 0:08:57.\n",
      "  Batch 3,120  of  5,602.    Elapsed: 0:09:04.\n",
      "  Batch 3,160  of  5,602.    Elapsed: 0:09:11.\n",
      "  Batch 3,200  of  5,602.    Elapsed: 0:09:19.\n",
      "  Batch 3,240  of  5,602.    Elapsed: 0:09:26.\n",
      "  Batch 3,280  of  5,602.    Elapsed: 0:09:33.\n",
      "  Batch 3,320  of  5,602.    Elapsed: 0:09:40.\n",
      "  Batch 3,360  of  5,602.    Elapsed: 0:09:47.\n",
      "  Batch 3,400  of  5,602.    Elapsed: 0:09:54.\n",
      "  Batch 3,440  of  5,602.    Elapsed: 0:10:01.\n",
      "  Batch 3,480  of  5,602.    Elapsed: 0:10:08.\n",
      "  Batch 3,520  of  5,602.    Elapsed: 0:10:15.\n",
      "  Batch 3,560  of  5,602.    Elapsed: 0:10:22.\n",
      "  Batch 3,600  of  5,602.    Elapsed: 0:10:29.\n",
      "  Batch 3,640  of  5,602.    Elapsed: 0:10:36.\n",
      "  Batch 3,680  of  5,602.    Elapsed: 0:10:43.\n",
      "  Batch 3,720  of  5,602.    Elapsed: 0:10:50.\n",
      "  Batch 3,760  of  5,602.    Elapsed: 0:10:57.\n",
      "  Batch 3,800  of  5,602.    Elapsed: 0:11:04.\n",
      "  Batch 3,840  of  5,602.    Elapsed: 0:11:11.\n",
      "  Batch 3,880  of  5,602.    Elapsed: 0:11:18.\n",
      "  Batch 3,920  of  5,602.    Elapsed: 0:11:25.\n",
      "  Batch 3,960  of  5,602.    Elapsed: 0:11:32.\n",
      "  Batch 4,000  of  5,602.    Elapsed: 0:11:39.\n",
      "  Batch 4,040  of  5,602.    Elapsed: 0:11:46.\n",
      "  Batch 4,080  of  5,602.    Elapsed: 0:11:53.\n",
      "  Batch 4,120  of  5,602.    Elapsed: 0:12:00.\n",
      "  Batch 4,160  of  5,602.    Elapsed: 0:12:07.\n",
      "  Batch 4,200  of  5,602.    Elapsed: 0:12:14.\n",
      "  Batch 4,240  of  5,602.    Elapsed: 0:12:20.\n",
      "  Batch 4,280  of  5,602.    Elapsed: 0:12:27.\n",
      "  Batch 4,320  of  5,602.    Elapsed: 0:12:34.\n",
      "  Batch 4,360  of  5,602.    Elapsed: 0:12:41.\n",
      "  Batch 4,400  of  5,602.    Elapsed: 0:12:48.\n",
      "  Batch 4,440  of  5,602.    Elapsed: 0:12:55.\n",
      "  Batch 4,480  of  5,602.    Elapsed: 0:13:02.\n",
      "  Batch 4,520  of  5,602.    Elapsed: 0:13:09.\n",
      "  Batch 4,560  of  5,602.    Elapsed: 0:13:16.\n",
      "  Batch 4,600  of  5,602.    Elapsed: 0:13:23.\n",
      "  Batch 4,640  of  5,602.    Elapsed: 0:13:30.\n",
      "  Batch 4,680  of  5,602.    Elapsed: 0:13:37.\n",
      "  Batch 4,720  of  5,602.    Elapsed: 0:13:44.\n",
      "  Batch 4,760  of  5,602.    Elapsed: 0:13:51.\n",
      "  Batch 4,800  of  5,602.    Elapsed: 0:13:58.\n",
      "  Batch 4,840  of  5,602.    Elapsed: 0:14:05.\n",
      "  Batch 4,880  of  5,602.    Elapsed: 0:14:12.\n",
      "  Batch 4,920  of  5,602.    Elapsed: 0:14:19.\n",
      "  Batch 4,960  of  5,602.    Elapsed: 0:14:26.\n",
      "  Batch 5,000  of  5,602.    Elapsed: 0:14:33.\n",
      "  Batch 5,040  of  5,602.    Elapsed: 0:14:39.\n",
      "  Batch 5,080  of  5,602.    Elapsed: 0:14:46.\n",
      "  Batch 5,120  of  5,602.    Elapsed: 0:14:53.\n",
      "  Batch 5,160  of  5,602.    Elapsed: 0:15:00.\n",
      "  Batch 5,200  of  5,602.    Elapsed: 0:15:07.\n",
      "  Batch 5,240  of  5,602.    Elapsed: 0:15:14.\n",
      "  Batch 5,280  of  5,602.    Elapsed: 0:15:21.\n",
      "  Batch 5,320  of  5,602.    Elapsed: 0:15:28.\n",
      "  Batch 5,360  of  5,602.    Elapsed: 0:15:35.\n",
      "  Batch 5,400  of  5,602.    Elapsed: 0:15:42.\n",
      "  Batch 5,440  of  5,602.    Elapsed: 0:15:49.\n",
      "  Batch 5,480  of  5,602.    Elapsed: 0:15:56.\n",
      "  Batch 5,520  of  5,602.    Elapsed: 0:16:03.\n",
      "  Batch 5,560  of  5,602.    Elapsed: 0:16:10.\n",
      "  Batch 5,600  of  5,602.    Elapsed: 0:16:17.\n",
      "\n",
      "  Average training loss: 1.86\n",
      "  Training epcoh took: 0:16:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.17\n",
      "  Validation Loss: 1.85\n",
      "  Validation took: 0:00:34\n",
      "Going on track\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch    40  of  5,602.    Elapsed: 0:00:07.\n",
      "  Batch    80  of  5,602.    Elapsed: 0:00:14.\n",
      "  Batch   120  of  5,602.    Elapsed: 0:00:21.\n",
      "  Batch   160  of  5,602.    Elapsed: 0:00:28.\n",
      "  Batch   200  of  5,602.    Elapsed: 0:00:35.\n",
      "  Batch   240  of  5,602.    Elapsed: 0:00:42.\n",
      "  Batch   280  of  5,602.    Elapsed: 0:00:48.\n",
      "  Batch   320  of  5,602.    Elapsed: 0:00:55.\n",
      "  Batch   360  of  5,602.    Elapsed: 0:01:02.\n",
      "  Batch   400  of  5,602.    Elapsed: 0:01:09.\n",
      "  Batch   440  of  5,602.    Elapsed: 0:01:16.\n",
      "  Batch   480  of  5,602.    Elapsed: 0:01:23.\n",
      "  Batch   520  of  5,602.    Elapsed: 0:01:30.\n",
      "  Batch   560  of  5,602.    Elapsed: 0:01:37.\n",
      "  Batch   600  of  5,602.    Elapsed: 0:01:44.\n",
      "  Batch   640  of  5,602.    Elapsed: 0:01:51.\n",
      "  Batch   680  of  5,602.    Elapsed: 0:01:58.\n",
      "  Batch   720  of  5,602.    Elapsed: 0:02:05.\n",
      "  Batch   760  of  5,602.    Elapsed: 0:02:12.\n",
      "  Batch   800  of  5,602.    Elapsed: 0:02:19.\n",
      "  Batch   840  of  5,602.    Elapsed: 0:02:26.\n",
      "  Batch   880  of  5,602.    Elapsed: 0:02:33.\n",
      "  Batch   920  of  5,602.    Elapsed: 0:02:40.\n",
      "  Batch   960  of  5,602.    Elapsed: 0:02:46.\n",
      "  Batch 1,000  of  5,602.    Elapsed: 0:02:53.\n",
      "  Batch 1,040  of  5,602.    Elapsed: 0:03:00.\n",
      "  Batch 1,080  of  5,602.    Elapsed: 0:03:07.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d1e4230f498a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-af7bd4be2b33>\u001b[0m in \u001b[0;36mclassifier\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-fb7a37ed6028>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_dataloader, validation_dataloader)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# single value; the `.item()` function just returns the Python value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# from the tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m# Perform a backward pass to calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier(models[model_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-11-13T20:48:08.694949Z",
     "iopub.status.idle": "2020-11-13T20:48:08.695234Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\"\"\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-11-13T20:48:08.696035Z",
     "iopub.status.idle": "2020-11-13T20:48:08.696278Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-11-13T20:48:08.697081Z",
     "iopub.status.idle": "2020-11-13T20:48:08.697320Z"
    }
   },
   "outputs": [],
   "source": [
    "posts = valid_x.values\n",
    "categories = valid_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-11-13T20:48:08.698082Z",
     "iopub.status.idle": "2020-11-13T20:48:08.698341Z"
    }
   },
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in posts:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        truncation=True,\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(categories)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-11-13T20:48:08.699166Z",
     "iopub.status.idle": "2020-11-13T20:48:08.699455Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-11-13T20:48:08.700211Z",
     "iopub.status.idle": "2020-11-13T20:48:08.700469Z"
    }
   },
   "outputs": [],
   "source": [
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2020-11-13T20:48:08.701189Z",
     "iopub.status.idle": "2020-11-13T20:48:08.701444Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
