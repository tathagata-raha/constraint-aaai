{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:32:48.845827Z",
     "iopub.status.busy": "2020-12-08T18:32:48.845587Z",
     "iopub.status.idle": "2020-12-08T18:32:48.851730Z",
     "shell.execute_reply": "2020-12-08T18:32:48.850998Z",
     "shell.execute_reply.started": "2020-12-08T18:32:48.845801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Insert code here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertConfig, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sent_encoder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:32:49.070350Z",
     "iopub.status.busy": "2020-12-08T18:32:49.070149Z",
     "iopub.status.idle": "2020-12-08T18:32:49.114517Z",
     "shell.execute_reply": "2020-12-08T18:32:49.113931Z",
     "shell.execute_reply.started": "2020-12-08T18:32:49.070327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:2\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:32:49.313876Z",
     "iopub.status.busy": "2020-12-08T18:32:49.313711Z",
     "iopub.status.idle": "2020-12-08T18:32:49.389212Z",
     "shell.execute_reply": "2020-12-08T18:32:49.388648Z",
     "shell.execute_reply.started": "2020-12-08T18:32:49.313856Z"
    }
   },
   "outputs": [],
   "source": [
    "models = ['bert-base-uncased', 'distilbert-base-uncased-finetuned-sst-2-english', 'textattack/roberta-base-SST-2','roberta-base', 'google/electra-base-discriminator', 'xlnet-base-cased', 'xlm-roberta-base', '/scratch/covid-tapt', '/scratch/covid-tapt/checkpoint-500']\n",
    "model_num = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[model_num])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:32:49.753751Z",
     "iopub.status.busy": "2020-12-08T18:32:49.753585Z",
     "iopub.status.idle": "2020-12-08T18:32:49.790420Z",
     "shell.execute_reply": "2020-12-08T18:32:49.789931Z",
     "shell.execute_reply.started": "2020-12-08T18:32:49.753731Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/covid/Constraint_English_Train - Sheet1.csv')\n",
    "test = pd.read_csv('../datasets/covid/Constraint_English_Val - Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:24.746303Z",
     "iopub.status.busy": "2020-12-08T18:35:24.746107Z",
     "iopub.status.idle": "2020-12-08T18:35:24.831662Z",
     "shell.execute_reply": "2020-12-08T18:35:24.831054Z",
     "shell.execute_reply.started": "2020-12-08T18:35:24.746279Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train.pickle','rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    train = pd.DataFrame.from_dict(train)\n",
    "    train.drop(train.head(1).index, inplace=True)\n",
    "with open('valid.pickle','rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "    valid = pd.DataFrame.from_dict(valid)\n",
    "    valid.drop(valid.head(1).index, inplace=True)\n",
    "with open('test.pickle','rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    del test['task_1']\n",
    "    test = pd.DataFrame.from_dict(test)\n",
    "#     test.drop(test.head(1).index, inplace=True)\n",
    "#     test = pd.DataFrame.from_dict(test)\n",
    "# test = pd.read_csv('data/valid.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:25.776747Z",
     "iopub.status.busy": "2020-12-08T18:35:25.776514Z",
     "iopub.status.idle": "2020-12-08T18:35:25.814153Z",
     "shell.execute_reply": "2020-12-08T18:35:25.813635Z",
     "shell.execute_reply.started": "2020-12-08T18:35:25.776720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>full_tweet</th>\n",
       "      <th>tweet_raw_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>smiley</th>\n",
       "      <th>emoji</th>\n",
       "      <th>url</th>\n",
       "      <th>mentions</th>\n",
       "      <th>numerals</th>\n",
       "      <th>reserved_word</th>\n",
       "      <th>emotext</th>\n",
       "      <th>segmented_hash</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our daily update is published. States reported...</td>\n",
       "      <td>Our daily update is published. States reported...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/wzSYMe0Sht]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[734, 39, 532, 30, 22]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Our daily update is published. States reported...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alfalfa is the only cure for COVID-19.</td>\n",
       "      <td>Alfalfa is the only cure for COVID-19.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Alfalfa is the only cure for COVID-19.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>President Trump Asked What He Would Do If He W...</td>\n",
       "      <td>President Trump Asked What He Would Do If He W...</td>\n",
       "      <td>[#donaldtrump, #coronavirus]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/3MEWhusRZI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[donald trump, coronavirus]</td>\n",
       "      <td>President Trump Asked What He Would Do If He W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>States reported 630 deaths. We are still seein...</td>\n",
       "      <td>States reported deaths. We are still seeing a ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/LBmcot3h9a]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[630, 28]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>States reported 630 deaths. We are still seein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This is the sixth time a global health emergen...</td>\n",
       "      <td>This is the sixth time a global health emergen...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/JvKC0PTett]</td>\n",
       "      <td>[@DrTedros]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>This is the sixth time a global health emergen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Low #vitaminD was an independent predictor of ...</td>\n",
       "      <td>Low was an independent predictor of worse prog...</td>\n",
       "      <td>[#vitaminD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/CGD6Kphn31, https://t.co/chtni8K...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[vitamin d]</td>\n",
       "      <td>Low   was an independent predictor of worse pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>A common question: why are the cumulative outc...</td>\n",
       "      <td>A common question: why are the cumulative outc...</td>\n",
       "      <td>[#s]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[s]</td>\n",
       "      <td>A common question: why are the cumulative outc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>The government should consider bringing in any...</td>\n",
       "      <td>The government should consider bringing in any...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/pdOls6cqoN]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>The government should consider bringing in any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Our daily update is published. Weâ€™ve now track...</td>\n",
       "      <td>Our daily update is published. Weve now tracke...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/PZrmH4bl5Y, https://t.co/2588xW5...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[2.9, 119, 1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Our daily update is published. Weâ€™ve now track...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Breakdown of testing: 4 air crew 97 hotel &amp;amp...</td>\n",
       "      <td>Breakdown of testing: air crew hotel &amp;amp; hea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[4, 97, 71, 2, 200]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Breakdown of testing: 4 air crew 97 hotel &amp;amp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tweet_id                                         full_tweet  \\\n",
       "0        1  Our daily update is published. States reported...   \n",
       "1        2             Alfalfa is the only cure for COVID-19.   \n",
       "2        3  President Trump Asked What He Would Do If He W...   \n",
       "3        4  States reported 630 deaths. We are still seein...   \n",
       "4        5  This is the sixth time a global health emergen...   \n",
       "5        6  Low #vitaminD was an independent predictor of ...   \n",
       "6        7  A common question: why are the cumulative outc...   \n",
       "7        8  The government should consider bringing in any...   \n",
       "8        9  Our daily update is published. Weâ€™ve now track...   \n",
       "9       10  Breakdown of testing: 4 air crew 97 hotel &amp...   \n",
       "\n",
       "                                      tweet_raw_text  \\\n",
       "0  Our daily update is published. States reported...   \n",
       "1             Alfalfa is the only cure for COVID-19.   \n",
       "2  President Trump Asked What He Would Do If He W...   \n",
       "3  States reported deaths. We are still seeing a ...   \n",
       "4  This is the sixth time a global health emergen...   \n",
       "5  Low was an independent predictor of worse prog...   \n",
       "6  A common question: why are the cumulative outc...   \n",
       "7  The government should consider bringing in any...   \n",
       "8  Our daily update is published. Weve now tracke...   \n",
       "9  Breakdown of testing: air crew hotel &amp; hea...   \n",
       "\n",
       "                       hashtags smiley emoji  \\\n",
       "0                            []     []    []   \n",
       "1                            []     []    []   \n",
       "2  [#donaldtrump, #coronavirus]     []    []   \n",
       "3                            []     []    []   \n",
       "4                            []     []    []   \n",
       "5                   [#vitaminD]     []    []   \n",
       "6                          [#s]     []    []   \n",
       "7                            []     []    []   \n",
       "8                            []     []    []   \n",
       "9                            []     []    []   \n",
       "\n",
       "                                                 url     mentions  \\\n",
       "0                          [https://t.co/wzSYMe0Sht]           []   \n",
       "1                                                 []           []   \n",
       "2                          [https://t.co/3MEWhusRZI]           []   \n",
       "3                          [https://t.co/LBmcot3h9a]           []   \n",
       "4                          [https://t.co/JvKC0PTett]  [@DrTedros]   \n",
       "5  [https://t.co/CGD6Kphn31, https://t.co/chtni8K...           []   \n",
       "6                                                 []           []   \n",
       "7                          [https://t.co/pdOls6cqoN]           []   \n",
       "8  [https://t.co/PZrmH4bl5Y, https://t.co/2588xW5...           []   \n",
       "9                                                 []           []   \n",
       "\n",
       "                 numerals reserved_word emotext               segmented_hash  \\\n",
       "0  [734, 39, 532, 30, 22]            []      []                           []   \n",
       "1                      []            []      []                           []   \n",
       "2                      []            []      []  [donald trump, coronavirus]   \n",
       "3               [630, 28]            []      []                           []   \n",
       "4                      []            []      []                           []   \n",
       "5                      []            []      []                  [vitamin d]   \n",
       "6                      []            []      []                          [s]   \n",
       "7                      []            []      []                           []   \n",
       "8           [2.9, 119, 1]            []      []                           []   \n",
       "9     [4, 97, 71, 2, 200]            []      []                           []   \n",
       "\n",
       "                                               clean  \n",
       "0  Our daily update is published. States reported...  \n",
       "1             Alfalfa is the only cure for COVID-19.  \n",
       "2  President Trump Asked What He Would Do If He W...  \n",
       "3  States reported 630 deaths. We are still seein...  \n",
       "4  This is the sixth time a global health emergen...  \n",
       "5  Low   was an independent predictor of worse pr...  \n",
       "6  A common question: why are the cumulative outc...  \n",
       "7  The government should consider bringing in any...  \n",
       "8  Our daily update is published. Weâ€™ve now track...  \n",
       "9  Breakdown of testing: 4 air crew 97 hotel &amp...  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train, valid])\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:27.260285Z",
     "iopub.status.busy": "2020-12-08T18:35:27.260114Z",
     "iopub.status.idle": "2020-12-08T18:35:27.262991Z",
     "shell.execute_reply": "2020-12-08T18:35:27.262455Z",
     "shell.execute_reply.started": "2020-12-08T18:35:27.260263Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = ['fake','real']\n",
    "def label_encode(val):\n",
    "    return labels.index(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:30.078264Z",
     "iopub.status.busy": "2020-12-08T18:35:30.078059Z",
     "iopub.status.idle": "2020-12-08T18:35:30.087399Z",
     "shell.execute_reply": "2020-12-08T18:35:30.086641Z",
     "shell.execute_reply.started": "2020-12-08T18:35:30.078239Z"
    }
   },
   "outputs": [],
   "source": [
    "train['label'] = train.task_1.apply(label_encode)\n",
    "train['tweet'] = train.full_tweet\n",
    "test['tweet'] = test.full_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:30.973101Z",
     "iopub.status.busy": "2020-12-08T18:35:30.972939Z",
     "iopub.status.idle": "2020-12-08T18:35:30.978204Z",
     "shell.execute_reply": "2020-12-08T18:35:30.977751Z",
     "shell.execute_reply.started": "2020-12-08T18:35:30.973082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1588    []\n",
       "1906    []\n",
       "1883    []\n",
       "829     []\n",
       "977     []\n",
       "983     []\n",
       "769     []\n",
       "1017    []\n",
       "368     []\n",
       "1294    []\n",
       "Name: emoji, dtype: object"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.emoji.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:31.336850Z",
     "iopub.status.busy": "2020-12-08T18:35:31.336707Z",
     "iopub.status.idle": "2020-12-08T18:35:31.467892Z",
     "shell.execute_reply": "2020-12-08T18:35:31.467421Z",
     "shell.execute_reply.started": "2020-12-08T18:35:31.336832Z"
    }
   },
   "outputs": [],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "train.tweet = train.tweet.apply(clean_text)\n",
    "train.tweet = train.tweet.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:31.679414Z",
     "iopub.status.busy": "2020-12-08T18:35:31.679268Z",
     "iopub.status.idle": "2020-12-08T18:35:31.715809Z",
     "shell.execute_reply": "2020-12-08T18:35:31.715345Z",
     "shell.execute_reply.started": "2020-12-08T18:35:31.679395Z"
    }
   },
   "outputs": [],
   "source": [
    "# test.label = test.label.apply(label_encode)\n",
    "test = test.reset_index(drop=True)\n",
    "test.tweet = test.tweet.apply(clean_text)\n",
    "test.tweet = test.tweet.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:32.077780Z",
     "iopub.status.busy": "2020-12-08T18:35:32.077619Z",
     "iopub.status.idle": "2020-12-08T18:35:32.082166Z",
     "shell.execute_reply": "2020-12-08T18:35:32.081654Z",
     "shell.execute_reply.started": "2020-12-08T18:35:32.077761Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5689    extremely good news bendigo hits covidzero but...\n",
       "5117     new cases of #covid lagos ogun fct borno kadu...\n",
       "4489    georgian homeopath discussed a homeopathic dru...\n",
       "4490    coronavirus can be transmitted through mosquit...\n",
       "5438    covid update there are two new cases of covid ...\n",
       "1640    rt cdcdirector cdcs guidelines to combat the s...\n",
       "6327    rt cdcdirector cdcgov to award  million to  st...\n",
       "6445    breathlessness excessive fatigue and muscle ac...\n",
       "5206    all family members has died infected by corona...\n",
       "8210      at the start was reasonably necessary but it...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tweet.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:32.453746Z",
     "iopub.status.busy": "2020-12-08T18:35:32.453603Z",
     "iopub.status.idle": "2020-12-08T18:35:32.457907Z",
     "shell.execute_reply": "2020-12-08T18:35:32.457352Z",
     "shell.execute_reply.started": "2020-12-08T18:35:32.453728Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['tweet'], train['label'])\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train['tweet'], train['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:33.050131Z",
     "iopub.status.busy": "2020-12-08T18:35:33.049969Z",
     "iopub.status.idle": "2020-12-08T18:35:33.052787Z",
     "shell.execute_reply": "2020-12-08T18:35:33.052319Z",
     "shell.execute_reply.started": "2020-12-08T18:35:33.050112Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    try:\n",
    "        return len(text.split())\n",
    "    except:\n",
    "        print(text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:33.768620Z",
     "iopub.status.busy": "2020-12-08T18:35:33.768478Z",
     "iopub.status.idle": "2020-12-08T18:35:33.784953Z",
     "shell.execute_reply": "2020-12-08T18:35:33.784513Z",
     "shell.execute_reply.started": "2020-12-08T18:35:33.768602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27.31673481308411, 1446, 5, 6848)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "maxw = 0\n",
    "large_count = 0\n",
    "for i in train_x:\n",
    "    temp = count_words(i)\n",
    "    total += temp\n",
    "    maxw = temp if temp > maxw else maxw\n",
    "    large_count += 1 if temp > 120 else 0\n",
    "total/len(train_x), maxw, large_count, len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:34.589322Z",
     "iopub.status.busy": "2020-12-08T18:35:34.589163Z",
     "iopub.status.idle": "2020-12-08T18:35:34.602064Z",
     "shell.execute_reply": "2020-12-08T18:35:34.601512Z",
     "shell.execute_reply.started": "2020-12-08T18:35:34.589304Z"
    }
   },
   "outputs": [],
   "source": [
    "# MAX_LENGTH = 50\n",
    "posts = train.values\n",
    "categories = train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:35.875324Z",
     "iopub.status.busy": "2020-12-08T18:35:35.875137Z",
     "iopub.status.idle": "2020-12-08T18:35:35.877984Z",
     "shell.execute_reply": "2020-12-08T18:35:35.877507Z",
     "shell.execute_reply.started": "2020-12-08T18:35:35.875304Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:36.791867Z",
     "iopub.status.busy": "2020-12-08T18:35:36.791717Z",
     "iopub.status.idle": "2020-12-08T18:35:36.810218Z",
     "shell.execute_reply": "2020-12-08T18:35:36.809760Z",
     "shell.execute_reply.started": "2020-12-08T18:35:36.791848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.models as gsm\n",
    "e2v = gsm.KeyedVectors.load_word2vec_format('emoji2vec.bin', binary=True)\n",
    "# happy_vector = e2v['ðŸ˜‚']    # Produces an embedding vector of length 300\n",
    "\n",
    "# Download the bin file from here https://github.com/uclnlp/emoji2vec/blob/master/pre-trained/emoji2vec.bin\n",
    "\n",
    "def getEmojiEmbeddings(emojiList,dim=300,verbose = False):\n",
    "  \"\"\" Generates an emoji vector by averaging the emoji representation for each emoji. If no emoji returns an empty list of dimension dim\"\"\"\n",
    "  if dim < 300:\n",
    "    raise IndexError(\"Dim has to be greater than 300\")\n",
    "  result = np.zeros(dim)\n",
    "  if (len(emojiList) == 0):\n",
    "    return result\n",
    "  else:\n",
    "    embs = None\n",
    "    for i in emojiList:\n",
    "      if verbose:\n",
    "        if i not in e2v.vocab:\n",
    "          print(i)\n",
    "    embs = np.mean([e2v[i] for i in emojiList if i in e2v.vocab], axis=0)\n",
    "  if np.any(np.isnan(embs)):\n",
    "    return result\n",
    "  result[:300] = embs\n",
    "  return result\n",
    "getEmojiEmbeddings(valid.emoji.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:38.009866Z",
     "iopub.status.busy": "2020-12-08T18:35:38.009720Z",
     "iopub.status.idle": "2020-12-08T18:35:38.016070Z",
     "shell.execute_reply": "2020-12-08T18:35:38.015519Z",
     "shell.execute_reply.started": "2020-12-08T18:35:38.009848Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1764: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([128]), torch.Size([300]))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode_plus(\n",
    "            valid.full_tweet.values[0],\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )['input_ids']\n",
    "torch.tensor(ids, dtype=torch.long).shape, torch.tensor(getEmojiEmbeddings(valid.emoji.values[0]), dtype=torch.long).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:38.957086Z",
     "iopub.status.busy": "2020-12-08T18:35:38.956941Z",
     "iopub.status.idle": "2020-12-08T18:35:38.965673Z",
     "shell.execute_reply": "2020-12-08T18:35:38.965208Z",
     "shell.execute_reply.started": "2020-12-08T18:35:38.957067Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len, t = False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.tweet\n",
    "        self.emoji = dataframe.emoji\n",
    "        self.hash = dataframe.segmented_hash\n",
    "        self.t = t\n",
    "        if not self.t:\n",
    "            self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        h_text = self.hash[index]\n",
    "        h_text = \" \".join(h_text)\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            h_text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        h_ids = inputs['input_ids']\n",
    "        h_mask = inputs['attention_mask']\n",
    "        h_token_type_ids = inputs[\"token_type_ids\"]\n",
    "#         h_inputs\n",
    "        emoji = getEmojiEmbeddings(self.emoji[index])\n",
    "        if self.t:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                'h_ids': torch.tensor(h_ids, dtype=torch.long),\n",
    "                'h_mask': torch.tensor(h_mask, dtype=torch.long),\n",
    "                'h_token_type_ids': torch.tensor(h_token_type_ids, dtype=torch.long),\n",
    "                'emoji' : torch.tensor(emoji, dtype=torch.long),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                'h_ids': torch.tensor(h_ids, dtype=torch.long),\n",
    "                'h_mask': torch.tensor(h_mask, dtype=torch.long),\n",
    "                'h_token_type_ids': torch.tensor(h_token_type_ids, dtype=torch.long),\n",
    "                'emoji' : torch.tensor(emoji, dtype=torch.long),\n",
    "                'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:40.393180Z",
     "iopub.status.busy": "2020-12-08T18:35:40.393027Z",
     "iopub.status.idle": "2020-12-08T18:35:40.406530Z",
     "shell.execute_reply": "2020-12-08T18:35:40.406048Z",
     "shell.execute_reply.started": "2020-12-08T18:35:40.393161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (8560, 16)\n",
      "TRAIN Dataset: (6848, 16)\n",
      "TEST Dataset: (1712, 16)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.8\n",
    "train_data=train.sample(frac=train_size,random_state=200)\n",
    "test_data=train.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(train.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = MultiLabelDataset(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = MultiLabelDataset(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:41.603176Z",
     "iopub.status.busy": "2020-12-08T18:35:41.602998Z",
     "iopub.status.idle": "2020-12-08T18:35:41.608124Z",
     "shell.execute_reply": "2020-12-08T18:35:41.607641Z",
     "shell.execute_reply.started": "2020-12-08T18:35:41.603155Z"
    }
   },
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:28:55.317294Z",
     "iopub.status.busy": "2020-12-08T18:28:55.317147Z",
     "iopub.status.idle": "2020-12-08T18:29:04.586645Z",
     "shell.execute_reply": "2020-12-08T18:29:04.585847Z",
     "shell.execute_reply.started": "2020-12-08T18:28:55.317275Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier_1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (pre_classifier_2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (pre_classifier_3): Linear(in_features=1836, out_features=1836, bias=True)\n",
       "  (classifier): Linear(in_features=1836, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(models[model_num])\n",
    "        self.l2 = AutoModel.from_pretrained(models[model_num])\n",
    "        \n",
    "        self.pre_classifier_1 = torch.nn.Linear(768, 768)\n",
    "        self.pre_classifier_2 = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.pre_classifier_3 = torch.nn.Linear(1836, 1836)\n",
    "#         self.pre_classifier_3 = torch.nn.Linear(768, 100)\n",
    "        self.classifier = torch.nn.Linear(1836, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state_1 = output_1[0]\n",
    "        pooler_1 = hidden_state_1[:, 0]\n",
    "        pooler_1 = self.pre_classifier_1(pooler_1)\n",
    "        pooler_1 = torch.nn.Tanh()(pooler_1)\n",
    "        pooler_1 = self.dropout(pooler_1)\n",
    "        output_2 = self.l2(input_ids=h_ids, attention_mask=h_mask)\n",
    "        hidden_state_2 = output_2[0]\n",
    "        pooler_2 = hidden_state_2[:, 0]\n",
    "        pooler_2 = self.pre_classifier_2(pooler_2)\n",
    "        pooler_2 = torch.nn.Tanh()(pooler_2)\n",
    "        pooler_2 = self.dropout(pooler_2)\n",
    "        pooler_3 = torch.cat((pooler_1, pooler_2), 1)\n",
    "        pooler_3 = torch.cat((pooler_3, emoji), 1)\n",
    "#         print(pooler_1.shape,hidden_state_1.shape, pooler_2.shape, emoji.type(torch.FloatTensor).shape)\n",
    "#         pooler_3 = torch.nn.Tanh()(emoji.type(torch.FloatTensor))\n",
    "#         pooler_3 = self.dropout(pooler_3)\n",
    "#         print(pooler_3.shape)\n",
    "        pooler_3 = self.pre_classifier_3(pooler_3)\n",
    "#         pooler_3 = self.pre_classifier_3(pooler_2)\n",
    "        pooler_3 = torch.nn.Tanh()(pooler_3)\n",
    "        pooler_3 = self.dropout(pooler_3)\n",
    "        output = self.classifier(pooler_3)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:29:04.588412Z",
     "iopub.status.busy": "2020-12-08T18:29:04.588221Z",
     "iopub.status.idle": "2020-12-08T18:29:04.591066Z",
     "shell.execute_reply": "2020-12-08T18:29:04.590566Z",
     "shell.execute_reply.started": "2020-12-08T18:29:04.588390Z"
    }
   },
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# print(repr(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:29:04.592355Z",
     "iopub.status.busy": "2020-12-08T18:29:04.592174Z",
     "iopub.status.idle": "2020-12-08T18:29:04.603849Z",
     "shell.execute_reply": "2020-12-08T18:29:04.603420Z",
     "shell.execute_reply.started": "2020-12-08T18:29:04.592336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 406 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "l1.embeddings.word_embeddings.weight                    (50265, 768)\n",
      "l1.embeddings.position_embeddings.weight                  (514, 768)\n",
      "l1.embeddings.token_type_embeddings.weight                  (1, 768)\n",
      "l1.embeddings.LayerNorm.weight                                (768,)\n",
      "l1.embeddings.LayerNorm.bias                                  (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "l1.encoder.layer.0.attention.self.query.weight            (768, 768)\n",
      "l1.encoder.layer.0.attention.self.query.bias                  (768,)\n",
      "l1.encoder.layer.0.attention.self.key.weight              (768, 768)\n",
      "l1.encoder.layer.0.attention.self.key.bias                    (768,)\n",
      "l1.encoder.layer.0.attention.self.value.weight            (768, 768)\n",
      "l1.encoder.layer.0.attention.self.value.bias                  (768,)\n",
      "l1.encoder.layer.0.attention.output.dense.weight          (768, 768)\n",
      "l1.encoder.layer.0.attention.output.dense.bias                (768,)\n",
      "l1.encoder.layer.0.attention.output.LayerNorm.weight          (768,)\n",
      "l1.encoder.layer.0.attention.output.LayerNorm.bias            (768,)\n",
      "l1.encoder.layer.0.intermediate.dense.weight             (3072, 768)\n",
      "l1.encoder.layer.0.intermediate.dense.bias                   (3072,)\n",
      "l1.encoder.layer.0.output.dense.weight                   (768, 3072)\n",
      "l1.encoder.layer.0.output.dense.bias                          (768,)\n",
      "l1.encoder.layer.0.output.LayerNorm.weight                    (768,)\n",
      "l1.encoder.layer.0.output.LayerNorm.bias                      (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "pre_classifier_3.weight                                 (1836, 1836)\n",
      "pre_classifier_3.bias                                        (1836,)\n",
      "classifier.weight                                          (2, 1836)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:29:04.604804Z",
     "iopub.status.busy": "2020-12-08T18:29:04.604613Z",
     "iopub.status.idle": "2020-12-08T18:29:04.607843Z",
     "shell.execute_reply": "2020-12-08T18:29:04.607360Z",
     "shell.execute_reply.started": "2020-12-08T18:29:04.604785Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:29:04.608708Z",
     "iopub.status.busy": "2020-12-08T18:29:04.608543Z",
     "iopub.status.idle": "2020-12-08T18:29:04.615299Z",
     "shell.execute_reply": "2020-12-08T18:29:04.614808Z",
     "shell.execute_reply.started": "2020-12-08T18:29:04.608689Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T17:36:19.419543Z",
     "iopub.status.busy": "2020-12-08T17:36:19.419387Z",
     "iopub.status.idle": "2020-12-08T17:36:19.434269Z",
     "shell.execute_reply": "2020-12-08T17:36:19.433730Z",
     "shell.execute_reply.started": "2020-12-08T17:36:19.419524Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    total_train_loss = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        h_ids = data['h_ids'].to(device, dtype = torch.long)\n",
    "        h_mask = data['h_mask'].to(device, dtype = torch.long)\n",
    "        h_token_type_ids = data['h_token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        emoji = data['emoji'].to(device, dtype = torch.long)\n",
    "        outputs = model(ids, mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji)\n",
    "        optimizer.zero_grad()\n",
    "#         loss = outputs.loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "#         if _%50==0:\n",
    "#             print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        total_train_loss += loss.item()\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    print(f'Epoch: {epoch}, Loss:  {total_train_loss/count}')\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            h_ids = data['h_ids'].to(device, dtype = torch.long)\n",
    "            h_mask = data['h_mask'].to(device, dtype = torch.long)\n",
    "            h_token_type_ids = data['h_token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            emoji = data['emoji'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    fin_outputs = list(np.argmax(np.array(fin_outputs), axis=1).flatten())\n",
    "    print(classification_report(fin_outputs, fin_targets))\n",
    "    torch.save(model, '/scratch/epoch_'+str(epoch))\n",
    "    return fin_outputs, fin_targets\n",
    "#     final_outputs = np.array(fin_outputs) >=0.5\n",
    "#     final = []\n",
    "#     final_t = []\n",
    "#     final_fine = [[],[],[],[]]\n",
    "#     final_fine_t = [[],[],[],[]]\n",
    "#     for (i,j) in zip(final_outputs, fin_targets):\n",
    "#         output_sum = sum(i)\n",
    "#         target_sum = sum(j)\n",
    "#         if output_sum == 0:\n",
    "#             final.append(0)\n",
    "#         else:\n",
    "#             final.append(1)\n",
    "#         if target_sum == 0:\n",
    "#             final_t.append(0)\n",
    "#         else:\n",
    "#             final_t.append(1)\n",
    "#         for p in range(4):\n",
    "#             final_fine[p].append(int(i[p]))\n",
    "#             final_fine_t[p].append(int(j[p]))\n",
    "#     print(\"Coarse:\")\n",
    "#     print(classification_report(final, final_t))\n",
    "#     for i in range(4):\n",
    "#         print(\"Fine\", i)\n",
    "    \n",
    "#     return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T17:36:19.435095Z",
     "iopub.status.busy": "2020-12-08T17:36:19.434943Z",
     "iopub.status.idle": "2020-12-08T17:59:26.338918Z",
     "shell.execute_reply": "2020-12-08T17:59:26.338248Z",
     "shell.execute_reply.started": "2020-12-08T17:36:19.435076Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:03,  3.35it/s]/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "428it [02:05,  3.41it/s]\n",
      "1it [00:00,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.1893800597737987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:09,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       776\n",
      "           1       0.98      0.96      0.97       936\n",
      "\n",
      "    accuracy                           0.97      1712\n",
      "   macro avg       0.97      0.97      0.97      1712\n",
      "weighted avg       0.97      0.97      0.97      1712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [02:07,  3.36it/s]\n",
      "1it [00:00,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.070123767337727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:09,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       783\n",
      "           1       0.97      0.97      0.97       929\n",
      "\n",
      "    accuracy                           0.97      1712\n",
      "   macro avg       0.97      0.97      0.97      1712\n",
      "weighted avg       0.97      0.97      0.97      1712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [02:06,  3.38it/s]\n",
      "1it [00:00,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss:  0.034703462969845764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:09,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       774\n",
      "           1       0.99      0.97      0.98       938\n",
      "\n",
      "    accuracy                           0.98      1712\n",
      "   macro avg       0.98      0.98      0.98      1712\n",
      "weighted avg       0.98      0.98      0.98      1712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [02:06,  3.37it/s]\n",
      "1it [00:00,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss:  0.018472962436152277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:09,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97       761\n",
      "           1       0.99      0.96      0.97       951\n",
      "\n",
      "    accuracy                           0.97      1712\n",
      "   macro avg       0.97      0.97      0.97      1712\n",
      "weighted avg       0.97      0.97      0.97      1712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [02:06,  3.37it/s]\n",
      "1it [00:00,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss:  0.015005094774436505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:09,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       773\n",
      "           1       0.99      0.97      0.98       939\n",
      "\n",
      "    accuracy                           0.98      1712\n",
      "   macro avg       0.98      0.98      0.98      1712\n",
      "weighted avg       0.98      0.98      0.98      1712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [02:06,  3.38it/s]\n",
      "1it [00:00,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss:  0.010796221844905963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:09,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       760\n",
      "           1       0.99      0.96      0.98       952\n",
      "\n",
      "    accuracy                           0.97      1712\n",
      "   macro avg       0.97      0.97      0.97      1712\n",
      "weighted avg       0.97      0.97      0.97      1712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [02:07,  3.37it/s]\n",
      "1it [00:00,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss:  0.020441482442616246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:09,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       783\n",
      "           1       0.98      0.97      0.98       929\n",
      "\n",
      "    accuracy                           0.97      1712\n",
      "   macro avg       0.97      0.97      0.97      1712\n",
      "weighted avg       0.97      0.97      0.97      1712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [02:07,  3.36it/s]\n",
      "1it [00:00,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss:  0.004851448877153101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:09,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       758\n",
      "           1       0.99      0.96      0.98       954\n",
      "\n",
      "    accuracy                           0.97      1712\n",
      "   macro avg       0.97      0.97      0.97      1712\n",
      "weighted avg       0.97      0.97      0.97      1712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [02:06,  3.37it/s]\n",
      "1it [00:00,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss:  0.009885061413540178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:09,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       776\n",
      "           1       0.98      0.97      0.98       936\n",
      "\n",
      "    accuracy                           0.98      1712\n",
      "   macro avg       0.97      0.98      0.98      1712\n",
      "weighted avg       0.98      0.98      0.98      1712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [02:06,  3.37it/s]\n",
      "1it [00:00,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss:  0.001821532530722715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:09,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95       731\n",
      "           1       0.99      0.94      0.96       981\n",
      "\n",
      "    accuracy                           0.96      1712\n",
      "   macro avg       0.96      0.96      0.96      1712\n",
      "weighted avg       0.96      0.96      0.96      1712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    out, tar = train(epoch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T17:59:26.340228Z",
     "iopub.status.busy": "2020-12-08T17:59:26.340050Z",
     "iopub.status.idle": "2020-12-08T17:59:26.345223Z",
     "shell.execute_reply": "2020-12-08T17:59:26.344747Z",
     "shell.execute_reply.started": "2020-12-08T17:59:26.340205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 1, 0, 1, 0, 0, 0, 1, 1, 1], [1, 1, 0, 1, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0:10], tar[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:48.843178Z",
     "iopub.status.busy": "2020-12-08T18:35:48.842968Z",
     "iopub.status.idle": "2020-12-08T18:35:49.485854Z",
     "shell.execute_reply": "2020-12-08T18:35:49.485319Z",
     "shell.execute_reply.started": "2020-12-08T18:35:48.843152Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "model = torch.load('/scratch/epoch_4')\n",
    "# train_size = 0.8\n",
    "# test_data=test.sample(frac=1,random_state=200)\n",
    "# test_data=train.drop(train_data.index).reset_index(drop=True)\n",
    "test_data = test.reset_index(drop=True)\n",
    "testing = MultiLabelDataset(test_data, tokenizer, MAX_LEN, t=True)\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "testing_loader = DataLoader(testing, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:35:50.441047Z",
     "iopub.status.busy": "2020-12-08T18:35:50.440848Z",
     "iopub.status.idle": "2020-12-08T18:36:02.010570Z",
     "shell.execute_reply": "2020-12-08T18:36:02.010119Z",
     "shell.execute_reply.started": "2020-12-08T18:35:50.441022Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:04,  5.81it/s]/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/tathagata.raha/miniconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "67it [00:11,  5.80it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "fin_targets=[]\n",
    "fin_outputs=[]\n",
    "# print(f'Epoch: {epoch}, Loss:  {total_train_loss/count}')\n",
    "with torch.no_grad():\n",
    "    for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        h_ids = data['h_ids'].to(device, dtype = torch.long)\n",
    "        h_mask = data['h_mask'].to(device, dtype = torch.long)\n",
    "        h_token_type_ids = data['h_token_type_ids'].to(device, dtype = torch.long)\n",
    "#         targets = data['targets'].to(device, dtype = torch.long)\n",
    "        emoji = data['emoji'].to(device, dtype = torch.long)\n",
    "        outputs = model(ids, mask, token_type_ids, h_ids, h_mask, h_token_type_ids, emoji)\n",
    "#         fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "        fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "fin_outputs = list(np.argmax(np.array(fin_outputs), axis=1).flatten())\n",
    "# print(classification_report(fin_outputs, fin_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:36:19.006000Z",
     "iopub.status.busy": "2020-12-08T18:36:19.005751Z",
     "iopub.status.idle": "2020-12-08T18:36:19.010721Z",
     "shell.execute_reply": "2020-12-08T18:36:19.010063Z",
     "shell.execute_reply.started": "2020-12-08T18:36:19.005974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_outputs[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:36:02.016223Z",
     "iopub.status.busy": "2020-12-08T18:36:02.016041Z",
     "iopub.status.idle": "2020-12-08T18:36:02.048715Z",
     "shell.execute_reply": "2020-12-08T18:36:02.048271Z",
     "shell.execute_reply.started": "2020-12-08T18:36:02.016202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>full_tweet</th>\n",
       "      <th>tweet_raw_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>smiley</th>\n",
       "      <th>emoji</th>\n",
       "      <th>url</th>\n",
       "      <th>mentions</th>\n",
       "      <th>numerals</th>\n",
       "      <th>reserved_word</th>\n",
       "      <th>emotext</th>\n",
       "      <th>segmented_hash</th>\n",
       "      <th>clean</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our daily update is published. States reported...</td>\n",
       "      <td>Our daily update is published. States reported...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/wzSYMe0Sht]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[734, 39, 532, 30, 22]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Our daily update is published. States reported...</td>\n",
       "      <td>our daily update is published states reported ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alfalfa is the only cure for COVID-19.</td>\n",
       "      <td>Alfalfa is the only cure for COVID-19.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Alfalfa is the only cure for COVID-19.</td>\n",
       "      <td>alfalfa is the only cure for covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>President Trump Asked What He Would Do If He W...</td>\n",
       "      <td>President Trump Asked What He Would Do If He W...</td>\n",
       "      <td>[#donaldtrump, #coronavirus]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/3MEWhusRZI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[donald trump, coronavirus]</td>\n",
       "      <td>President Trump Asked What He Would Do If He W...</td>\n",
       "      <td>president trump asked what he would do if he w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>States reported 630 deaths. We are still seein...</td>\n",
       "      <td>States reported deaths. We are still seeing a ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/LBmcot3h9a]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[630, 28]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>States reported 630 deaths. We are still seein...</td>\n",
       "      <td>states reported  deaths we are still seeing a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This is the sixth time a global health emergen...</td>\n",
       "      <td>This is the sixth time a global health emergen...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/JvKC0PTett]</td>\n",
       "      <td>[@DrTedros]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>This is the sixth time a global health emergen...</td>\n",
       "      <td>this is the sixth time a global health emergen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Low #vitaminD was an independent predictor of ...</td>\n",
       "      <td>Low was an independent predictor of worse prog...</td>\n",
       "      <td>[#vitaminD]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/CGD6Kphn31, https://t.co/chtni8K...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[vitamin d]</td>\n",
       "      <td>Low   was an independent predictor of worse pr...</td>\n",
       "      <td>low #vitamind was an independent predictor of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>A common question: why are the cumulative outc...</td>\n",
       "      <td>A common question: why are the cumulative outc...</td>\n",
       "      <td>[#s]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[s]</td>\n",
       "      <td>A common question: why are the cumulative outc...</td>\n",
       "      <td>a common question why are the cumulative outco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>The government should consider bringing in any...</td>\n",
       "      <td>The government should consider bringing in any...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/pdOls6cqoN]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>The government should consider bringing in any...</td>\n",
       "      <td>the government should consider bringing in any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Our daily update is published. Weâ€™ve now track...</td>\n",
       "      <td>Our daily update is published. Weve now tracke...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/PZrmH4bl5Y, https://t.co/2588xW5...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[2.9, 119, 1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Our daily update is published. Weâ€™ve now track...</td>\n",
       "      <td>our daily update is published weve now tracked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Breakdown of testing: 4 air crew 97 hotel &amp;amp...</td>\n",
       "      <td>Breakdown of testing: air crew hotel &amp;amp; hea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[4, 97, 71, 2, 200]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Breakdown of testing: 4 air crew 97 hotel &amp;amp...</td>\n",
       "      <td>breakdown of testing  air crew  hotel amp heal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tweet_id                                         full_tweet  \\\n",
       "0        1  Our daily update is published. States reported...   \n",
       "1        2             Alfalfa is the only cure for COVID-19.   \n",
       "2        3  President Trump Asked What He Would Do If He W...   \n",
       "3        4  States reported 630 deaths. We are still seein...   \n",
       "4        5  This is the sixth time a global health emergen...   \n",
       "5        6  Low #vitaminD was an independent predictor of ...   \n",
       "6        7  A common question: why are the cumulative outc...   \n",
       "7        8  The government should consider bringing in any...   \n",
       "8        9  Our daily update is published. Weâ€™ve now track...   \n",
       "9       10  Breakdown of testing: 4 air crew 97 hotel &amp...   \n",
       "\n",
       "                                      tweet_raw_text  \\\n",
       "0  Our daily update is published. States reported...   \n",
       "1             Alfalfa is the only cure for COVID-19.   \n",
       "2  President Trump Asked What He Would Do If He W...   \n",
       "3  States reported deaths. We are still seeing a ...   \n",
       "4  This is the sixth time a global health emergen...   \n",
       "5  Low was an independent predictor of worse prog...   \n",
       "6  A common question: why are the cumulative outc...   \n",
       "7  The government should consider bringing in any...   \n",
       "8  Our daily update is published. Weve now tracke...   \n",
       "9  Breakdown of testing: air crew hotel &amp; hea...   \n",
       "\n",
       "                       hashtags smiley emoji  \\\n",
       "0                            []     []    []   \n",
       "1                            []     []    []   \n",
       "2  [#donaldtrump, #coronavirus]     []    []   \n",
       "3                            []     []    []   \n",
       "4                            []     []    []   \n",
       "5                   [#vitaminD]     []    []   \n",
       "6                          [#s]     []    []   \n",
       "7                            []     []    []   \n",
       "8                            []     []    []   \n",
       "9                            []     []    []   \n",
       "\n",
       "                                                 url     mentions  \\\n",
       "0                          [https://t.co/wzSYMe0Sht]           []   \n",
       "1                                                 []           []   \n",
       "2                          [https://t.co/3MEWhusRZI]           []   \n",
       "3                          [https://t.co/LBmcot3h9a]           []   \n",
       "4                          [https://t.co/JvKC0PTett]  [@DrTedros]   \n",
       "5  [https://t.co/CGD6Kphn31, https://t.co/chtni8K...           []   \n",
       "6                                                 []           []   \n",
       "7                          [https://t.co/pdOls6cqoN]           []   \n",
       "8  [https://t.co/PZrmH4bl5Y, https://t.co/2588xW5...           []   \n",
       "9                                                 []           []   \n",
       "\n",
       "                 numerals reserved_word emotext               segmented_hash  \\\n",
       "0  [734, 39, 532, 30, 22]            []      []                           []   \n",
       "1                      []            []      []                           []   \n",
       "2                      []            []      []  [donald trump, coronavirus]   \n",
       "3               [630, 28]            []      []                           []   \n",
       "4                      []            []      []                           []   \n",
       "5                      []            []      []                  [vitamin d]   \n",
       "6                      []            []      []                          [s]   \n",
       "7                      []            []      []                           []   \n",
       "8           [2.9, 119, 1]            []      []                           []   \n",
       "9     [4, 97, 71, 2, 200]            []      []                           []   \n",
       "\n",
       "                                               clean  \\\n",
       "0  Our daily update is published. States reported...   \n",
       "1             Alfalfa is the only cure for COVID-19.   \n",
       "2  President Trump Asked What He Would Do If He W...   \n",
       "3  States reported 630 deaths. We are still seein...   \n",
       "4  This is the sixth time a global health emergen...   \n",
       "5  Low   was an independent predictor of worse pr...   \n",
       "6  A common question: why are the cumulative outc...   \n",
       "7  The government should consider bringing in any...   \n",
       "8  Our daily update is published. Weâ€™ve now track...   \n",
       "9  Breakdown of testing: 4 air crew 97 hotel &amp...   \n",
       "\n",
       "                                               tweet  \n",
       "0  our daily update is published states reported ...  \n",
       "1                 alfalfa is the only cure for covid  \n",
       "2  president trump asked what he would do if he w...  \n",
       "3  states reported  deaths we are still seeing a ...  \n",
       "4  this is the sixth time a global health emergen...  \n",
       "5  low #vitamind was an independent predictor of ...  \n",
       "6  a common question why are the cumulative outco...  \n",
       "7  the government should consider bringing in any...  \n",
       "8  our daily update is published weve now tracked...  \n",
       "9  breakdown of testing  air crew  hotel amp heal...  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:36:45.196372Z",
     "iopub.status.busy": "2020-12-08T18:36:45.196103Z",
     "iopub.status.idle": "2020-12-08T18:36:45.200641Z",
     "shell.execute_reply": "2020-12-08T18:36:45.199934Z",
     "shell.execute_reply.started": "2020-12-08T18:36:45.196346Z"
    }
   },
   "outputs": [],
   "source": [
    "test['label'] = np.array(fin_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:36:48.432630Z",
     "iopub.status.busy": "2020-12-08T18:36:48.432484Z",
     "iopub.status.idle": "2020-12-08T18:36:48.435773Z",
     "shell.execute_reply": "2020-12-08T18:36:48.435223Z",
     "shell.execute_reply.started": "2020-12-08T18:36:48.432612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2140"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fin_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:36:51.839326Z",
     "iopub.status.busy": "2020-12-08T18:36:51.839176Z",
     "iopub.status.idle": "2020-12-08T18:36:51.842650Z",
     "shell.execute_reply": "2020-12-08T18:36:51.842101Z",
     "shell.execute_reply.started": "2020-12-08T18:36:51.839308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2140"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.full_tweet.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:16:33.713722Z",
     "iopub.status.busy": "2020-12-08T18:16:33.713415Z",
     "iopub.status.idle": "2020-12-08T18:16:33.756056Z",
     "shell.execute_reply": "2020-12-08T18:16:33.755238Z",
     "shell.execute_reply.started": "2020-12-08T18:16:33.713693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>full_tweet</th>\n",
       "      <th>tweet_raw_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>smiley</th>\n",
       "      <th>emoji</th>\n",
       "      <th>url</th>\n",
       "      <th>mentions</th>\n",
       "      <th>numerals</th>\n",
       "      <th>reserved_word</th>\n",
       "      <th>emotext</th>\n",
       "      <th>segmented_hash</th>\n",
       "      <th>clean</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>1568</td>\n",
       "      <td>The opening ceremony of the London 2012 Olympi...</td>\n",
       "      <td>The opening ceremony of the London Olympics an...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[2012]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>The opening ceremony of the London 2012 Olympi...</td>\n",
       "      <td>the opening ceremony of the london  olympics a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>1171</td>\n",
       "      <td>RT @drharshvardhan: .@MoHFW_INDIA has decided ...</td>\n",
       "      <td>: . has decided to deploy high level Central t...</td>\n",
       "      <td>[#UttarPradesh, #Jharkhand, #Chhattisgarh]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@drharshvardhan, @MoHFW_INDIA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[RT]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[uttar pradesh, jharkhand, chhattisgarh]</td>\n",
       "      <td>RT  : .  has decided to deploy high level Cent...</td>\n",
       "      <td>rt drharshvardhan mohfw_india has decided to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>285</td>\n",
       "      <td>Our daily update is published. States reported...</td>\n",
       "      <td>Our daily update is published. States reported...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/hDJwxhheNS]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[727, 39, 475, 30]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Our daily update is published. States reported...</td>\n",
       "      <td>our daily update is published states reported ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>1436</td>\n",
       "      <td>(4/4)India's calibrated testing strategy formu...</td>\n",
       "      <td>(4/4)India's calibrated testing strategy formu...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>(4/4)India's calibrated testing strategy formu...</td>\n",
       "      <td>indias calibrated testing strategy formulati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>1226</td>\n",
       "      <td>A video of a policeman taking down a man wande...</td>\n",
       "      <td>A video of a policeman taking down a man wande...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>A video of a policeman taking down a man wande...</td>\n",
       "      <td>a video of a policeman taking down a man wande...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>1707</td>\n",
       "      <td>Our daily update is published. Weâ€™ve now track...</td>\n",
       "      <td>Our daily update is published. Weve now tracke...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/PZrmH4bl5Y, https://t.co/dTN3Ivm...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[167, 2]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Our daily update is published. Weâ€™ve now track...</td>\n",
       "      <td>our daily update is published weve now tracked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>1251</td>\n",
       "      <td>We also just a number of new cases for Texasâ€”1...</td>\n",
       "      <td>We also just a number of new cases for Texas18...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/GtgfKkinE8]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>We also just a number of new cases for Texasâ€”1...</td>\n",
       "      <td>we also just a number of new cases for texasin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>273</td>\n",
       "      <td>#CoronaVirusUpdates: #COVID19 testing status u...</td>\n",
       "      <td>: testing status update: stated that samples t...</td>\n",
       "      <td>[#CoronaVirusUpdates, #COVID19, #StaySafe, #In...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/6G5M8rsZ66]</td>\n",
       "      <td>[@ICMRDELHI]</td>\n",
       "      <td>[66279462, 22, 2020, 953683, 22, 2020]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[corona virus updates, covid 19, stay safe, in...</td>\n",
       "      <td>:   testing status update:   stated that 6627...</td>\n",
       "      <td>#coronavirusupdates #covid testing status upda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>151</td>\n",
       "      <td>There are 4 #COVID19 Govt. testing lab in #Agr...</td>\n",
       "      <td>There are Govt. testing lab in . Kindly refer ...</td>\n",
       "      <td>[#COVID19, #Agra, #UttarPradesh, #COVID__19, #...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/SQCvfE2ZNc, https://t.co/nPLgGkj...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[covid 19, agra, uttar pradesh, covid _ _ 19, ...</td>\n",
       "      <td>There are 4   Govt. testing lab in    . Kindly...</td>\n",
       "      <td>there are  #covid govt testing lab in #agra #u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>1280</td>\n",
       "      <td>As per @ICMRDELHI it is not recommended to rel...</td>\n",
       "      <td>As per it is not recommended to rely on numeri...</td>\n",
       "      <td>[#COVID19, #COVID__19, #COVID, #COVID_19, #COV...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://t.co/o61SDKSKpg, https://t.co/GzEg37D...</td>\n",
       "      <td>[@ICMRDELHI]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[covid 19, covid _ _ 19, covid, covid _ 19, co...</td>\n",
       "      <td>As per   it is not recommended to rely on nume...</td>\n",
       "      <td>as per icmrdelhi it is not recommended to rely...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id                                         full_tweet  \\\n",
       "1566     1568  The opening ceremony of the London 2012 Olympi...   \n",
       "1169     1171  RT @drharshvardhan: .@MoHFW_INDIA has decided ...   \n",
       "283       285  Our daily update is published. States reported...   \n",
       "1434     1436  (4/4)India's calibrated testing strategy formu...   \n",
       "1224     1226  A video of a policeman taking down a man wande...   \n",
       "1705     1707  Our daily update is published. Weâ€™ve now track...   \n",
       "1249     1251  We also just a number of new cases for Texasâ€”1...   \n",
       "271       273  #CoronaVirusUpdates: #COVID19 testing status u...   \n",
       "149       151  There are 4 #COVID19 Govt. testing lab in #Agr...   \n",
       "1278     1280  As per @ICMRDELHI it is not recommended to rel...   \n",
       "\n",
       "                                         tweet_raw_text  \\\n",
       "1566  The opening ceremony of the London Olympics an...   \n",
       "1169  : . has decided to deploy high level Central t...   \n",
       "283   Our daily update is published. States reported...   \n",
       "1434  (4/4)India's calibrated testing strategy formu...   \n",
       "1224  A video of a policeman taking down a man wande...   \n",
       "1705  Our daily update is published. Weve now tracke...   \n",
       "1249  We also just a number of new cases for Texas18...   \n",
       "271   : testing status update: stated that samples t...   \n",
       "149   There are Govt. testing lab in . Kindly refer ...   \n",
       "1278  As per it is not recommended to rely on numeri...   \n",
       "\n",
       "                                               hashtags smiley emoji  \\\n",
       "1566                                                 []     []    []   \n",
       "1169         [#UttarPradesh, #Jharkhand, #Chhattisgarh]     []    []   \n",
       "283                                                  []     []    []   \n",
       "1434                                                 []     []    []   \n",
       "1224                                                 []     []    []   \n",
       "1705                                                 []     []    []   \n",
       "1249                                                 []     []    []   \n",
       "271   [#CoronaVirusUpdates, #COVID19, #StaySafe, #In...     []    []   \n",
       "149   [#COVID19, #Agra, #UttarPradesh, #COVID__19, #...     []    []   \n",
       "1278  [#COVID19, #COVID__19, #COVID, #COVID_19, #COV...     []    []   \n",
       "\n",
       "                                                    url  \\\n",
       "1566                                                 []   \n",
       "1169                                                 []   \n",
       "283                           [https://t.co/hDJwxhheNS]   \n",
       "1434                                                 []   \n",
       "1224                                                 []   \n",
       "1705  [https://t.co/PZrmH4bl5Y, https://t.co/dTN3Ivm...   \n",
       "1249                          [https://t.co/GtgfKkinE8]   \n",
       "271                           [https://t.co/6G5M8rsZ66]   \n",
       "149   [https://t.co/SQCvfE2ZNc, https://t.co/nPLgGkj...   \n",
       "1278  [https://t.co/o61SDKSKpg, https://t.co/GzEg37D...   \n",
       "\n",
       "                             mentions                                numerals  \\\n",
       "1566                               []                                  [2012]   \n",
       "1169  [@drharshvardhan, @MoHFW_INDIA]                                      []   \n",
       "283                                []                      [727, 39, 475, 30]   \n",
       "1434                               []                                      []   \n",
       "1224                               []                                      []   \n",
       "1705                               []                                [167, 2]   \n",
       "1249                               []                                      []   \n",
       "271                      [@ICMRDELHI]  [66279462, 22, 2020, 953683, 22, 2020]   \n",
       "149                                []                                     [4]   \n",
       "1278                     [@ICMRDELHI]                                      []   \n",
       "\n",
       "     reserved_word emotext                                     segmented_hash  \\\n",
       "1566            []      []                                                 []   \n",
       "1169          [RT]      []           [uttar pradesh, jharkhand, chhattisgarh]   \n",
       "283             []      []                                                 []   \n",
       "1434            []      []                                                 []   \n",
       "1224            []      []                                                 []   \n",
       "1705            []      []                                                 []   \n",
       "1249            []      []                                                 []   \n",
       "271             []      []  [corona virus updates, covid 19, stay safe, in...   \n",
       "149             []      []  [covid 19, agra, uttar pradesh, covid _ _ 19, ...   \n",
       "1278            []      []  [covid 19, covid _ _ 19, covid, covid _ 19, co...   \n",
       "\n",
       "                                                  clean  \\\n",
       "1566  The opening ceremony of the London 2012 Olympi...   \n",
       "1169  RT  : .  has decided to deploy high level Cent...   \n",
       "283   Our daily update is published. States reported...   \n",
       "1434  (4/4)India's calibrated testing strategy formu...   \n",
       "1224  A video of a policeman taking down a man wande...   \n",
       "1705  Our daily update is published. Weâ€™ve now track...   \n",
       "1249  We also just a number of new cases for Texasâ€”1...   \n",
       "271    :   testing status update:   stated that 6627...   \n",
       "149   There are 4   Govt. testing lab in    . Kindly...   \n",
       "1278  As per   it is not recommended to rely on nume...   \n",
       "\n",
       "                                                  tweet  \n",
       "1566  the opening ceremony of the london  olympics a...  \n",
       "1169  rt drharshvardhan mohfw_india has decided to d...  \n",
       "283   our daily update is published states reported ...  \n",
       "1434    indias calibrated testing strategy formulati...  \n",
       "1224  a video of a policeman taking down a man wande...  \n",
       "1705  our daily update is published weve now tracked...  \n",
       "1249  we also just a number of new cases for texasin...  \n",
       "271   #coronavirusupdates #covid testing status upda...  \n",
       "149   there are  #covid govt testing lab in #agra #u...  \n",
       "1278  as per icmrdelhi it is not recommended to rely...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:39:26.437041Z",
     "iopub.status.busy": "2020-12-08T18:39:26.436817Z",
     "iopub.status.idle": "2020-12-08T18:39:26.441882Z",
     "shell.execute_reply": "2020-12-08T18:39:26.441219Z",
     "shell.execute_reply.started": "2020-12-08T18:39:26.437015Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_decode(val):\n",
    "    return labels[val]\n",
    "test.label = test.label.apply(label_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T18:40:22.296972Z",
     "iopub.status.busy": "2020-12-08T18:40:22.296705Z",
     "iopub.status.idle": "2020-12-08T18:40:22.322986Z",
     "shell.execute_reply": "2020-12-08T18:40:22.322417Z",
     "shell.execute_reply.started": "2020-12-08T18:40:22.296945Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test.to_csv(path_or_buf='answers2.txt', index=False, columns = ['tweet_id', 'label'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
